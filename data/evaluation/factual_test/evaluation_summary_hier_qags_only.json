{
  "eurollm_gen_hier_summary": {
    "avg_rouge1_precision": 0.44658595989478794,
    "avg_rouge1_recall": 0.2345293712623666,
    "avg_rouge1_fmeasure": 0.28003495100564846,
    "avg_rouge2_precision": 0.09907270224208345,
    "avg_rouge2_recall": 0.05127246839074626,
    "avg_rouge2_fmeasure": 0.061108010020632084,
    "avg_rougeL_precision": 0.1949751512874965,
    "avg_rougeL_recall": 0.100146002998243,
    "avg_rougeL_fmeasure": 0.11985584072373527,
    "avg_qags_score": 0.18753914419311402,
    "avg_qags_question_count": 4.840191387559809,
    "avg_qags_valid_answers": 4.874638379942141,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "openai_gpt_4o_gen_hier_summary": {
    "avg_rouge1_precision": 0.5886106694320781,
    "avg_rouge1_recall": 0.28257429515740456,
    "avg_rouge1_fmeasure": 0.35842544531113846,
    "avg_rouge2_precision": 0.20649496218828933,
    "avg_rouge2_recall": 0.09776773309624454,
    "avg_rouge2_fmeasure": 0.12423819132841263,
    "avg_rougeL_precision": 0.2894020694976612,
    "avg_rougeL_recall": 0.13884815084474036,
    "avg_rougeL_fmeasure": 0.17577715458823756,
    "avg_qags_score": 0.2636511359525857,
    "avg_qags_question_count": 4.780861244019139,
    "avg_qags_valid_answers": 4.7982708933717575,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "llama_3_8b_gen_hier_summary": {
    "avg_rouge1_precision": 0.5001654697340994,
    "avg_rouge1_recall": 0.229815153931163,
    "avg_rouge1_fmeasure": 0.29269096530443905,
    "avg_rouge2_precision": 0.13608691914825985,
    "avg_rouge2_recall": 0.06102338120184269,
    "avg_rouge2_fmeasure": 0.07799708055726658,
    "avg_rougeL_precision": 0.23138707289330696,
    "avg_rougeL_recall": 0.10562981939433824,
    "avg_rougeL_fmeasure": 0.13438540309890773,
    "avg_qags_score": 0.22893817604657354,
    "avg_qags_question_count": 4.902392344497608,
    "avg_qags_valid_answers": 4.906130268199234,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "teuken_gen_hier_summary": {
    "avg_rouge1_precision": 0.37776248184652905,
    "avg_rouge1_recall": 0.1186281366387511,
    "avg_rouge1_fmeasure": 0.1630487645099904,
    "avg_rouge2_precision": 0.052430389342869504,
    "avg_rouge2_recall": 0.015314069589762398,
    "avg_rouge2_fmeasure": 0.021254194251825127,
    "avg_rougeL_precision": 0.1739863661772137,
    "avg_rougeL_recall": 0.05084364415632153,
    "avg_rougeL_fmeasure": 0.07032084691045767,
    "avg_qags_score": 0.16072666853303583,
    "avg_qags_question_count": 4.9415708812260535,
    "avg_qags_valid_answers": 4.9453499520613615,
    "total_samples": 1045,
    "successful_generations": 1044,
    "failed_generations": 1
  },
  "mistral_v03_gen_hier_summary": {
    "avg_rouge1_precision": 0.5531749996024001,
    "avg_rouge1_recall": 0.2987169839841311,
    "avg_rouge1_fmeasure": 0.35707592620425777,
    "avg_rouge2_precision": 0.1956021198397645,
    "avg_rouge2_recall": 0.1007422406279151,
    "avg_rouge2_fmeasure": 0.12180427303465413,
    "avg_rougeL_precision": 0.2573815885555755,
    "avg_rougeL_recall": 0.13708853276122748,
    "avg_rougeL_fmeasure": 0.16381251015669934,
    "avg_qags_score": 0.2385607809562122,
    "avg_qags_question_count": 4.691866028708134,
    "avg_qags_valid_answers": 4.737198067632851,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "llama_3_3_70B_gen_hier_summary": {
    "avg_rouge1_precision": 0.6123822534485421,
    "avg_rouge1_recall": 0.29836082044187284,
    "avg_rouge1_fmeasure": 0.37458252587022367,
    "avg_rouge2_precision": 0.23554642788020008,
    "avg_rouge2_recall": 0.1117991654170167,
    "avg_rouge2_fmeasure": 0.14105230260588322,
    "avg_rougeL_precision": 0.3054388079637426,
    "avg_rougeL_recall": 0.1489792166774887,
    "avg_rougeL_fmeasure": 0.18637087120645476,
    "avg_qags_score": 0.28634879828801335,
    "avg_qags_question_count": 4.941626794258373,
    "avg_qags_valid_answers": 4.94066985645933,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  }
}