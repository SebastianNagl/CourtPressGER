{
  "eurollm_gen_hier_summary": {
    "avg_rouge1_precision": 0.44658595989478794,
    "avg_rouge1_recall": 0.2345293712623666,
    "avg_rouge1_fmeasure": 0.28003495100564846,
    "avg_rouge2_precision": 0.09907270224208345,
    "avg_rouge2_recall": 0.05127246839074626,
    "avg_rouge2_fmeasure": 0.061108010020632084,
    "avg_rougeL_precision": 0.1949751512874965,
    "avg_rougeL_recall": 0.100146002998243,
    "avg_rougeL_fmeasure": 0.11985584072373527,
    "avg_llm_judge_faktische_korrektheit": 4.973887814313346,
    "avg_llm_judge_vollständigkeit": 4.425531914893617,
    "avg_llm_judge_klarheit": 6.404255319148936,
    "avg_llm_judge_struktur": 6.687620889748549,
    "avg_llm_judge_vergleich_mit_referenz": 3.5435203094777563,
    "avg_llm_judge_gesamtscore": 5.206963249516441,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "openai_gpt_4o_gen_hier_summary": {
    "avg_rouge1_precision": 0.5886106694320781,
    "avg_rouge1_recall": 0.28257429515740456,
    "avg_rouge1_fmeasure": 0.35842544531113846,
    "avg_rouge2_precision": 0.20649496218828933,
    "avg_rouge2_recall": 0.09776773309624454,
    "avg_rouge2_fmeasure": 0.12423819132841263,
    "avg_rougeL_precision": 0.2894020694976612,
    "avg_rougeL_recall": 0.13884815084474036,
    "avg_rougeL_fmeasure": 0.17577715458823756,
    "avg_llm_judge_faktische_korrektheit": 8.107003891050583,
    "avg_llm_judge_vollständigkeit": 7.08852140077821,
    "avg_llm_judge_klarheit": 8.745136186770427,
    "avg_llm_judge_struktur": 8.407587548638132,
    "avg_llm_judge_vergleich_mit_referenz": 6.841439688715953,
    "avg_llm_judge_gesamtscore": 7.837937743190661,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "llama_3_8b_gen_hier_summary": {
    "avg_rouge1_precision": 0.5001654697340994,
    "avg_rouge1_recall": 0.229815153931163,
    "avg_rouge1_fmeasure": 0.29269096530443905,
    "avg_rouge2_precision": 0.13608691914825985,
    "avg_rouge2_recall": 0.06102338120184269,
    "avg_rouge2_fmeasure": 0.07799708055726658,
    "avg_rougeL_precision": 0.23138707289330696,
    "avg_rougeL_recall": 0.10562981939433824,
    "avg_rougeL_fmeasure": 0.13438540309890773,
    "avg_llm_judge_faktische_korrektheit": 5.277992277992278,
    "avg_llm_judge_vollständigkeit": 4.54054054054054,
    "avg_llm_judge_klarheit": 6.306949806949807,
    "avg_llm_judge_struktur": 6.42953667953668,
    "avg_llm_judge_vergleich_mit_referenz": 3.775096525096525,
    "avg_llm_judge_gesamtscore": 5.2660231660231664,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "teuken_gen_hier_summary": {
    "avg_rouge1_precision": 0.37776248184652905,
    "avg_rouge1_recall": 0.1186281366387511,
    "avg_rouge1_fmeasure": 0.1630487645099904,
    "avg_rouge2_precision": 0.052430389342869504,
    "avg_rouge2_recall": 0.015314069589762398,
    "avg_rouge2_fmeasure": 0.021254194251825127,
    "avg_rougeL_precision": 0.1739863661772137,
    "avg_rougeL_recall": 0.05084364415632153,
    "avg_rougeL_fmeasure": 0.07032084691045767,
    "avg_llm_judge_faktische_korrektheit": 3.0634615384615387,
    "avg_llm_judge_vollständigkeit": 2.160576923076923,
    "avg_llm_judge_klarheit": 4.235576923076923,
    "avg_llm_judge_struktur": 4.407692307692308,
    "avg_llm_judge_vergleich_mit_referenz": 1.8269230769230769,
    "avg_llm_judge_gesamtscore": 3.138846153846154,
    "total_samples": 1045,
    "successful_generations": 1044,
    "failed_generations": 1
  },
  "mistral_v03_gen_hier_summary": {
    "avg_rouge1_precision": 0.5531749996024001,
    "avg_rouge1_recall": 0.2987169839841311,
    "avg_rouge1_fmeasure": 0.35707592620425777,
    "avg_rouge2_precision": 0.1956021198397645,
    "avg_rouge2_recall": 0.1007422406279151,
    "avg_rouge2_fmeasure": 0.12180427303465413,
    "avg_rougeL_precision": 0.2573815885555755,
    "avg_rougeL_recall": 0.13708853276122748,
    "avg_rougeL_fmeasure": 0.16381251015669934,
    "avg_llm_judge_faktische_korrektheit": 5.53757225433526,
    "avg_llm_judge_vollständigkeit": 4.965317919075145,
    "avg_llm_judge_klarheit": 5.557803468208093,
    "avg_llm_judge_struktur": 5.244701348747592,
    "avg_llm_judge_vergleich_mit_referenz": 3.7369942196531793,
    "avg_llm_judge_gesamtscore": 5.008477842003853,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "llama_3_3_70B_gen_hier_summary": {
    "avg_rouge1_precision": 0.6123822534485421,
    "avg_rouge1_recall": 0.29836082044187284,
    "avg_rouge1_fmeasure": 0.37458252587022367,
    "avg_rouge2_precision": 0.23554642788020008,
    "avg_rouge2_recall": 0.1117991654170167,
    "avg_rouge2_fmeasure": 0.14105230260588322,
    "avg_rougeL_precision": 0.3054388079637426,
    "avg_rougeL_recall": 0.1489792166774887,
    "avg_rougeL_fmeasure": 0.18637087120645476,
    "avg_llm_judge_faktische_korrektheit": 7.341650671785029,
    "avg_llm_judge_vollständigkeit": 6.363723608445298,
    "avg_llm_judge_klarheit": 8.154510556621881,
    "avg_llm_judge_struktur": 7.619961612284069,
    "avg_llm_judge_vergleich_mit_referenz": 5.900191938579654,
    "avg_llm_judge_gesamtscore": 7.076007677543186,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  }
}