{
  "llama_3_3_70B_generated_judgement_summary": {
    "avg_rouge1_precision": 0.6406758861127981,
    "avg_rouge1_recall": 0.29994411655211817,
    "avg_rouge1_fmeasure": 0.382309190595661,
    "avg_rouge2_precision": 0.2721738552903488,
    "avg_rouge2_recall": 0.12506195911981816,
    "avg_rouge2_fmeasure": 0.16006398727979504,
    "avg_rougeL_precision": 0.3350115884241865,
    "avg_rougeL_recall": 0.15705122248043243,
    "avg_rougeL_fmeasure": 0.19972296183451183,
    "avg_bleu1": 0.22475859147842753,
    "avg_bleu2": 0.1384626016372003,
    "avg_bleu3": 0.0945761456363515,
    "avg_bleu4": 0.06684492208520836,
    "avg_meteor": 0.19860103490433692,
    "avg_bertscore_precision": 0.788891447103765,
    "avg_bertscore_recall": 0.7508236180367082,
    "avg_bertscore_f1": 0.7690641379812688,
    "avg_keyword_overlap": 0.2197669731464027,
    "avg_entity_overlap": 0.23107104059610314,
    "avg_length_ratio": 0.49717657390019176,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "mistral_v03_generated_judgement_summary": {
    "avg_rouge1_precision": 0.6161534368791741,
    "avg_rouge1_recall": 0.2873331223248295,
    "avg_rouge1_fmeasure": 0.36123532731274194,
    "avg_rouge2_precision": 0.27403466880397037,
    "avg_rouge2_recall": 0.12261058333932776,
    "avg_rouge2_fmeasure": 0.15611640109629962,
    "avg_rougeL_precision": 0.3224440273931828,
    "avg_rougeL_recall": 0.14567425127526423,
    "avg_rougeL_fmeasure": 0.18442262880727775,
    "avg_bleu1": 0.21263034267680142,
    "avg_bleu2": 0.13036075521045018,
    "avg_bleu3": 0.09066480486957779,
    "avg_bleu4": 0.06600734131728012,
    "avg_meteor": 0.1900858183811906,
    "avg_bertscore_precision": 0.7706092999920006,
    "avg_bertscore_recall": 0.7255094435722124,
    "avg_bertscore_f1": 0.7464611968877112,
    "avg_keyword_overlap": 0.21315585457862332,
    "avg_entity_overlap": 0.20741876467820392,
    "avg_length_ratio": 0.4928544542382533,
    "total_samples": 1045,
    "successful_generations": 1039,
    "failed_generations": 6
  },
  "openai_gpt_4o_generated_judgement_summary": {
    "avg_rouge1_precision": 0.6436643582166336,
    "avg_rouge1_recall": 0.27614514759815034,
    "avg_rouge1_fmeasure": 0.36266692645414716,
    "avg_rouge2_precision": 0.26057573825077834,
    "avg_rouge2_recall": 0.11050603909245002,
    "avg_rouge2_fmeasure": 0.14515418795177074,
    "avg_rougeL_precision": 0.34015175121574326,
    "avg_rougeL_recall": 0.1464354352868417,
    "avg_rougeL_fmeasure": 0.19178838322744626,
    "avg_bleu1": 0.21049168238935154,
    "avg_bleu2": 0.1266272876618973,
    "avg_bleu3": 0.08318548860366391,
    "avg_bleu4": 0.05589988911115032,
    "avg_meteor": 0.1845403812228536,
    "avg_bertscore_precision": 0.7745507887676002,
    "avg_bertscore_recall": 0.7395800350955799,
    "avg_bertscore_f1": 0.7563365244979493,
    "avg_keyword_overlap": 0.20824748031649642,
    "avg_entity_overlap": 0.22900950162482464,
    "avg_length_ratio": 0.45723092529798165,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  }
}