{
  "llama_3_3_70B_generated_judgement_summary": {
    "avg_rouge1_precision": 0.6406758861127981,
    "avg_rouge1_recall": 0.29994411655211817,
    "avg_rouge1_fmeasure": 0.382309190595661,
    "avg_rouge2_precision": 0.2721738552903488,
    "avg_rouge2_recall": 0.12506195911981816,
    "avg_rouge2_fmeasure": 0.16006398727979504,
    "avg_rougeL_precision": 0.3350115884241865,
    "avg_rougeL_recall": 0.15705122248043243,
    "avg_rougeL_fmeasure": 0.19972296183451183,
    "avg_llm_judge_faktische_korrektheit": 8.172103487064117,
    "avg_llm_judge_vollständigkeit": 6.866141732283465,
    "avg_llm_judge_klarheit": 8.633295838020247,
    "avg_llm_judge_struktur": 8.155230596175478,
    "avg_llm_judge_vergleich_mit_referenz": 6.66029246344207,
    "avg_llm_judge_gesamtscore": 7.697412823397076,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  },
  "mistral_v03_generated_judgement_summary": {
    "avg_rouge1_precision": 0.6161534368791741,
    "avg_rouge1_recall": 0.2873331223248295,
    "avg_rouge1_fmeasure": 0.36123532731274194,
    "avg_rouge2_precision": 0.27403466880397037,
    "avg_rouge2_recall": 0.12261058333932776,
    "avg_rouge2_fmeasure": 0.15611640109629962,
    "avg_rougeL_precision": 0.3224440273931828,
    "avg_rougeL_recall": 0.14567425127526423,
    "avg_rougeL_fmeasure": 0.18442262880727775,
    "avg_llm_judge_faktische_korrektheit": 6.961240310077519,
    "avg_llm_judge_vollständigkeit": 5.714147286821706,
    "avg_llm_judge_klarheit": 7.1395348837209305,
    "avg_llm_judge_struktur": 6.811046511627907,
    "avg_llm_judge_vergleich_mit_referenz": 5.0271317829457365,
    "avg_llm_judge_gesamtscore": 6.330620155038759,
    "total_samples": 1045,
    "successful_generations": 1039,
    "failed_generations": 6
  },
  "openai_gpt_4o_generated_judgement_summary": {
    "avg_rouge1_precision": 0.6436643582166336,
    "avg_rouge1_recall": 0.27614514759815034,
    "avg_rouge1_fmeasure": 0.36266692645414716,
    "avg_rouge2_precision": 0.26057573825077834,
    "avg_rouge2_recall": 0.11050603909245002,
    "avg_rouge2_fmeasure": 0.14515418795177074,
    "avg_rougeL_precision": 0.34015175121574326,
    "avg_rougeL_recall": 0.1464354352868417,
    "avg_rougeL_fmeasure": 0.19178838322744626,
    "avg_llm_judge_faktische_korrektheit": 8.393269230769231,
    "avg_llm_judge_vollständigkeit": 7.161538461538462,
    "avg_llm_judge_klarheit": 8.819230769230769,
    "avg_llm_judge_struktur": 8.538461538461538,
    "avg_llm_judge_vergleich_mit_referenz": 7.0115384615384615,
    "avg_llm_judge_gesamtscore": 7.984807692307693,
    "total_samples": 1045,
    "successful_generations": 1045,
    "failed_generations": 0
  }
}