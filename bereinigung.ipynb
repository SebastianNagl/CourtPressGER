{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bereinigung des Gerichtsurteils-Presseerklärungs-Datensatzes\n",
    "\n",
    "Dieses Notebook implementiert verschiedene Ansätze zur Bereinigung des Datensatzes mit Gerichtsurteilen und zugehörigen Pressemitteilungen. Das Ziel ist die Identifikation und Entfernung von Pressemitteilungen, die keinen direkten Bezug zu einem ergangenen Urteil haben (z.B. Ankündigungen zukünftiger Verhandlungen oder allgemeine Mitteilungen).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Überblick\n",
    "\n",
    "Wir implementieren und evaluieren folgende Ansätze:\n",
    "\n",
    "1. **Regelbasierte Filter**: \n",
    "   - Keywords und Muster zur Erkennung von Ankündigungen\n",
    "   - Regex-Patterns für typische Formulierungen\n",
    "   - Strukturelle Merkmale der Texte\n",
    "\n",
    "2. **Semantische Ähnlichkeit**:\n",
    "   - Berechnung von Text-Embeddings\n",
    "   - Kosinus-Ähnlichkeit zwischen Urteil und Pressemitteilung\n",
    "   - Identifikation inhaltlich abweichender Mitteilungen\n",
    "\n",
    "3. **Überwachtes Machine Learning**:\n",
    "   - Trainieren eines Klassifikators auf Basis der vorherigen Methoden\n",
    "   - Logistische Regression mit TF-IDF Features\n",
    "   - Analyse der wichtigsten Merkmale\n",
    "\n",
    "4. **Unüberwachtes Clustering**:\n",
    "   - Gruppierung ähnlicher Pressemitteilungen\n",
    "   - Topic Modeling zur Mustererkennung\n",
    "   - Identifikation von Ankündigungs-Clustern\n",
    "\n",
    "Am Ende vergleichen wir die Methoden und kombinieren sie zu einem robusten Gesamtansatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erweiterte Bereinigungsansätze\n",
    "\n",
    "In diesem Abschnitt implementieren wir weitere Ansätze zur Bereinigung des Datensatzes, die in `ansatz_bereinigung.md` beschrieben wurden:\n",
    "\n",
    "1. Erweiterter regelbasierter Ansatz\n",
    "2. Semantische Ähnlichkeit mit Embeddings\n",
    "3. Überwachtes Machine Learning (Klassifikation)\n",
    "4. Unüberwachte Verfahren (Clustering/Topic Modeling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Vorbereitung für erweiterte Bereinigungsansätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten geladen: 6592 Einträge\n",
      "SpaCy deutsches Modell geladen\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU-Beschleunigung importieren\n",
    "try:\n",
    "    import cudf\n",
    "    import cuml\n",
    "    import cupy as cp\n",
    "    from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "    from cuml.decomposition import PCA as cuPCA\n",
    "    from cuml.cluster import KMeans as cuKMeans\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.metrics import pairwise_distances\n",
    "    from cuml.preprocessing import normalize\n",
    "    USE_GPU = True\n",
    "    print(\"GPU-Beschleunigung ist aktiviert (RAPIDS-Bibliotheken geladen)\")\n",
    "except ImportError:\n",
    "    USE_GPU = False\n",
    "    print(\"GPU-Beschleunigung ist nicht verfügbar, verwende CPU-Version\")\n",
    "\n",
    "# Für alle Methoden laden wir die Daten neu\n",
    "try:\n",
    "    df = pd.read_csv('data/german_courts.csv')\n",
    "    print(f\"Daten geladen: {len(df)} Einträge\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
    "    # Fallback auf den bereits geladenen DataFrame, falls vorhanden\n",
    "\n",
    "# Sprachdaten vorbereiten\n",
    "try:\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"SpaCy deutsches Modell geladen\")\n",
    "except:\n",
    "    print(\"SpaCy deutsches Modell wird installiert...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"de_core_news_sm\"])\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    print(\"SpaCy deutsches Modell geladen\")\n",
    "\n",
    "# Kleine Hilfsfunktion zum Preprocessen von Text\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    doc = nlp(text)\n",
    "    # Nur Substantive, Verben, Adjektive und Adverbien behalten\n",
    "    tokens = [token.lemma_.lower() for token in doc \n",
    "              if not token.is_stop and not token.is_punct \n",
    "              and token.is_alpha and len(token.text) > 2 \n",
    "              and token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Sample für schnelleres Testen erstellen (optional)\n",
    "sample_size = 1000  # Anzahl der zufällig ausgewählten Datensätze\n",
    "sample_df = df.sample(min(sample_size, len(df)), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regelbasierte Filter\n",
    "\n",
    "Der erste Bereinigungsansatz basiert auf expliziten Regeln, die typische Muster in Ankündigungen und irrelevanten Pressemitteilungen erkennen:\n",
    "\n",
    "- **Keywords**: Typische Wörter für Ankündigungen (z.B. \"Terminankündigung\", \"wird verhandelt\")\n",
    "- **Zeitliche Marker**: Formulierungen, die auf zukünftige Ereignisse hinweisen\n",
    "- **Dokumentstruktur**: Analyse von Überschriften und Textanfängen\n",
    "- **Kombinierte Regeln**: Gewichtung verschiedener Indikatoren\n",
    "\n",
    "Die Funktion `is_announcement_rule_based()` implementiert diese Logik und klassifiziert jeden Eintrag als Ankündigung oder relevante Pressemitteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regelbasierte Erkennung: 1625 von 6592 Einträgen (24.65%) sind vermutlich Ankündigungen.\n",
      "\n",
      "Beispiele für erkannte Ankündigungen:\n",
      "ID: bfh_009-17\n",
      "Datum: 08. Februar 2017\n",
      "Pressemitteilung (Anfang): Kostümparty eines gemeinnützigen Karnevalsvereins kein Zweckbetrieb\n",
      "                \n",
      "                  08. Februar 2017\n",
      "                \n",
      "                -\n",
      "                  Nummer\n",
      "                  00...\n",
      "--------------------------------------------------------------------------------\n",
      "ID: bfh_011-13\n",
      "Datum: 20. Februar 2013\n",
      "Pressemitteilung (Anfang): Kosten einer Betriebsveranstaltung sind erst bei Überschreiten einer Freigrenze Arbeitslohn. Die Freigrenze beträgt auch 2007 noch 110 €.\n",
      "                \n",
      "                  20. Februar 2013\n",
      "          ...\n",
      "--------------------------------------------------------------------------------\n",
      "ID: bfh_032-17\n",
      "Datum: 17. Mai 2017\n",
      "Pressemitteilung (Anfang): Gewerbesteuerliche Hinzurechnung von Mieten für Konzertsäle\n",
      "                \n",
      "                  17. Mai 2017\n",
      "                \n",
      "                -\n",
      "                  Nummer\n",
      "                  032/17 - Urtei...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### 1. Erweiterter regelbasierter Ansatz (Keywords und Muster)\n",
    "\n",
    "# Wir erweitern den bereits vorhandenen regelbasierten Ansatz mit detaillierteren Regex-Mustern \n",
    "# und umfangreicheren Wortlisten, um Ankündigungen und allgemeine Mitteilungen zu identifizieren\n",
    "\n",
    "def is_announcement_rule_based(row):\n",
    "    \"\"\"Erkennt Ankündigungen und nicht urteilsbezogene Mitteilungen anhand von Keywords und Mustern\"\"\"\n",
    "    if pd.isna(row['summary']):\n",
    "        return False\n",
    "    \n",
    "    summary = str(row['summary']).lower()\n",
    "    \n",
    "    # 1. Keywords, die auf Ankündigungen hindeuten\n",
    "    future_indicators = [\n",
    "        'ankündigung', 'terminankündigung', 'terminhinweis', 'hinweis auf termin',\n",
    "        'wird verhandelt', 'wird verhandeln', 'wird.*stattfinden', 'findet statt',\n",
    "        'einladung', 'pressetermin', 'pressekonferenz', 'veranstaltung',\n",
    "        'wochenvorschau', 'jahrespressekonferenz', 'pressegespräch', 'rundgang',\n",
    "        'beginnt am', 'laden ein', 'lädt ein', 'sitzung vom'\n",
    "    ]\n",
    "    \n",
    "    # 2. Regex-Muster für Datumsangaben in der Zukunft (relativ zum Erstelldatum)\n",
    "    date_patterns = [\n",
    "        r'am\\s+\\d{1,2}\\.\\s*\\d{1,2}\\.\\s*\\d{4}',  # \"am 15.05.2023\"\n",
    "        r'vom\\s+\\d{1,2}\\.\\s*\\d{1,2}\\.',         # \"vom 15.05.\"\n",
    "        r'terminiert auf den \\d{1,2}',           # \"terminiert auf den 15\"\n",
    "        r'in der kommenden woche',               # zukünftige Verweise\n",
    "        r'in der nächsten woche',\n",
    "        r'demnächst'\n",
    "    ]\n",
    "    \n",
    "    # 3. Überschriften/Anfänge, die auf Ankündigungen hindeuten\n",
    "    headline_indicators = [\n",
    "        'pressemitteilung nr', 'presseinformation', 'information für die presse',\n",
    "        'medieninformation', 'zur information', 'mündliche verhandlung',\n",
    "        'terminvorschau', 'jahresbericht', 'geschäftsbericht', 'tätigkeitsbericht',\n",
    "        'stellenausschreibung', 'personelle veränderungen', 'neuer präsident'\n",
    "    ]\n",
    "    \n",
    "    # 4. Prüfen auf Keywords am Anfang des Textes (größeres Gewicht)\n",
    "    summary_start = summary[:100]\n",
    "    headline_match = any(indicator in summary_start for indicator in headline_indicators)\n",
    "    \n",
    "    # 5. Prüfen auf allgemeine Ankündigungskeywords im gesamten Text\n",
    "    keyword_match = any(re.search(r'\\b' + re.escape(indicator) + r'\\b', summary, re.IGNORECASE) \n",
    "                      for indicator in future_indicators)\n",
    "    \n",
    "    # 6. Prüfen auf Datumsmuster\n",
    "    date_match = any(re.search(pattern, summary, re.IGNORECASE) for pattern in date_patterns)\n",
    "    \n",
    "    # Kombinierte Entscheidung mit unterschiedlicher Gewichtung\n",
    "    if headline_match:\n",
    "        return True  # Überschriften/Anfänge sind starke Indikatoren\n",
    "    elif keyword_match and date_match:\n",
    "        return True  # Kombination aus Keyword und Datumsmuster\n",
    "    elif keyword_match and ('mündliche verhandlung' in summary or 'termin' in summary):\n",
    "        return True  # Spezifische Kombinationen\n",
    "    \n",
    "    return False  # Default: keine Ankündigung\n",
    "\n",
    "# Anwenden der regelbasierten Methode auf den Datensatz\n",
    "df['is_announcement_rule'] = df.apply(is_announcement_rule_based, axis=1)\n",
    "\n",
    "# Auswertung der regelbasierten Methode\n",
    "announcement_count = df['is_announcement_rule'].sum()\n",
    "print(f\"Regelbasierte Erkennung: {announcement_count} von {len(df)} Einträgen ({announcement_count/len(df)*100:.2f}%) sind vermutlich Ankündigungen.\")\n",
    "\n",
    "# Beispiele für erkannte Ankündigungen anzeigen\n",
    "print(\"\\nBeispiele für erkannte Ankündigungen:\")\n",
    "announcement_examples = df[df['is_announcement_rule']].head(3)\n",
    "for _, row in announcement_examples.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Datum: {row['date']}\")\n",
    "    print(f\"Pressemitteilung (Anfang): {row['summary'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantische Ähnlichkeit\n",
    "\n",
    "Der zweite Ansatz nutzt TF-IDF Vektorisierung und Kosinus-Ähnlichkeit, um inhaltliche Unterschiede zwischen Urteilen und Pressemitteilungen zu erkennen:\n",
    "\n",
    "- **Vorverarbeitung**: Lemmatisierung und Filterung mit spaCy\n",
    "- **TF-IDF**: Berechnung von Dokumentvektoren\n",
    "- **Ähnlichkeitsmetrik**: Kosinus-Ähnlichkeit zwischen Urteil und Pressemitteilung\n",
    "- **Schwellenwert**: Identifikation von Paaren mit geringer Ähnlichkeit\n",
    "\n",
    "Dieser Ansatz ist besonders effektiv bei der Erkennung von Pressemitteilungen, die inhaltlich stark vom zugehörigen Urteil abweichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Semantische Ähnlichkeit mit TF-IDF Embeddings\n",
      "Berechne TF-IDF Vektoren...\n",
      "Führe Text-Preprocessing durch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFühre Text-Preprocessing durch...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_summary\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).apply(preprocess_text)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_judgement\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjudgement\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Kombiniere alle Texte für das Training des Vektorisierers\u001b[39;00m\n\u001b[32m     25\u001b[39m all_texts = \u001b[38;5;28mlist\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_summary\u001b[39m\u001b[33m'\u001b[39m]) + \u001b[38;5;28mlist\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_judgement\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/CourtPressGER/.venv_local/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/CourtPressGER/.venv_local/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/CourtPressGER/.venv_local/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/CourtPressGER/.venv_local/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/CourtPressGER/.venv_local/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(text):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Nur Substantive, Verben, Adjektive und Adverbien behalten\u001b[39;00m\n\u001b[32m     43\u001b[39m tokens = [token.lemma_.lower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \n\u001b[32m     44\u001b[39m           \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token.is_stop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token.is_punct \n\u001b[32m     45\u001b[39m           \u001b[38;5;129;01mand\u001b[39;00m token.is_alpha \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token.text) > \u001b[32m2\u001b[39m \n\u001b[32m     46\u001b[39m           \u001b[38;5;129;01mand\u001b[39;00m token.pos_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mNOUN\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVERB\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mADJ\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mADV\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/CourtPressGER/.venv_local/lib/python3.12/site-packages/spacy/language.py:1052\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1050\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1054\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### 2. Semantische Ähnlichkeit mit Embeddings\n",
    "\n",
    "# Bei diesem Ansatz berechnen wir die semantische Ähnlichkeit zwischen Urteil und Pressemitteilung\n",
    "# Pressemitteilungen mit niedriger Ähnlichkeit zum zugehörigen Urteil werden als potenziell irrelevant eingestuft\n",
    "\n",
    "# Wir beginnen mit einem einfachen TF-IDF-basierten Ansatz\n",
    "print(\"\\n## Semantische Ähnlichkeit mit TF-IDF Embeddings\")\n",
    "\n",
    "# TF-IDF Vektorisierung für alle Texte\n",
    "print(\"Berechne TF-IDF Vektoren...\")\n",
    "\n",
    "if USE_GPU:\n",
    "    tfidf_vectorizer = cuTfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        min_df=5,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2),  # Uni- und Bi-gramme\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "else:\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        min_df=5,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2),  # Uni- und Bi-gramme\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "\n",
    "# Preprocessed Texte erstellen\n",
    "print(\"Führe Text-Preprocessing durch...\")\n",
    "df['preprocessed_summary'] = df['summary'].fillna('').apply(preprocess_text)\n",
    "df['preprocessed_judgement'] = df['judgement'].fillna('').apply(preprocess_text)\n",
    "\n",
    "# Kombiniere alle Texte für das Training des Vektorisierers\n",
    "all_texts = list(df['preprocessed_summary']) + list(df['preprocessed_judgement'])\n",
    "tfidf_vectorizer.fit(all_texts)\n",
    "print(f\"Vokabulargröße: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Vektorisiere Urteile und Pressemitteilungen\n",
    "summary_vectors = tfidf_vectorizer.transform(df['preprocessed_summary'])\n",
    "judgement_vectors = tfidf_vectorizer.transform(df['preprocessed_judgement'])\n",
    "\n",
    "# Berechne die Kosinus-Ähnlichkeit für jedes Paar\n",
    "print(\"Berechne Kosinus-Ähnlichkeiten...\")\n",
    "similarities = []\n",
    "\n",
    "# GPU-beschleunigte oder CPU-basierte Berechnung je nach Verfügbarkeit\n",
    "if USE_GPU:\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i < summary_vectors.shape[0] and i < judgement_vectors.shape[0]:\n",
    "            # Für GPU: normalisierte Vektoren und paarweise Ähnlichkeiten mit cuML\n",
    "            summary_vec = cp.sparse.csr_matrix(summary_vectors[i])\n",
    "            judgement_vec = cp.sparse.csr_matrix(judgement_vectors[i])\n",
    "            \n",
    "            # Umwandlung in dichte Arrays (notwendig für einige cuML-Operationen)\n",
    "            summary_dense = summary_vec.toarray().astype(cp.float32)\n",
    "            judgement_dense = judgement_vec.toarray().astype(cp.float32)\n",
    "            \n",
    "            # Normalisierung (für Kosinus-Ähnlichkeit)\n",
    "            if cp.sum(summary_dense) > 0:\n",
    "                summary_dense = normalize(summary_dense)\n",
    "            if cp.sum(judgement_dense) > 0:\n",
    "                judgement_dense = normalize(judgement_dense)\n",
    "            \n",
    "            # Kosinus-Ähnlichkeit = Skalarprodukt der normalisierten Vektoren\n",
    "            similarity = cp.dot(summary_dense.flatten(), judgement_dense.flatten())\n",
    "            similarities.append(float(similarity))\n",
    "        else:\n",
    "            similarities.append(0.0)\n",
    "else:\n",
    "    # Originale CPU-Implementierung\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i < summary_vectors.shape[0] and i < judgement_vectors.shape[0]:\n",
    "            # Extrahiere die Vektoren für das aktuelle Paar\n",
    "            summary_vec = summary_vectors[i]\n",
    "            judgement_vec = judgement_vectors[i]\n",
    "            \n",
    "            # Berechne die Kosinus-Ähnlichkeit\n",
    "            similarity = cosine_similarity(summary_vec, judgement_vec)[0][0]\n",
    "            similarities.append(similarity)\n",
    "        else:\n",
    "            similarities.append(0.0)\n",
    "\n",
    "# Speichere die Ähnlichkeiten im DataFrame\n",
    "df['tfidf_similarity'] = similarities\n",
    "\n",
    "# Analysiere die Ähnlichkeitsverteilung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['tfidf_similarity'], bins=50)\n",
    "plt.title('Verteilung der TF-IDF Kosinus-Ähnlichkeiten')\n",
    "plt.xlabel('Kosinus-Ähnlichkeit')\n",
    "plt.ylabel('Anzahl der Paare')\n",
    "plt.axvline(x=df['tfidf_similarity'].quantile(0.1), color='r', linestyle='--', \n",
    "            label=f'10% Quantil: {df[\"tfidf_similarity\"].quantile(0.1):.3f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Definiere einen Schwellenwert für die Ähnlichkeit\n",
    "# Wir nehmen an, dass die 10% der Paare mit der geringsten Ähnlichkeit \n",
    "# potenziell irrelevante Pressemitteilungen enthalten\n",
    "similarity_threshold = df['tfidf_similarity'].quantile(0.1)\n",
    "print(f\"Schwellenwert für TF-IDF Ähnlichkeit: {similarity_threshold:.3f}\")\n",
    "\n",
    "# Identifiziere Paare mit niedriger Ähnlichkeit\n",
    "df['is_dissimilar_tfidf'] = df['tfidf_similarity'] < similarity_threshold\n",
    "\n",
    "# Auswertung der semantischen Ähnlichkeitsmethode\n",
    "dissimilar_count = df['is_dissimilar_tfidf'].sum()\n",
    "print(f\"TF-IDF Ähnlichkeitserkennung: {dissimilar_count} von {len(df)} Einträgen ({dissimilar_count/len(df)*100:.2f}%) haben geringe Ähnlichkeit.\")\n",
    "\n",
    "# Vergleich mit regelbasierter Methode\n",
    "overlap_count = (df['is_announcement_rule'] & df['is_dissimilar_tfidf']).sum()\n",
    "print(f\"Überlappung mit regelbasierter Methode: {overlap_count} Einträge ({overlap_count/dissimilar_count*100:.2f}% der Einträge mit geringer Ähnlichkeit)\")\n",
    "\n",
    "# Beispiele für Einträge mit niedriger Ähnlichkeit anzeigen\n",
    "print(\"\\nBeispiele für Einträge mit niedriger semantischer Ähnlichkeit:\")\n",
    "dissimilar_examples = df[df['is_dissimilar_tfidf']].sort_values('tfidf_similarity').head(3)\n",
    "for _, row in dissimilar_examples.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Ähnlichkeit: {row['tfidf_similarity']:.4f}\")\n",
    "    print(f\"Pressemitteilung (Anfang): {row['summary'][:200]}...\")\n",
    "    print(f\"Urteil (Anfang): {row['judgement'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Überwachtes Machine Learning\n",
    "\n",
    "Der dritte Ansatz nutzt die Ergebnisse der vorherigen Methoden als \"schwache Labels\" für einen ML-Klassifikator:\n",
    "\n",
    "- **Feature-Engineering**: TF-IDF Vektorisierung der Texte\n",
    "- **Modell**: Logistische Regression mit Klassengewichtung\n",
    "- **Evaluation**: Confusion Matrix und Klassifikationsreport\n",
    "- **Feature-Analyse**: Wichtigste Wörter für beide Klassen\n",
    "\n",
    "Das trainierte Modell lernt komplexere Muster als die regelbasierten Ansätze und kann diese auf neue Texte anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Überwachtes Machine Learning (Klassifikation)\n",
    "\n",
    "# In diesem Ansatz trainieren wir ein Modell, das auf Basis von Texteigenschaften \n",
    "# automatisch zwischen relevanten und irrelevanten Pressemitteilungen unterscheidet\n",
    "\n",
    "print(\"\\n## Überwachtes Machine Learning (Klassifikation)\")\n",
    "\n",
    "# Da wir keine manuell gelabelten Daten haben, nutzen wir die Ergebnisse\n",
    "# der vorherigen Methoden als \"schwache\" Labels für das Training\n",
    "print(\"Erstelle Labels für das Trainieren basierend auf regelbasierter Methode...\")\n",
    "\n",
    "# Wir betrachten Einträge als \"irrelevant\" (Label 1), wenn sie entweder\n",
    "# durch die regelbasierte Methode als Ankündigungen erkannt wurden\n",
    "# oder eine sehr niedrige semantische Ähnlichkeit aufweisen\n",
    "df['is_irrelevant'] = ((df['is_announcement_rule']) | \n",
    "                       (df['tfidf_similarity'] < similarity_threshold * 0.8))\n",
    "\n",
    "# Balancieren der Daten (optional)\n",
    "irrelevant_count = df['is_irrelevant'].sum()\n",
    "print(f\"Anzahl der als irrelevant markierten Einträge: {irrelevant_count} ({irrelevant_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Features für das Training erstellen (TF-IDF auf den Pressemitteilungen)\n",
    "print(\"Erstelle Features für das Klassifikationsmodell...\")\n",
    "classifier_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Wir nutzen die Originaltexte (nicht die vorverarbeiteten)\n",
    "# um auch strukturelle Merkmale zu erfassen\n",
    "X = classifier_vectorizer.fit_transform(df['summary'].fillna(''))\n",
    "y = df['is_irrelevant'].astype(int)\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Trainingsdaten: {X_train.shape[0]} Einträge, Testdaten: {X_test.shape[0]} Einträge\")\n",
    "print(f\"Klassen-Verteilung im Training: {Counter(y_train)}\")\n",
    "\n",
    "# Logistische Regression für Klassifikation\n",
    "print(\"Trainiere Logistic Regression Klassifikator...\")\n",
    "\n",
    "if USE_GPU:\n",
    "    # GPU-beschleunigte Logistische Regression\n",
    "    classifier = cuLogisticRegression(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    # CPU-basierte Logistische Regression\n",
    "    classifier = LogisticRegression(\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Modell evaluieren\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Zeige Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Klassifikationsreport zeigen\n",
    "print(\"\\nKlassifikationsreport:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Die wichtigsten Merkmale für jede Klasse anzeigen\n",
    "print(\"\\nWichtigste Merkmale für die Erkennung von irrelevanten Pressemitteilungen:\")\n",
    "if hasattr(classifier, 'coef_'):\n",
    "    feature_names = classifier_vectorizer.get_feature_names_out()\n",
    "    # Sortiere nach absoluten Koeffizienten (größte zuerst)\n",
    "    sorted_coef_indices = np.argsort(np.abs(classifier.coef_[0]))[::-1]\n",
    "    \n",
    "    # Top N positive und negative Features\n",
    "    n_features = 20\n",
    "    \n",
    "    # Features, die für \"irrelevant\" sprechen (positive Koeffizienten)\n",
    "    print(\"Features, die für 'irrelevant' sprechen:\")\n",
    "    pos_indices = [idx for idx in sorted_coef_indices if classifier.coef_[0][idx] > 0][:n_features]\n",
    "    for idx in pos_indices:\n",
    "        print(f\"  {feature_names[idx]}: {classifier.coef_[0][idx]:.4f}\")\n",
    "    \n",
    "    # Features, die für \"relevant\" sprechen (negative Koeffizienten)\n",
    "    print(\"\\nFeatures, die für 'relevant' sprechen:\")\n",
    "    neg_indices = [idx for idx in sorted_coef_indices if classifier.coef_[0][idx] < 0][:n_features]\n",
    "    for idx in neg_indices:\n",
    "        print(f\"  {feature_names[idx]}: {classifier.coef_[0][idx]:.4f}\")\n",
    "\n",
    "# Anwenden des Modells auf den gesamten Datensatz\n",
    "print(\"\\nWende trainiertes Modell auf den gesamten Datensatz an...\")\n",
    "df['irrelevant_prob'] = classifier.predict_proba(X)[:, 1]\n",
    "df['is_irrelevant_ml'] = classifier.predict(X)\n",
    "\n",
    "# Auswertung der ML-Methode\n",
    "ml_irrelevant_count = df['is_irrelevant_ml'].sum()\n",
    "print(f\"ML-Klassifikator: {ml_irrelevant_count} von {len(df)} Einträgen ({ml_irrelevant_count/len(df)*100:.2f}%) wurden als irrelevant klassifiziert.\")\n",
    "\n",
    "# Vergleich mit vorherigen Methoden\n",
    "print(\"\\nÜberlappung mit anderen Methoden:\")\n",
    "print(f\"- Mit regelbasierter Methode: {(df['is_announcement_rule'] & df['is_irrelevant_ml']).sum()} Einträge\")\n",
    "print(f\"- Mit TF-IDF Ähnlichkeit: {(df['is_dissimilar_tfidf'] & df['is_irrelevant_ml']).sum()} Einträge\")\n",
    "\n",
    "# Visualisierung der Wahrscheinlichkeiten\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['irrelevant_prob'], bins=50, kde=True)\n",
    "plt.title('Verteilung der ML-Wahrscheinlichkeiten für \"irrelevant\"')\n",
    "plt.xlabel('Wahrscheinlichkeit')\n",
    "plt.ylabel('Anzahl der Einträge')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Beispiele für als irrelevant klassifizierte Einträge anzeigen\n",
    "print(\"\\nBeispiele für als irrelevant klassifizierte Einträge:\")\n",
    "high_confidence_irrelevant = df[df['is_irrelevant_ml'] & (df['irrelevant_prob'] > 0.9)].sort_values('irrelevant_prob', ascending=False).head(3)\n",
    "for _, row in high_confidence_irrelevant.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Irrelevanz-Wahrscheinlichkeit: {row['irrelevant_prob']:.4f}\")\n",
    "    print(f\"Regelbasiert: {'Ja' if row['is_announcement_rule'] else 'Nein'}, TF-IDF unähnlich: {'Ja' if row['is_dissimilar_tfidf'] else 'Nein'}\")\n",
    "    print(f\"Pressemitteilung (Anfang): {row['summary'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unüberwachtes Clustering\n",
    "\n",
    "Der vierte Ansatz gruppiert die Pressemitteilungen basierend auf ihrer inhaltlichen Ähnlichkeit:\n",
    "\n",
    "- **Dimensionsreduktion**: PCA für effizienteres Clustering\n",
    "- **K-Means**: Clustering in verschiedene Themenbereiche\n",
    "- **Cluster-Analyse**: Identifikation von Ankündigungs-Clustern\n",
    "- **Validierung**: Vergleich mit anderen Methoden\n",
    "\n",
    "Diese Methode hilft dabei, natürliche Gruppierungen in den Daten zu finden und potenzielle Ankündigungscluster zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Unüberwachte Verfahren (Clustering/Topic Modeling)\n",
    "\n",
    "# In diesem Ansatz gruppieren wir die Pressemitteilungen anhand ihrer Ähnlichkeit,\n",
    "# um Cluster zu identifizieren, die potenziell Ankündigungen oder irrelevante Inhalte enthalten\n",
    "\n",
    "print(\"\\n## Unüberwachte Verfahren (Clustering/Topic Modeling)\")\n",
    "\n",
    "# Wir nutzen die bereits berechneten TF-IDF Vektoren der Pressemitteilungen\n",
    "print(\"Verwende TF-IDF Vektoren für Clustering...\")\n",
    "\n",
    "# Dimensionsreduktion mit PCA und K-Means Clustering\n",
    "print(\"Reduziere Dimensionalität mit PCA und führe Clustering durch...\")\n",
    "\n",
    "if USE_GPU:\n",
    "    # GPU-beschleunigte PCA\n",
    "    pca = cuPCA(n_components=50, copy=True)\n",
    "    summary_vectors_dense = summary_vectors.toarray().astype(cp.float32)\n",
    "    summary_vectors_reduced = pca.fit_transform(summary_vectors_dense)\n",
    "    \n",
    "    # GPU-beschleunigtes K-Means\n",
    "    n_clusters = 5  # Wir probieren 5 Cluster\n",
    "    kmeans = cuKMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(summary_vectors_reduced)\n",
    "else:\n",
    "    # CPU-basierte PCA\n",
    "    pca = PCA(n_components=50)  # Wir reduzieren auf 50 Dimensionen\n",
    "    summary_vectors_dense = summary_vectors.toarray()\n",
    "    summary_vectors_reduced = pca.fit_transform(summary_vectors_dense)\n",
    "    \n",
    "    # CPU-basiertes K-Means\n",
    "    n_clusters = 5  # Wir probieren 5 Cluster\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(summary_vectors_reduced)\n",
    "\n",
    "# Speichere die Cluster-Labels im DataFrame\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"Erklärte Varianz durch PCA: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# Visualisierung der Cluster (auf 2 Dimensionen reduziert für die Darstellung)\n",
    "print(\"Reduziere auf 2 Dimensionen für Visualisierung...\")\n",
    "pca_vis = PCA(n_components=2)\n",
    "summary_vectors_2d = pca_vis.fit_transform(summary_vectors_reduced)\n",
    "\n",
    "# Plotte die Cluster\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(summary_vectors_2d[:, 0], summary_vectors_2d[:, 1], \n",
    "                      c=cluster_labels, cmap='viridis', alpha=0.7, s=30)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('K-Means Clustering der Pressemitteilungen (PCA-reduziert)')\n",
    "plt.xlabel('PCA Komponente 1')\n",
    "plt.ylabel('PCA Komponente 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analysiere die Cluster: Welche Cluster enthalten mehr regelbasiert erkannte Ankündigungen?\n",
    "print(\"\\nAnalyse der Cluster im Vergleich zu regelbasierten Erkennungen:\")\n",
    "cluster_analysis = df.groupby('cluster')['is_announcement_rule'].mean().sort_values(ascending=False)\n",
    "print(\"Anteil der Ankündigungen in jedem Cluster (regelbasiert):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Bestimme den Cluster mit dem höchsten Anteil an Ankündigungen\n",
    "announcement_cluster = cluster_analysis.index[0]\n",
    "print(f\"\\nCluster {announcement_cluster} hat den höchsten Anteil an potenziellen Ankündigungen ({cluster_analysis[0]:.2%})\")\n",
    "\n",
    "# Extrahiere die wichtigsten Wörter für jeden Cluster\n",
    "print(\"\\nWichtigste Wörter pro Cluster:\")\n",
    "# Verwende den TF-IDF Vektorisierer, um die wichtigsten Wörter zu identifizieren\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Für jeden Cluster die Top-Wörter basierend auf den Cluster-Zentroiden bestimmen\n",
    "for i in range(n_clusters):\n",
    "    # Hole die Indizes der Dokumente in diesem Cluster\n",
    "    cluster_docs = df[df['cluster'] == i].index\n",
    "    \n",
    "    if len(cluster_docs) > 0:\n",
    "        # Berechne die durchschnittlichen TF-IDF-Werte für diesen Cluster\n",
    "        cluster_tfidf = summary_vectors[cluster_docs].mean(axis=0)\n",
    "        cluster_tfidf = np.asarray(cluster_tfidf).flatten()\n",
    "        \n",
    "        # Sortiere nach TF-IDF-Werten und hole die Top-Wörter\n",
    "        top_indices = np.argsort(cluster_tfidf)[::-1][:20]\n",
    "        top_words = [feature_names[idx] for idx in top_indices]\n",
    "        \n",
    "        # Zeige die Top-Wörter an\n",
    "        print(f\"Cluster {i} (n={len(cluster_docs)}) - Top-Wörter: {', '.join(top_words[:10])}\")\n",
    "        \n",
    "        # Einige Beispiele aus dem Cluster anzeigen\n",
    "        if i == announcement_cluster:\n",
    "            print(\"\\nBeispiele aus dem erkannten 'Ankündigungs'-Cluster:\")\n",
    "            cluster_examples = df[df['cluster'] == i].sample(min(3, len(df[df['cluster'] == i]))).reset_index(drop=True)\n",
    "            for idx, row in cluster_examples.iterrows():\n",
    "                print(f\"Beispiel {idx+1}:\")\n",
    "                print(f\"ID: {row['id']}\")\n",
    "                print(f\"Regelbasiert als Ankündigung erkannt: {'Ja' if row['is_announcement_rule'] else 'Nein'}\")\n",
    "                print(f\"Pressemitteilung (Anfang): {row['summary'][:200]}...\")\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "# Vergleiche Cluster-basierte Erkennung mit anderen Methoden\n",
    "# Wir betrachten den identifizierten Ankündigungs-Cluster als potenziell irrelevant\n",
    "df['is_irrelevant_cluster'] = df['cluster'] == announcement_cluster\n",
    "\n",
    "# Auswertung der Cluster-Methode\n",
    "cluster_irrelevant_count = df['is_irrelevant_cluster'].sum()\n",
    "print(f\"\\nCluster-Methode: {cluster_irrelevant_count} von {len(df)} Einträgen ({cluster_irrelevant_count/len(df)*100:.2f}%) im Ankündigungs-Cluster\")\n",
    "\n",
    "# Überlappung mit anderen Methoden\n",
    "print(\"\\nÜberlappung mit anderen Methoden:\")\n",
    "print(f\"- Mit regelbasierter Methode: {(df['is_announcement_rule'] & df['is_irrelevant_cluster']).sum()} Einträge\")\n",
    "print(f\"- Mit TF-IDF Ähnlichkeit: {(df['is_dissimilar_tfidf'] & df['is_irrelevant_cluster']).sum()} Einträge\")\n",
    "print(f\"- Mit ML-Klassifikator: {(df['is_irrelevant_ml'] & df['is_irrelevant_cluster']).sum()} Einträge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vergleich und Kombination der Methoden\n",
    "\n",
    "Im letzten Abschnitt vergleichen wir die Ergebnisse aller Methoden und kombinieren sie zu einem robusten Gesamtansatz:\n",
    "\n",
    "- **Methodenvergleich**: Überlappung und Unterschiede\n",
    "- **Ensemble-Ansatz**: Kombination durch Mehrheitsentscheidung\n",
    "- **Grenzfälle**: Analyse von Beispielen mit unterschiedlichen Klassifikationen\n",
    "- **Finaler Datensatz**: Bereinigung basierend auf kombiniertem Ansatz\n",
    "\n",
    "Der finale bereinigte Datensatz enthält nur noch Pressemitteilungen, die von der Mehrheit der Methoden als relevant eingestuft wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zusammenfassung und Vergleich der Methoden\n",
    "\n",
    "# Nach der Implementierung aller vier Ansätze vergleichen wir nun die Ergebnisse\n",
    "\n",
    "print(\"\\n## Vergleich der verschiedenen Bereinigungsansätze\")\n",
    "\n",
    "# Erstelle einen Vergleich der verschiedenen Methoden\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Methode': [\n",
    "        '1. Regelbasiert (Keywords/Patterns)',\n",
    "        '2. Semantische Ähnlichkeit (TF-IDF)',\n",
    "        '3. ML-Klassifikation',\n",
    "        '4. Clustering'\n",
    "    ],\n",
    "    'Anzahl irrelevanter Einträge': [\n",
    "        df['is_announcement_rule'].sum(),\n",
    "        df['is_dissimilar_tfidf'].sum(),\n",
    "        df['is_irrelevant_ml'].sum(),\n",
    "        df['is_irrelevant_cluster'].sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Berechne den Prozentsatz\n",
    "comparison_df['Prozent vom Datensatz'] = comparison_df['Anzahl irrelevanter Einträge'] / len(df) * 100\n",
    "\n",
    "# Zeige Vergleichstabelle\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Überlappung der Methoden visualisieren\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Venn-Diagramm Daten vorbereiten\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "\n",
    "# Erstelle Sets der irrelevanten Einträge für jede Methode\n",
    "rule_based_set = set(df[df['is_announcement_rule']].index)\n",
    "similarity_set = set(df[df['is_dissimilar_tfidf']].index)\n",
    "ml_set = set(df[df['is_irrelevant_ml']].index)\n",
    "\n",
    "# Venn-Diagramm zeichnen\n",
    "venn = venn3(\n",
    "    [rule_based_set, similarity_set, ml_set],\n",
    "    ('Regelbasiert', 'Semantische Ähnlichkeit', 'ML-Klassifikation')\n",
    ")\n",
    "plt.title('Überlappung der erkannten irrelevanten Einträge')\n",
    "plt.show()\n",
    "\n",
    "# Kombinierter Ansatz: Einträge identifizieren, die von mindestens 2 Methoden als irrelevant eingestuft wurden\n",
    "print(\"\\n## Kombinierter Bereinigungsansatz\")\n",
    "\n",
    "# Zähle, von wie vielen Methoden ein Eintrag als irrelevant eingestuft wurde\n",
    "df['irrelevant_votes'] = (\n",
    "    df['is_announcement_rule'].astype(int) +\n",
    "    df['is_dissimilar_tfidf'].astype(int) +\n",
    "    df['is_irrelevant_ml'].astype(int) +\n",
    "    df['is_irrelevant_cluster'].astype(int)\n",
    ")\n",
    "\n",
    "# Analysiere die Verteilung der Stimmen\n",
    "vote_counts = df['irrelevant_votes'].value_counts().sort_index()\n",
    "print(\"Verteilung der 'irrelevant'-Stimmen:\")\n",
    "for votes, count in vote_counts.items():\n",
    "    print(f\"{votes} Methode(n): {count} Einträge ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Wir betrachten Einträge als irrelevant, wenn mindestens 2 Methoden sie als irrelevant eingestuft haben\n",
    "vote_threshold = 2\n",
    "df['is_irrelevant_combined'] = df['irrelevant_votes'] >= vote_threshold\n",
    "\n",
    "# Auswertung des kombinierten Ansatzes\n",
    "combined_irrelevant_count = df['is_irrelevant_combined'].sum()\n",
    "print(f\"\\nKombinierter Ansatz (≥{vote_threshold} Methoden): {combined_irrelevant_count} von {len(df)} Einträgen ({combined_irrelevant_count/len(df)*100:.2f}%) als irrelevant identifiziert.\")\n",
    "\n",
    "# Beispiele für Einträge, die von allen Methoden als irrelevant eingestuft wurden\n",
    "full_agreement = df[df['irrelevant_votes'] == 4].head(3)\n",
    "print(\"\\nBeispiele für Einträge, die von ALLEN Methoden als irrelevant eingestuft wurden:\")\n",
    "for _, row in full_agreement.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Pressemitteilung (Anfang): {row['summary'][:200]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Beispiele für Grenzfälle (genau am Schwellenwert)\n",
    "borderline_cases = df[df['irrelevant_votes'] == vote_threshold].head(3)\n",
    "print(f\"\\nBeispiele für Grenzfälle (genau {vote_threshold} Methoden):\")\n",
    "for _, row in borderline_cases.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Regelbasiert: {'Ja' if row['is_announcement_rule'] else 'Nein'}, \"\n",
    "          f\"TF-IDF unähnlich: {'Ja' if row['is_dissimilar_tfidf'] else 'Nein'}, \"\n",
    "          f\"ML: {'Ja' if row['is_irrelevant_ml'] else 'Nein'}, \"\n",
    "          f\"Cluster: {'Ja' if row['is_irrelevant_cluster'] else 'Nein'}\")\n",
    "    print(f\"Pressemitteilung (Anfang): {row['summary'][:200]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "## Anwendung der kombinierten Methode auf den Datensatz\n",
    "\n",
    "# Bereinigten Datensatz erstellen\n",
    "cleaned_df = df[~df['is_irrelevant_combined']].copy()\n",
    "print(f\"\\nBereinigter Datensatz: {len(cleaned_df)} Einträge (von ursprünglich {len(df)})\")\n",
    "print(f\"Entfernt: {len(df) - len(cleaned_df)} Einträge ({(len(df) - len(cleaned_df))/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Speichern des bereinigten Datensatzes\n",
    "output_dir = Path('cleaned_data')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Speichere den bereinigten Datensatz\n",
    "cleaned_output_file = output_dir / \"cleaned_combined_methods.csv\"\n",
    "cleaned_df.to_csv(cleaned_output_file, index=False)\n",
    "print(f\"Bereinigter Datensatz gespeichert unter: {cleaned_output_file}\")\n",
    "\n",
    "# Speichere auch die Metadaten der Bereinigung\n",
    "metadata_df = df[['id', 'is_announcement_rule', 'tfidf_similarity', 'is_dissimilar_tfidf', \n",
    "                 'is_irrelevant_ml', 'irrelevant_prob', 'cluster', 'is_irrelevant_cluster',\n",
    "                 'irrelevant_votes', 'is_irrelevant_combined']]\n",
    "metadata_output_file = output_dir / \"cleaning_metadata.csv\"\n",
    "metadata_df.to_csv(metadata_output_file, index=False)\n",
    "print(f\"Bereinigungsmetadaten gespeichert unter: {metadata_output_file}\")\n",
    "\n",
    "print(\"\\nBereinigungsprozess abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
