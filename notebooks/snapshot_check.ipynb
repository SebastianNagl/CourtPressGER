{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b76032",
   "metadata": {},
   "source": [
    "# Synthetic Prompts Checkpoint Analyzer\n",
    "\n",
    "This notebook analyzes the checkpoint files created during the synthetic prompt generation process to verify the process is working as expected. Since synthetic prompt generation can be computationally expensive and time-consuming, this analysis helps monitor progress and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251d039",
   "metadata": {},
   "source": [
    "## Load Checkpoint Files\n",
    "\n",
    "The checkpoint directory contains CSV files of synthetic prompts at different stages of generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80214c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint directory path (relative to project root)\n",
    "checkpoint_dir = '../checkpoints/'\n",
    "\n",
    "# Find all CSV files in the checkpoint directory\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'synthetic_prompts_*.csv'))\n",
    "\n",
    "# Sort files by the numeric value in their filename\n",
    "checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "# Display found checkpoint files\n",
    "print(f\"Found {len(checkpoint_files)} checkpoint files:\")\n",
    "for file in checkpoint_files:\n",
    "    print(f\" - {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43439ddf",
   "metadata": {},
   "source": [
    "## Analyze Checkpoint Growth\n",
    "\n",
    "Check how the number of synthetic prompts grows across checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store checkpoint info\n",
    "checkpoint_stats = {}\n",
    "\n",
    "# Load each checkpoint file and extract basic statistics\n",
    "for file in checkpoint_files:\n",
    "    checkpoint_num = int(file.split('_')[-1].split('.')[0])\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Store stats\n",
    "    checkpoint_stats[checkpoint_num] = {\n",
    "        'num_prompts': len(df),\n",
    "        'file_size_kb': os.path.getsize(file) / 1024,\n",
    "        'data': df\n",
    "    }\n",
    "\n",
    "# Create a dataframe of checkpoint statistics\n",
    "stats_df = pd.DataFrame({\n",
    "    'checkpoint': list(checkpoint_stats.keys()),\n",
    "    'num_prompts': [stats['num_prompts'] for stats in checkpoint_stats.values()],\n",
    "    'file_size_kb': [stats['file_size_kb'] for stats in checkpoint_stats.values()]\n",
    "})\n",
    "\n",
    "# Display the stats\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e39708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the growth of prompts across checkpoints\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.set_xlabel('Checkpoint Number')\n",
    "ax1.set_ylabel('Number of Prompts', color='tab:blue')\n",
    "ax1.plot(stats_df['checkpoint'], stats_df['num_prompts'], marker='o', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()  # Create a second y-axis\n",
    "ax2.set_ylabel('File Size (KB)', color='tab:red')\n",
    "ax2.plot(stats_df['checkpoint'], stats_df['file_size_kb'], marker='s', color='tab:red')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "plt.title('Checkpoint Growth Analysis')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd727f4",
   "metadata": {},
   "source": [
    "## Analyze Prompt Content\n",
    "\n",
    "Examine the latest checkpoint file in detail to check the quality of generated prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02811ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest checkpoint file\n",
    "latest_checkpoint_num = max(checkpoint_stats.keys())\n",
    "latest_df = checkpoint_stats[latest_checkpoint_num]['data']\n",
    "\n",
    "# Display basic info about the latest checkpoint\n",
    "print(f\"Latest checkpoint: {latest_checkpoint_num}\")\n",
    "print(f\"Number of prompts: {len(latest_df)}\")\n",
    "print(f\"\\nColumns in the dataset:\")\n",
    "for col in latest_df.columns:\n",
    "    print(f\" - {col}\")\n",
    "\n",
    "# Display the first few rows of the latest checkpoint\n",
    "latest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prompt length distribution\n",
    "if 'prompt' in latest_df.columns:\n",
    "    latest_df['prompt_length'] = latest_df['prompt'].apply(len)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(latest_df['prompt_length'], kde=True)\n",
    "    plt.title('Distribution of Prompt Lengths')\n",
    "    plt.xlabel('Character Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(latest_df['prompt_length'].mean(), color='red', linestyle='--', label=f'Mean: {latest_df[\"prompt_length\"].mean():.1f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Minimum prompt length: {latest_df['prompt_length'].min()}\")\n",
    "    print(f\"Maximum prompt length: {latest_df['prompt_length'].max()}\")\n",
    "    print(f\"Average prompt length: {latest_df['prompt_length'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff54ea6",
   "metadata": {},
   "source": [
    "## Sample Prompts\n",
    "\n",
    "Review a random sample of prompts from the latest checkpoint to manually assess quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595350d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random samples from the latest checkpoint\n",
    "sample_size = min(5, len(latest_df))\n",
    "random_samples = latest_df.sample(sample_size)\n",
    "\n",
    "print(f\"Showing {sample_size} random prompts from the latest checkpoint:\")\n",
    "for i, (_, row) in enumerate(random_samples.iterrows(), 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    if 'prompt' in row:\n",
    "        print(f\"Prompt: {row['prompt']}\")\n",
    "    \n",
    "    # Print any other relevant columns\n",
    "    other_cols = [col for col in row.index if col != 'prompt' and not pd.isna(row[col])]\n",
    "    for col in other_cols:\n",
    "        print(f\"{col}: {row[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130a343",
   "metadata": {},
   "source": [
    "## Checkpoint Comparison\n",
    "\n",
    "If multiple checkpoints exist, compare how the content has evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prompt length distribution across checkpoints if we have multiple\n",
    "if len(checkpoint_stats) > 1:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for checkpoint_num, stats in checkpoint_stats.items():\n",
    "        df = stats['data']\n",
    "        if 'prompt' in df.columns:\n",
    "            df['prompt_length'] = df['prompt'].apply(len)\n",
    "            sns.kdeplot(df['prompt_length'], label=f'Checkpoint {checkpoint_num}')\n",
    "    \n",
    "    plt.title('Prompt Length Distribution Across Checkpoints')\n",
    "    plt.xlabel('Character Count')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfef22a",
   "metadata": {},
   "source": [
    "## Cost Analysis\n",
    "\n",
    "If the checkpoint contains information about token counts or API calls, analyze the cost implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have token count information to estimate costs\n",
    "token_columns = [col for col in latest_df.columns if 'token' in col.lower()]\n",
    "\n",
    "if token_columns:\n",
    "    print(\"Token-related columns found. Performing cost analysis...\")\n",
    "    \n",
    "    # Assuming 'input_tokens' and 'output_tokens' columns exist and using hypothetical pricing\n",
    "    if 'input_tokens' in latest_df.columns and 'output_tokens' in latest_df.columns:\n",
    "        # Example pricing - adjust based on your actual model and pricing\n",
    "        INPUT_TOKEN_PRICE = 0.0015 / 1000  # $0.0015 per 1000 tokens\n",
    "        OUTPUT_TOKEN_PRICE = 0.002 / 1000  # $0.002 per 1000 tokens\n",
    "        \n",
    "        total_input_tokens = latest_df['input_tokens'].sum()\n",
    "        total_output_tokens = latest_df['output_tokens'].sum()\n",
    "        \n",
    "        input_cost = total_input_tokens * INPUT_TOKEN_PRICE\n",
    "        output_cost = total_output_tokens * OUTPUT_TOKEN_PRICE\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        print(f\"Total input tokens: {total_input_tokens:,}\")\n",
    "        print(f\"Total output tokens: {total_output_tokens:,}\")\n",
    "        print(f\"Estimated input cost: ${input_cost:.2f}\")\n",
    "        print(f\"Estimated output cost: ${output_cost:.2f}\")\n",
    "        print(f\"Estimated total cost: ${total_cost:.2f}\")\n",
    "else:\n",
    "    print(\"No token-related columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7ca7a",
   "metadata": {},
   "source": [
    "## Generation Progress Rate\n",
    "\n",
    "Analyze how quickly prompts are being generated based on checkpoint timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get file modification times to estimate generation rate\n",
    "timestamps = {}\n",
    "for file in checkpoint_files:\n",
    "    checkpoint_num = int(file.split('_')[-1].split('.')[0])\n",
    "    timestamps[checkpoint_num] = os.path.getmtime(file)\n",
    "\n",
    "if len(timestamps) > 1:\n",
    "    checkpoint_nums = sorted(timestamps.keys())\n",
    "    \n",
    "    # Calculate time differences and generation rates\n",
    "    time_diffs = []\n",
    "    rates = []\n",
    "    \n",
    "    for i in range(1, len(checkpoint_nums)):\n",
    "        prev_num = checkpoint_nums[i-1]\n",
    "        curr_num = checkpoint_nums[i]\n",
    "        \n",
    "        time_diff = timestamps[curr_num] - timestamps[prev_num]  # in seconds\n",
    "        prompt_diff = checkpoint_stats[curr_num]['num_prompts'] - checkpoint_stats[prev_num]['num_prompts']\n",
    "        \n",
    "        if time_diff > 0:\n",
    "            rate = prompt_diff / (time_diff / 3600)  # prompts per hour\n",
    "            time_diffs.append(time_diff / 3600)  # convert to hours\n",
    "            rates.append(rate)\n",
    "    \n",
    "    if rates:\n",
    "        # Calculate average generation rate\n",
    "        avg_rate = sum(rates) / len(rates)\n",
    "        \n",
    "        # Estimate time to generate 1000 more prompts\n",
    "        time_for_1000 = 1000 / avg_rate\n",
    "        \n",
    "        print(f\"Average generation rate: {avg_rate:.2f} prompts/hour\")\n",
    "        print(f\"Estimated time to generate 1000 more prompts: {time_for_1000:.2f} hours\")\n",
    "        \n",
    "        # Visualize generation rate\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(rates)), rates)\n",
    "        plt.axhline(avg_rate, color='red', linestyle='--', label=f'Average: {avg_rate:.2f}')\n",
    "        plt.xlabel('Checkpoint Transition')\n",
    "        plt.ylabel('Generation Rate (prompts/hour)')\n",
    "        plt.title('Synthetic Prompt Generation Rate')\n",
    "        plt.xticks(range(len(rates)), [f'{checkpoint_nums[i]}-{checkpoint_nums[i+1]}' for i in range(len(rates))])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Not enough checkpoints to analyze generation rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93695d0",
   "metadata": {},
   "source": [
    "## Checking for Duplicates\n",
    "\n",
    "Verify that synthetic prompts are unique across checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the latest checkpoint\n",
    "if 'prompt' in latest_df.columns:\n",
    "    duplicate_count = latest_df.duplicated(subset=['prompt']).sum()\n",
    "    duplicate_percent = (duplicate_count / len(latest_df)) * 100 if len(latest_df) > 0 else 0\n",
    "    \n",
    "    print(f\"Number of duplicate prompts in latest checkpoint: {duplicate_count}\")\n",
    "    print(f\"Percentage of duplicates: {duplicate_percent:.2f}%\")\n",
    "\n",
    "    # Check for near-duplicates using a simple similarity metric\n",
    "    if len(latest_df) > 1:\n",
    "        print(\"\\nChecking for near-duplicates (this might take a while for large datasets)...\")\n",
    "        \n",
    "        # For demonstration, we'll just check a sample of prompts\n",
    "        sample_size = min(100, len(latest_df))\n",
    "        prompt_sample = latest_df['prompt'].sample(sample_size).tolist()\n",
    "        \n",
    "        import difflib\n",
    "        similarity_threshold = 0.9\n",
    "        near_duplicate_pairs = []\n",
    "        \n",
    "        for i in range(len(prompt_sample)):\n",
    "            for j in range(i+1, len(prompt_sample)):\n",
    "                similarity = difflib.SequenceMatcher(None, prompt_sample[i], prompt_sample[j]).ratio()\n",
    "                if similarity > similarity_threshold:\n",
    "                    near_duplicate_pairs.append((i, j, similarity))\n",
    "        \n",
    "        if near_duplicate_pairs:\n",
    "            print(f\"Found {len(near_duplicate_pairs)} potential near-duplicate pairs in the sample:\")\n",
    "            for i, j, similarity in near_duplicate_pairs[:3]:  # Show just a few examples\n",
    "                print(f\"\\nSimilarity: {similarity:.2f}\")\n",
    "                print(f\"Prompt 1: {prompt_sample[i][:100]}...\")\n",
    "                print(f\"Prompt 2: {prompt_sample[j][:100]}...\")\n",
    "            if len(near_duplicate_pairs) > 3:\n",
    "                print(f\"...and {len(near_duplicate_pairs) - 3} more pairs.\")\n",
    "        else:\n",
    "            print(\"No near-duplicates found in the sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411e0a6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Summarize the findings from the checkpoint analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the findings\n",
    "print(\"## Synthetic Prompts Checkpoint Analysis Summary\")\n",
    "\n",
    "print(f\"\\n### Process Status:\")\n",
    "print(f\"- Total checkpoints found: {len(checkpoint_stats)}\")\n",
    "if len(checkpoint_stats) > 0:\n",
    "    print(f\"- Latest checkpoint: {max(checkpoint_stats.keys())}\")\n",
    "    print(f\"- Total prompts generated so far: {checkpoint_stats[max(checkpoint_stats.keys())]['num_prompts']}\")\n",
    "\n",
    "if len(checkpoint_stats) > 1:\n",
    "    print(f\"\\n### Generation Progress:\")\n",
    "    first_checkpoint = min(checkpoint_stats.keys())\n",
    "    last_checkpoint = max(checkpoint_stats.keys())\n",
    "    total_growth = checkpoint_stats[last_checkpoint]['num_prompts'] - checkpoint_stats[first_checkpoint]['num_prompts']\n",
    "    print(f\"- Prompt growth: +{total_growth} prompts since first checkpoint\")\n",
    "    if 'avg_rate' in locals() and avg_rate > 0:\n",
    "        print(f\"- Average generation rate: {avg_rate:.2f} prompts/hour\")\n",
    "        print(f\"- Estimated completion time for 1000 more prompts: {time_for_1000:.2f} hours\")\n",
    "\n",
    "print(f\"\\n### Data Quality:\")\n",
    "if 'duplicate_count' in locals():\n",
    "    print(f\"- Duplicate prompts: {duplicate_count} ({duplicate_percent:.2f}%)\")\n",
    "\n",
    "if 'token_columns' in locals() and token_columns:\n",
    "    print(f\"\\n### Resource Usage:\")\n",
    "    if 'total_cost' in locals():\n",
    "        print(f\"- Estimated cost so far: ${total_cost:.2f}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "if len(checkpoint_stats) < 2:\n",
    "    print(\"- Create more checkpoints to enable better progress tracking\")\n",
    "elif 'duplicate_count' in locals() and duplicate_percent > 5:\n",
    "    print(\"- Review generation process to reduce duplicates\")\n",
    "elif 'avg_rate' in locals() and avg_rate < 10:\n",
    "    print(\"- Consider optimization to improve generation rate\")\n",
    "else:\n",
    "    print(\"- Process appears to be running as expected\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
