{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67517895",
   "metadata": {},
   "source": [
    "# Generierung synthetischer Prompts für Gerichtsurteile und Pressemitteilungen\n",
    "\n",
    "Dieses Notebook verwendet die Claude API, um synthetische Prompts für die Pressemitteilungen - Gerichtsurteilen Paare zu erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e366f61",
   "metadata": {},
   "source": [
    "## 1. Setup und Datensatz-Laden\n",
    "\n",
    "Zuerst importieren wir die notwendigen Bibliotheken und laden den bereinigten Datensatz aus `bereinigung.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d3f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import anthropic\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aab7155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bereinigter Datensatz mit 6432 Einträgen geladen\n"
     ]
    }
   ],
   "source": [
    "# Lade den bereinigten Datensatz\n",
    "try:\n",
    "    df = pd.read_csv('../data/interim/cleaned.csv')\n",
    "    print(f\"Bereinigter Datensatz mit {len(df)} Einträgen geladen\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Bereinigter Datensatz nicht gefunden.\")\n",
    "\n",
    "# Stichprobe des Datensatzes für Tests (für Produktionseinsatz auskommentiert)\n",
    "# df = df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c534023",
   "metadata": {},
   "source": [
    "## 2. Setup Claude API-Zugang\n",
    "\n",
    "Als nächstes richten wir die Verbindung zur Claude API ein. Wir verwenden den Anthropic Python-Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78c54166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: /Users/sebastiannagl/Desktop/Code/CourtPressGER/.env\n",
      "Set environment variable: ANTHROPIC_API_KEY\n",
      "API key found: Yes\n",
      "API-Schlüssel aus der Umgebung geladen!\n",
      "API-Verbindung erfolgreich!\n",
      "Claude sagt: Hallo! Schön, von dir zu hören.\n",
      "Currently available models:\n",
      "- claude-3-7-sonnet-20250219: Claude 3.7 Sonnet\n",
      "- claude-3-5-sonnet-20241022: Claude 3.5 Sonnet (New)\n",
      "- claude-3-5-haiku-20241022: Claude 3.5 Haiku\n",
      "- claude-3-5-sonnet-20240620: Claude 3.5 Sonnet (Old)\n",
      "- claude-3-haiku-20240307: Claude 3 Haiku\n",
      "- claude-3-opus-20240229: Claude 3 Opus\n"
     ]
    }
   ],
   "source": [
    "# Load the .env file with API key\n",
    "env_path = '/Users/sebastiannagl/Desktop/Code/CourtPressGER/.env'\n",
    "print(f\"Looking for .env file at: {env_path}\")\n",
    "\n",
    "# Explicitly read the .env file content\n",
    "try:\n",
    "    with open(env_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() and '=' in line:\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value\n",
    "                print(f\"Set environment variable: {key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading .env file: {e}\")\n",
    "\n",
    "# Backup approach using dotenv\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Get the API key\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "print(f\"API key found: {'Yes' if ANTHROPIC_API_KEY else 'No'}\")\n",
    "\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"Kein API-Schlüssel in der Umgebung gefunden.\")\n",
    "    ANTHROPIC_API_KEY = input(\"Bitte gib deinen Anthropic API-Schlüssel ein: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "else:\n",
    "    print(\"API-Schlüssel aus der Umgebung geladen!\")\n",
    "\n",
    "# Initialize the Claude client with the API key as a direct parameter\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "# Test the API connection\n",
    "try:\n",
    "    # Make a simple query to test the connection\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=100,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Hallo, Claude! Bitte antworte mit einer sehr kurzen Begrüßung.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"API-Verbindung erfolgreich!\")\n",
    "    print(f\"Claude sagt: {response.content[0].text}\")\n",
    "    \n",
    "    # Get available models - with proper error handling\n",
    "    try:\n",
    "        print(\"Currently available models:\")\n",
    "        models = client.models.list()\n",
    "        # Handle both possible response formats\n",
    "        if hasattr(models, 'data'):\n",
    "            for model in models.data:\n",
    "                if hasattr(model, 'id') and hasattr(model, 'display_name'):\n",
    "                    print(f\"- {model.id}: {model.display_name}\")\n",
    "                else:\n",
    "                    print(f\"- {model}\")\n",
    "        else:\n",
    "            for model in models:\n",
    "                print(f\"- {model.get('id', 'unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen der Modelle: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler bei der Verbindung zur Claude API: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ce810",
   "metadata": {},
   "source": [
    "## 3. Hilfsfunktionen für die Generierung synthetischer Prompts\n",
    "\n",
    "Wir erstellen Hilfsfunktionen, um synthetische Prompts aus den Gerichtsurteilen und Pressemitteilungen zu generieren.\n",
    "Die Implementierung berücksichtigt die Rate-Limits der Claude API (50 Anfragen/Minute, 20.000 Input-Tokens/Minute, 8.000 Output-Tokens/Minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fff6b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Klasse zur Verwaltung der Rate-Limits für die Claude API.\n",
    "    - 50 Anfragen pro Minute\n",
    "    - 20.000 Input-Tokens pro Minute\n",
    "    - 8.000 Output-Tokens pro Minute\n",
    "    \"\"\"\n",
    "    def __init__(self, requests_per_minute=50, input_tokens_per_minute=20000, output_tokens_per_minute=8000):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.input_tokens_per_minute = input_tokens_per_minute\n",
    "        self.output_tokens_per_minute = output_tokens_per_minute\n",
    "        \n",
    "        # Trackers für die letzten 60 Sekunden (Zeitfenster von 1 Minute)\n",
    "        self.request_timestamps = []\n",
    "        self.input_token_usage = []  # [timestamp, tokens]\n",
    "        self.output_token_usage = []  # [timestamp, tokens]\n",
    "        \n",
    "    def wait_if_needed(self, input_tokens_estimate, output_tokens_estimate=500):\n",
    "        \"\"\"\n",
    "        Wartet, wenn nötig, um die Rate-Limits einzuhalten.\n",
    "        Schätzt die Anzahl der benötigten Tokens und wartet, bis genügend Kapazität verfügbar ist.\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Bereinige abgelaufene Einträge (älter als 60 Sekunden)\n",
    "        self._clean_expired_entries(current_time)\n",
    "        \n",
    "        # Berechne aktuelle Nutzung\n",
    "        recent_requests = len(self.request_timestamps)\n",
    "        recent_input_tokens = sum(tokens for _, tokens in self.input_token_usage)\n",
    "        recent_output_tokens = sum(tokens for _, tokens in self.output_token_usage)\n",
    "        \n",
    "        wait_time = 0\n",
    "        \n",
    "        # Prüfe Request-Limit\n",
    "        if recent_requests >= self.requests_per_minute:\n",
    "            oldest_request = self.request_timestamps[0]\n",
    "            wait_time = max(wait_time, oldest_request + 60 - current_time)\n",
    "        \n",
    "        # Prüfe Input-Token-Limit\n",
    "        if recent_input_tokens + input_tokens_estimate > self.input_tokens_per_minute:\n",
    "            wait_time_input = self._calculate_token_wait_time(self.input_token_usage, \n",
    "                                                            input_tokens_estimate, \n",
    "                                                            self.input_tokens_per_minute)\n",
    "            wait_time = max(wait_time, wait_time_input)\n",
    "        \n",
    "        # Prüfe Output-Token-Limit\n",
    "        if recent_output_tokens + output_tokens_estimate > self.output_tokens_per_minute:\n",
    "            wait_time_output = self._calculate_token_wait_time(self.output_token_usage, \n",
    "                                                             output_tokens_estimate, \n",
    "                                                             self.output_tokens_per_minute)\n",
    "            wait_time = max(wait_time, wait_time_output)\n",
    "        \n",
    "        # Warte, wenn nötig\n",
    "        if wait_time > 0:\n",
    "            print(f\"Rate-Limit erreicht. Warte {wait_time:.1f} Sekunden...\")\n",
    "            time.sleep(wait_time)\n",
    "            # Nach dem Warten die abgelaufenen Einträge erneut bereinigen\n",
    "            current_time = time.time()\n",
    "            self._clean_expired_entries(current_time)\n",
    "    \n",
    "    def _clean_expired_entries(self, current_time):\n",
    "        \"\"\"Entfernt Einträge, die älter als 60 Sekunden sind.\"\"\"\n",
    "        cutoff_time = current_time - 60\n",
    "        \n",
    "        # Bereinige Request-Timestamps\n",
    "        while self.request_timestamps and self.request_timestamps[0] < cutoff_time:\n",
    "            self.request_timestamps.pop(0)\n",
    "        \n",
    "        # Bereinige Input-Token-Nutzung\n",
    "        while self.input_token_usage and self.input_token_usage[0][0] < cutoff_time:\n",
    "            self.input_token_usage.pop(0)\n",
    "        \n",
    "        # Bereinige Output-Token-Nutzung\n",
    "        while self.output_token_usage and self.output_token_usage[0][0] < cutoff_time:\n",
    "            self.output_token_usage.pop(0)\n",
    "    \n",
    "    def _calculate_token_wait_time(self, token_usage, new_tokens, limit):\n",
    "        \"\"\"Berechnet die Wartezeit für Token-basierte Limits.\"\"\"\n",
    "        if not token_usage:\n",
    "            return 0\n",
    "            \n",
    "        current_usage = sum(tokens for _, tokens in token_usage)\n",
    "        \n",
    "        # Wenn das Hinzufügen neuer Tokens das Limit überschreitet, müssen wir warten\n",
    "        if current_usage + new_tokens > limit:\n",
    "            # Berechne, wie viele Token wir freigeben müssen\n",
    "            tokens_to_free = (current_usage + new_tokens) - limit\n",
    "            \n",
    "            # Finde den Zeitpunkt, zu dem genügend Token freigegeben werden\n",
    "            freed_tokens = 0\n",
    "            for timestamp, tokens in token_usage:\n",
    "                freed_tokens += tokens\n",
    "                if freed_tokens >= tokens_to_free:\n",
    "                    # Warte bis dieser Zeitstempel + 60 Sekunden\n",
    "                    return timestamp + 60 - time.time()\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def record_usage(self, input_tokens, output_tokens):\n",
    "        \"\"\"Zeichnet die Nutzung einer API-Anfrage auf.\"\"\"\n",
    "        current_time = time.time()\n",
    "        self.request_timestamps.append(current_time)\n",
    "        self.input_token_usage.append([current_time, input_tokens])\n",
    "        self.output_token_usage.append([current_time, output_tokens])\n",
    "\n",
    "# Erstelle eine globale Rate Limiter-Instanz\n",
    "rate_limiter = RateLimiter()\n",
    "\n",
    "def estimate_token_count(text):\n",
    "    \"\"\"\n",
    "    Schätzt die Anzahl der Tokens im Text. \n",
    "    Einfache Approximation: 1 Token ≈ 4 Zeichen für Englisch/Deutsch.\n",
    "    \"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def generate_synthetic_prompt(court_ruling, press_release, model=\"claude-3-7-sonnet-20250219\", retries=3, wait_time=2):\n",
    "    \"\"\"\n",
    "    Generiert einen synthetischen Prompt, der die gegebene Pressemitteilung aus dem Gerichtsurteil erzeugen könnte.\n",
    "    \n",
    "    Args:\n",
    "        court_ruling (str): Der Text des Gerichtsurteils\n",
    "        press_release (str): Der Text der Pressemitteilung\n",
    "        model (str): Das zu verwendende Claude-Modell\n",
    "        retries (int): Anzahl der Wiederholungsversuche bei API-Fehlern\n",
    "        wait_time (int): Wartezeit zwischen Wiederholungsversuchen in Sekunden\n",
    "        \n",
    "    Returns:\n",
    "        str: Der generierte synthetische Prompt\n",
    "    \"\"\"\n",
    "    # Vorbereitung des System-Prompts\n",
    "    system_prompt = \"\"\"\n",
    "    Du bist ein Experte für juristische Texte und Kommunikation. Deine Aufgabe ist es, ein Gerichtsurteil und die \n",
    "    dazugehörige Pressemitteilung zu analysieren und dann herauszufinden, welcher Prompt verwendet worden sein könnte, \n",
    "    um diese Pressemitteilung aus dem Gerichtsurteil zu generieren, wenn man ihn einem LLM gegeben hätte.\n",
    "    \n",
    "    1. Analysiere, wie die Pressemitteilung Informationen aus dem Urteil vereinfacht, umstrukturiert und Schlüsselinformationen hervorhebt\n",
    "    2. Berücksichtige den Ton, die Struktur und den Detaillierungsgrad der Pressemitteilung\n",
    "    3. Identifiziere, welche Anweisungen nötig wären, um den juristischen Text in diese Pressemitteilung zu transformieren\n",
    "    \n",
    "    Erkläre NICHT deine Überlegungen und füge KEINE Meta-Kommentare hinzu. Gib NUR den tatsächlichen Prompt aus, der die \n",
    "    Pressemitteilung aus dem Gerichtsurteil generieren würde. Sei spezifisch und detailliert in deinem synthetisierten Prompt.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vorbereitung des Benutzer-Prompts\n",
    "    user_prompt = f\"\"\"\n",
    "    Hier ist das originale Gerichtsurteil:\n",
    "    \n",
    "    ```\n",
    "    {court_ruling}\n",
    "    ```\n",
    "    \n",
    "    Und hier ist die Pressemitteilung, die daraus erstellt wurde:\n",
    "    \n",
    "    ```\n",
    "    {press_release}\n",
    "    ```\n",
    "    \n",
    "    Erstelle einen detaillierten Prompt, der einem LLM gegeben werden könnte, um die obige Pressemitteilung aus dem Gerichtsurteil zu generieren. \n",
    "    Schreibe NUR den Prompt selbst, ohne Erklärungen oder Meta-Kommentare.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Schätze den Token-Verbrauch\n",
    "    input_tokens_estimate = estimate_token_count(system_prompt) + estimate_token_count(user_prompt)\n",
    "    output_tokens_estimate = 500  # Konservative Schätzung für die Antwortlänge\n",
    "    \n",
    "    # Warte, wenn nötig, um Rate-Limits einzuhalten\n",
    "    rate_limiter.wait_if_needed(input_tokens_estimate, output_tokens_estimate)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=1000,\n",
    "                system=system_prompt,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Zeichne tatsächliche Token-Nutzung auf, falls verfügbar\n",
    "            input_tokens = response.usage.input_tokens if hasattr(response, 'usage') and hasattr(response.usage, 'input_tokens') else input_tokens_estimate\n",
    "            output_tokens = response.usage.output_tokens if hasattr(response, 'usage') and hasattr(response.usage, 'output_tokens') else output_tokens_estimate\n",
    "            rate_limiter.record_usage(input_tokens, output_tokens)\n",
    "            \n",
    "            return response.content[0].text.strip()\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Fehler: {e}. Neuer Versuch in {wait_time} Sekunden...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Nach {retries} Versuchen fehlgeschlagen: {e}\")\n",
    "                return f\"Fehler bei der Generierung des Prompts: {e}\"\n",
    "\n",
    "def process_batch(df, batch_size=5, start_idx=0, save_interval=10, fix_errors=False):\n",
    "    \"\"\"\n",
    "    Verarbeitet einen Dataframe in Batches und generiert synthetische Prompts für jede Zeile.\n",
    "    Der Batch-Size wurde auf 5 reduziert, um die Rate-Limits besser einzuhalten.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Der Dataframe mit Gerichtsurteilen und Pressemitteilungen\n",
    "        batch_size (int): Anzahl der Elemente, die vor dem Speichern von Zwischenergebnissen verarbeitet werden\n",
    "        start_idx (int): Index, bei dem die Verarbeitung beginnen soll (für die Wiederaufnahme)\n",
    "        save_interval (int): Wie oft Zwischenergebnisse gespeichert werden sollen\n",
    "        fix_errors (bool): Wenn True, werden Zeilen mit API-Fehlermeldungen erneut verarbeitet\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Der Dataframe mit hinzugefügten synthetischen Prompts\n",
    "    \"\"\"\n",
    "    # Erstelle eine synthetic_prompt-Spalte, falls sie nicht existiert\n",
    "    if 'synthetic_prompt' not in df.columns:\n",
    "        df['synthetic_prompt'] = None\n",
    "    \n",
    "    # Identifiziere Zeilen mit API-Fehlermeldungen, falls fix_errors=True\n",
    "    if fix_errors:\n",
    "        error_mask = df['synthetic_prompt'].astype(str).str.contains(\"Fehler bei der Generierung des Prompts\", na=False)\n",
    "        error_indices = df[error_mask].index.tolist()\n",
    "        if error_indices:\n",
    "            print(f\"Gefunden: {len(error_indices)} Einträge mit API-Fehlern\")\n",
    "            # Setze die fehlerhaften Einträge auf None zurück, damit sie neu verarbeitet werden\n",
    "            df.loc[error_indices, 'synthetic_prompt'] = None\n",
    "    \n",
    "    # Erstelle ein Verzeichnis für Checkpoints, falls es nicht existiert\n",
    "    checkpoint_dir = Path('checkpoints')\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Verarbeite in Batches\n",
    "    for i in tqdm(range(start_idx, len(df), batch_size)):\n",
    "        batch_end = min(i + batch_size, len(df))\n",
    "        batch = df.iloc[i:batch_end].copy()\n",
    "        \n",
    "        # Zähler für erfolgreiche Verarbeitungen in diesem Batch\n",
    "        successful_in_batch = 0\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            if pd.isna(df.at[idx, 'synthetic_prompt']):\n",
    "                court_ruling = row['judgement']\n",
    "                press_release = row['summary']\n",
    "                \n",
    "                # Kürze sehr lange Eingaben, um Token-Limits zu vermeiden\n",
    "                if len(court_ruling) > 12000:  # Reduziert für bessere Tokenbegrenzung\n",
    "                    court_ruling = court_ruling[:12000] + \"...\"\n",
    "                if len(press_release) > 4000:  # Reduziert für bessere Tokenbegrenzung\n",
    "                    press_release = press_release[:4000] + \"...\"\n",
    "                \n",
    "                # Generiere den synthetischen Prompt\n",
    "                synthetic_prompt = generate_synthetic_prompt(court_ruling, press_release)\n",
    "                \n",
    "                # Prüfe, ob die Antwort einen API-Fehler enthält\n",
    "                if \"Fehler bei der Generierung des Prompts\" in str(synthetic_prompt):\n",
    "                    print(f\"⚠️ API-Fehler bei Index {idx}, wird in einem zukünftigen Durchlauf erneut versucht\")\n",
    "                else:\n",
    "                    successful_in_batch += 1\n",
    "                \n",
    "                df.at[idx, 'synthetic_prompt'] = synthetic_prompt\n",
    "                \n",
    "                # Zeige Fortschritt an\n",
    "                if (idx - i) % 5 == 0 or idx == batch_end - 1:\n",
    "                    print(f\"{idx+1}/{len(df)} Einträge verarbeitet - {successful_in_batch}/{batch_end-i} erfolgreich in diesem Batch\")\n",
    "        \n",
    "        # Speichere Checkpoint in regelmäßigen Abständen\n",
    "        if (i // batch_size) % save_interval == 0 or batch_end == len(df):\n",
    "            checkpoint_path = checkpoint_dir / f\"synthetic_prompts_checkpoint_{batch_end}.csv\"\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "            print(f\"Checkpoint gespeichert unter {checkpoint_path}\")\n",
    "            \n",
    "            # Kurze Pause nach jedem Batch, um Rate-Limits zu entspannen\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d689d",
   "metadata": {},
   "source": [
    "## 4. Checkpoint laden oder von vorne beginnen\n",
    "\n",
    "Wir prüfen, ob es eine Checkpoint-Datei gibt, um von dort fortzufahren, oder beginnen die Verarbeitung von Anfang an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "244a7443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuester Checkpoint gefunden: checkpoints/synthetic_prompts_checkpoint_560.csv\n",
      "Checkpoint mit 6432 Einträgen geladen\n",
      "Verarbeitung wird ab Index 560 fortgesetzt\n"
     ]
    }
   ],
   "source": [
    "# Prüfe auf vorhandene Checkpoints\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_files = list(checkpoint_dir.glob(\"synthetic_prompts_checkpoint_*.csv\")) if checkpoint_dir.exists() else []\n",
    "\n",
    "start_idx = 0\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Finde den neuesten Checkpoint basierend auf der Nummer im Dateinamen\n",
    "    latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))\n",
    "    print(f\"Neuester Checkpoint gefunden: {latest_checkpoint}\")\n",
    "    \n",
    "    # Frage, ob vom Checkpoint fortgefahren werden soll\n",
    "    continue_from_checkpoint = input(\"Vom letzten Checkpoint fortfahren? (j/n): \").lower().strip() == 'j'\n",
    "    \n",
    "    if continue_from_checkpoint:\n",
    "        # Lade den Checkpoint\n",
    "        df = pd.read_csv(latest_checkpoint)\n",
    "        print(f\"Checkpoint mit {len(df)} Einträgen geladen\")\n",
    "        \n",
    "        # Finde die erste Zeile ohne synthetischen Prompt\n",
    "        if 'synthetic_prompt' in df.columns:\n",
    "            missing_prompts = df['synthetic_prompt'].isna()\n",
    "            if missing_prompts.any():\n",
    "                start_idx = missing_prompts.idxmax()\n",
    "                print(f\"Verarbeitung wird ab Index {start_idx} fortgesetzt\")\n",
    "            else:\n",
    "                print(\"Alle Einträge haben bereits synthetische Prompts!\")\n",
    "        else:\n",
    "            print(\"Keine synthetic_prompt-Spalte im Checkpoint gefunden, beginne von vorne\")\n",
    "    else:\n",
    "        print(\"Starte neue Verarbeitung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d44b0",
   "metadata": {},
   "source": [
    "## Fehlerhafte Einträge korrigieren\n",
    "\n",
    "In diesem Abschnitt identifizieren und korrigieren wir Einträge, bei denen die API mit einem Authentifizierungsfehler geantwortet hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a9a2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuester Checkpoint geladen: checkpoints/synthetic_prompts_checkpoint_580.csv\n",
      "Checkpoint mit 6432 Einträgen geladen\n",
      "Gefunden: 540 Einträge mit API-Fehlern\n",
      "Erste 5 Indices mit Fehlern: [0, 1, 2, 3, 4]\n",
      "Beispiel-Fehlermeldung: Fehler bei der Generierung des Prompts: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n",
      "Korrektur abgebrochen.\n"
     ]
    }
   ],
   "source": [
    "# Lade den zuletzt erstellten Checkpoint, falls vorhanden\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "checkpoint_files = list(checkpoint_dir.glob(\"synthetic_prompts_checkpoint_*.csv\")) if checkpoint_dir.exists() else []\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Finde den neuesten Checkpoint basierend auf der Nummer im Dateinamen\n",
    "    latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))\n",
    "    print(f\"Neuester Checkpoint geladen: {latest_checkpoint}\")\n",
    "    \n",
    "    # Lade den Checkpoint\n",
    "    fix_df = pd.read_csv(latest_checkpoint)\n",
    "    print(f\"Checkpoint mit {len(fix_df)} Einträgen geladen\")\n",
    "    \n",
    "    # Identifiziere Zeilen mit API-Fehlern - sucht nach dem generellen Fehlerpattern\n",
    "    error_mask = fix_df['synthetic_prompt'].astype(str).str.contains(\"Fehler bei der Generierung des Prompts\", na=False)\n",
    "    error_indices = fix_df[error_mask].index.tolist()\n",
    "    \n",
    "    if error_indices:\n",
    "        print(f\"Gefunden: {len(error_indices)} Einträge mit API-Fehlern\")\n",
    "        print(f\"Erste 5 Indices mit Fehlern: {error_indices[:5]}\")\n",
    "        print(f\"Beispiel-Fehlermeldung: {fix_df.loc[error_indices[0], 'synthetic_prompt']}\")\n",
    "        \n",
    "        # Option zum Korrigieren aller fehlerhaften Einträge\n",
    "        fix_all = input(\"Alle fehlerhaften Einträge korrigieren? (j/n): \").lower().strip() == 'j'\n",
    "        \n",
    "        if fix_all:\n",
    "            print(\"Korrigiere fehlerhafte Einträge...\")\n",
    "            # Verarbeite den Datensatz und korrigiere fehlerhafte Einträge\n",
    "            fix_df = process_batch(fix_df, batch_size=5, start_idx=0, save_interval=2, fix_errors=True)\n",
    "            \n",
    "            # Prüfe, ob noch Fehler vorhanden sind\n",
    "            remaining_errors = fix_df['synthetic_prompt'].astype(str).str.contains(\"Fehler bei der Generierung des Prompts\", na=False).sum()\n",
    "            print(f\"Verbleibende Fehler nach Korrektur: {remaining_errors}\")\n",
    "            \n",
    "            # Speichere das Ergebnis mit korrigierten Einträgen\n",
    "            fixed_output_path = checkpoint_dir / f\"synthetic_prompts_fixed_{int(time.time())}.csv\"\n",
    "            fix_df.to_csv(fixed_output_path, index=False)\n",
    "            print(f\"Korrigierter Datensatz gespeichert unter {fixed_output_path}\")\n",
    "        else:\n",
    "            print(\"Korrektur abgebrochen.\")\n",
    "    else:\n",
    "        print(\"Keine Einträge mit API-Fehlern gefunden.\")\n",
    "else:\n",
    "    print(\"Keine Checkpoints gefunden. Bitte führe zunächst die Generierung der synthetischen Prompts durch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2a7b6",
   "metadata": {},
   "source": [
    "## 5. Synthetische Prompts generieren\n",
    "\n",
    "Jetzt verarbeiten wir den Datensatz, um die synthetischen Prompts zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguriere Parameter für die Batch-Verarbeitung\n",
    "batch_size = 5  # Reduziert von 10 auf 5 für bessere Rate-Limiting\n",
    "save_interval = 5  # Speichere Checkpoint alle 5 Batches\n",
    "\n",
    "# Verarbeite den Datensatz\n",
    "print(f\"Starte Generierung synthetischer Prompts für {len(df)} Einträge ab Index {start_idx}\")\n",
    "print(f\"Rate-Limits: 50 Anfragen/Minute, 20.000 Input-Tokens/Minute, 8.000 Output-Tokens/Minute\")\n",
    "df = process_batch(df, batch_size=batch_size, start_idx=start_idx, save_interval=save_interval)\n",
    "\n",
    "# Speichere das endgültige Ergebnis\n",
    "output_path = Path('../data/processed/court_press_with_synthetic_prompts.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Vollständiger Datensatz mit synthetischen Prompts gespeichert unter {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b0ace",
   "metadata": {},
   "source": [
    "## 6. Beispiele der generierten synthetischen Prompts analysieren\n",
    "\n",
    "Schauen wir uns einige Beispiele der generierten synthetischen Prompts an, um die Art der Anweisungen zu verstehen, die Claude vorgeschlagen hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wähle einige Beispiele zur Anzeige aus\n",
    "if 'synthetic_prompt' in df.columns and not df['synthetic_prompt'].isna().all():\n",
    "    # Hole 5 zufällige Beispiele, bei denen synthetic_prompt nicht None/NaN ist\n",
    "    sample_df = df[df['synthetic_prompt'].notna()].sample(min(5, len(df[df['synthetic_prompt'].notna()])))\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_df.iterrows(), 1):\n",
    "        print(f\"\\n==== Beispiel {i} ====\")\n",
    "        print(f\"Gerichtsfall-ID: {row['id']}\")\n",
    "        print(\"\\nSynthetischer Prompt:\")\n",
    "        print(row['synthetic_prompt'])\n",
    "        \n",
    "        # Optional, zeige auch den ersten Teil der Pressemitteilung\n",
    "        print(\"\\nErste 150 Zeichen der Pressemitteilung:\")\n",
    "        print(row['summary'][:150] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Noch keine synthetischen Prompts verfügbar. Führe zunächst die Verarbeitungszelle aus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bc3d7",
   "metadata": {},
   "source": [
    "## 7. Muster in den synthetischen Prompts analysieren\n",
    "\n",
    "Analysieren wir die generierten Prompts, um häufige Muster und Eigenschaften zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfe, ob wir Prompts zur Analyse haben\n",
    "if 'synthetic_prompt' in df.columns and not df['synthetic_prompt'].isna().all():\n",
    "    # Erstelle einen Dataframe nur mit den synthetischen Prompts für die Analyse\n",
    "    prompts_df = df[df['synthetic_prompt'].notna()].copy()\n",
    "    \n",
    "    # Berechne grundlegende Statistiken\n",
    "    prompt_lengths = prompts_df['synthetic_prompt'].str.len()\n",
    "    avg_length = prompt_lengths.mean()\n",
    "    min_length = prompt_lengths.min()\n",
    "    max_length = prompt_lengths.max()\n",
    "    \n",
    "    print(f\"Anzahl synthetischer Prompts: {len(prompts_df)}\")\n",
    "    print(f\"Durchschnittliche Prompt-Länge: {avg_length:.1f} Zeichen\")\n",
    "    print(f\"Kürzester Prompt: {min_length} Zeichen\")\n",
    "    print(f\"Längster Prompt: {max_length} Zeichen\")\n",
    "    \n",
    "    # Suche nach häufigen Phrasen oder Anweisungstypen in den Prompts\n",
    "    common_phrases = [\n",
    "        \"zusammenfassen\", \"vereinfachen\", \"erklären\", \"übersetzen\", \"umwandeln\", \n",
    "        \"transformieren\", \"erstelle eine pressemitteilung\", \"schreibe eine pressemitteilung\", \n",
    "        \"nicht-technisch\", \"laien\", \"öffentliches publikum\", \"prägnant\"\n",
    "    ]\n",
    "    \n",
    "    phrase_counts = {}\n",
    "    for phrase in common_phrases:\n",
    "        count = prompts_df['synthetic_prompt'].str.lower().str.contains(phrase).sum()\n",
    "        if count > 0:\n",
    "            phrase_counts[phrase] = count\n",
    "    \n",
    "    # Konvertiere Anzahl in Prozentsätze\n",
    "    phrase_percentages = {k: v/len(prompts_df)*100 for k, v in phrase_counts.items()}\n",
    "    \n",
    "    # Sortiere nach Häufigkeit\n",
    "    sorted_phrases = sorted(phrase_percentages.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nHäufige Anweisungstypen in synthetischen Prompts:\")\n",
    "    for phrase, percentage in sorted_phrases:\n",
    "        print(f\"{phrase}: {percentage:.1f}% der Prompts\")\n",
    "else:\n",
    "    print(\"Noch keine synthetischen Prompts verfügbar. Führe zunächst die Verarbeitungszelle aus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a465c42",
   "metadata": {},
   "source": [
    "## 8. Datensatz für weitere Analysen exportieren\n",
    "\n",
    "Schließlich speichern wir unseren Datensatz mit den synthetischen Prompts für weitere Analysen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57608b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endgültiger Datensatz mit 6432 Einträgen gespeichert unter ../data/processed/court_press_with_synthetic_prompts_final.csv\n",
      "Wesentlicher Datensatz mit 6432 Einträgen gespeichert unter ../data/processed/court_press_with_synthetic_prompts_essential.csv\n"
     ]
    }
   ],
   "source": [
    "# Erstelle eine endgültige Version des Datensatzes mit Spalten in einer logischen Reihenfolge\n",
    "if 'synthetic_prompt' in df.columns:\n",
    "    # Reorganisiere Spalten in einer logischen Reihenfolge\n",
    "    columns_order = [\n",
    "        'id', 'date', 'subset_name', 'split_name',  # Kennungen\n",
    "        'synthetic_prompt',  # Neue generierte Spalte\n",
    "        'summary', 'judgement'  # Ursprüngliche Textspalten\n",
    "    ]\n",
    "    \n",
    "    # Füge alle verbleibenden Spalten hinzu\n",
    "    remaining_columns = [col for col in df.columns if col not in columns_order]\n",
    "    final_columns = columns_order + remaining_columns\n",
    "    \n",
    "    # Erstelle endgültigen Datensatz mit Spalten in gewünschter Reihenfolge\n",
    "    final_df = df[final_columns].copy()\n",
    "    \n",
    "    # Speichere als endgültigen Datensatz\n",
    "    final_output_path = Path('../data/processed/court_press_with_synthetic_prompts_final.csv')\n",
    "    final_df.to_csv(final_output_path, index=False)\n",
    "    print(f\"Endgültiger Datensatz mit {len(final_df)} Einträgen gespeichert unter {final_output_path}\")\n",
    "    \n",
    "    # Speichere auch eine leichtere Version mit nur den wesentlichen Spalten\n",
    "    essential_columns = ['id', 'date', 'subset_name', 'split_name', 'synthetic_prompt', 'summary', 'judgement']\n",
    "    essential_df = final_df[essential_columns].copy()\n",
    "    \n",
    "    essential_output_path = Path('../data/processed/court_press_with_synthetic_prompts_essential.csv')\n",
    "    essential_df.to_csv(essential_output_path, index=False)\n",
    "    print(f\"Wesentlicher Datensatz mit {len(essential_df)} Einträgen gespeichert unter {essential_output_path}\")\n",
    "else:\n",
    "    print(\"Noch keine synthetischen Prompts verfügbar. Führe zunächst die Verarbeitungszelle aus.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
