{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c74c9ee",
   "metadata": {},
   "source": [
    "# Evaluierung der LLM-Pipeline für Pressemitteilungsgenerierung\n",
    "\n",
    "Dieses Notebook demonstriert die Verwendung der Evaluierungspipeline für verschiedene LLMs zur Generierung von Pressemitteilungen aus Gerichtsurteilen mit Hilfe synthetischer Prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b226c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Projekt-spezifische Importe\n",
    "from courtpressger.evaluation.pipeline import LLMEvaluationPipeline\n",
    "from courtpressger.evaluation.models import create_model_config\n",
    "from courtpressger.evaluation.utils import (\n",
    "    load_evaluation_results,\n",
    "    results_to_dataframe,\n",
    "    visualize_rouge_scores,\n",
    "    visualize_metric_comparison,\n",
    "    extract_top_examples,\n",
    "    compute_statistical_significance\n",
    ")\n",
    "\n",
    "# Stile für schönere Visualisierungen setzen\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d067dd6",
   "metadata": {},
   "source": [
    "## 1. Konfiguration und Datenvorbereitung\n",
    "\n",
    "Zuerst laden wir die Modellkonfiguration und den Evaluierungsdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f928e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade definieren\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "CONFIG_PATH = PROJECT_ROOT / \"models\" / \"evaluation_models_config.json\"\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"evaluation_dataset.csv\"  # Pfad anpassen\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"evaluation\"\n",
    "\n",
    "# Ausgabeverzeichnis erstellen, falls nicht vorhanden\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Modellkonfiguration laden\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "    models_config = config_data.get('models', [])\n",
    "\n",
    "print(f\"Geladen: {len(models_config)} Modelle aus der Konfiguration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20300a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testdaten oder tatsächliche Daten laden\n",
    "# Falls die CSV-Datei nicht existiert, verwenden wir einen kleinen Beispieldatensatz\n",
    "try:\n",
    "    dataset = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Datensatz geladen: {len(dataset)} Einträge\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Datei {DATA_PATH} nicht gefunden. Verwende einen Testdatensatz.\")\n",
    "    \n",
    "    # Wir können alternativ einen Checkpoint-Datensatz laden\n",
    "    checkpoint_files = list((PROJECT_ROOT / \"data\" / \"checkpoints\").glob(\"cases_prs_synth_prompts_*.csv\"))\n",
    "    if checkpoint_files:\n",
    "        # Den ersten Checkpoint-Datensatz verwenden\n",
    "        dataset = pd.read_csv(checkpoint_files[0])\n",
    "        print(f\"Checkpoint-Datensatz geladen: {len(dataset)} Einträge aus {checkpoint_files[0]}\")\n",
    "    else:\n",
    "        # Oder einen einfachen Testdatensatz erstellen\n",
    "        dataset = pd.DataFrame({\n",
    "            'synth_prompt': [\"Erstelle eine Pressemitteilung für das folgende Urteil:\"],\n",
    "            'ruling_text': [\"Das Gericht hat entschieden...\"],\n",
    "            'press_release': [\"In einer heutigen Entscheidung hat das Gericht...\"]\n",
    "        })\n",
    "        print(f\"Testdatensatz erstellt mit {len(dataset)} Einträgen\")\n",
    "\n",
    "# Spalten für die Evaluierung definieren\n",
    "prompt_column = 'synth_prompt'  # Anpassen an tatsächliche Spaltenbezeichnung\n",
    "ruling_column = 'ruling_text'   # Anpassen an tatsächliche Spaltenbezeichnung\n",
    "press_column = 'press_release'  # Anpassen an tatsächliche Spaltenbezeichnung\n",
    "\n",
    "# Einen ersten Blick auf die Daten werfen\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e76de",
   "metadata": {},
   "source": [
    "## 2. Modellkonfiguration\n",
    "\n",
    "Wir bereiten die Modellkonfiguration für die Evaluierungspipeline vor. Da die API-Aufrufe zeitaufwändig und möglicherweise kostspielig sind, bieten wir die Option, entweder vorhandene Ergebnisse zu laden oder die Pipeline tatsächlich auszuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ece9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellkonfiguration vorbereiten und Generatorfunktionen erstellen\n",
    "prepared_models = []\n",
    "\n",
    "for model_config in models_config:\n",
    "    try:\n",
    "        model_info = create_model_config(model_config)\n",
    "        prepared_models.append(model_info)\n",
    "        print(f\"Modell konfiguriert: {model_info['name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Konfiguration von {model_config.get('name', 'Unbekannt')}: {str(e)}\")\n",
    "\n",
    "# Für dieses Notebook können wir optional eine Teilmenge der Modelle auswählen\n",
    "# Beispiel: Nur die ersten beiden Modelle verwenden\n",
    "# selected_models = prepared_models[:2]  # Auskommentieren, um alle Modelle zu verwenden\n",
    "selected_models = prepared_models  # Alle konfigurierten Modelle verwenden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba97a3",
   "metadata": {},
   "source": [
    "## 3. Evaluierungspipeline ausführen oder vorhandene Ergebnisse laden\n",
    "\n",
    "Hier können wir entweder die Pipeline tatsächlich ausführen (was zeitaufwändig sein kann) oder vorhandene Ergebnisse laden, wenn die Evaluation bereits durchgeführt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfen, ob Ergebnisse bereits vorhanden sind\n",
    "evaluation_summary_path = OUTPUT_DIR / \"evaluation_summary.json\"\n",
    "load_existing_results = evaluation_summary_path.exists()\n",
    "\n",
    "if load_existing_results:\n",
    "    print(f\"Lade vorhandene Evaluierungsergebnisse aus {evaluation_summary_path}\")\n",
    "    results = load_evaluation_results(str(OUTPUT_DIR))\n",
    "else:\n",
    "    print(\"Führe Evaluierungspipeline aus (dies kann einige Zeit dauern)...\")\n",
    "    \n",
    "    # Nur eine kleine Teilmenge des Datensatzes verwenden, falls der Datensatz groß ist\n",
    "    if len(dataset) > 10:\n",
    "        eval_dataset = dataset.sample(10, random_state=42)  # 10 zufällige Einträge für Demo\n",
    "        print(f\"Verwende 10 zufällige Einträge für die Demo-Evaluation\")\n",
    "    else:\n",
    "        eval_dataset = dataset\n",
    "    \n",
    "    # Pipeline initialisieren und ausführen\n",
    "    pipeline = LLMEvaluationPipeline(selected_models, output_dir=str(OUTPUT_DIR))\n",
    "    pipeline_results = pipeline.run_evaluation(\n",
    "        dataset=eval_dataset,\n",
    "        prompt_column=prompt_column,\n",
    "        ruling_column=ruling_column,\n",
    "        reference_press_column=press_column,\n",
    "        batch_size=5,  # Kleine Batch-Größe für Demo\n",
    "        checkpoint_freq=5  # Häufige Checkpoints für Demo\n",
    "    )\n",
    "    \n",
    "    # Ergebnisse laden\n",
    "    results = load_evaluation_results(str(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7e76a",
   "metadata": {},
   "source": [
    "## 4. Ergebnisanalyse und Visualisierung\n",
    "\n",
    "Jetzt analysieren und visualisieren wir die Evaluierungsergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse in DataFrame konvertieren für einfachere Analyse\n",
    "results_df = results_to_dataframe(results)\n",
    "\n",
    "# Überblick über die Ergebnisse\n",
    "print(\"Zusammenfassung der Evaluierungsergebnisse:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-Scores visualisieren\n",
    "plt.figure(figsize=(14, 8))\n",
    "visualize_rouge_scores(results_df)\n",
    "plt.title(\"ROUGE-Scores nach Modell\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"rouge_scores_comparison.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich aller verfügbaren Metriken (Radar-Chart)\n",
    "metrics = [col for col in results_df.columns if col != \"model\"]\n",
    "plt.figure(figsize=(12, 12))\n",
    "visualize_metric_comparison(results_df, metrics)\n",
    "plt.title(\"Vergleich aller Metriken nach Modell\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"metrics_radar_comparison.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d907603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusätzliche Visualisierung: Heatmap der Metriken\n",
    "plt.figure(figsize=(14, 10))\n",
    "heatmap_data = results_df.set_index('model')\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", linewidths=.5)\n",
    "plt.title(\"Heatmap der Evaluierungsmetriken nach Modell\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"metrics_heatmap.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dbce4e",
   "metadata": {},
   "source": [
    "## 5. Detaillierte Analyse einzelner Modelle\n",
    "\n",
    "Wir können auch in die Ergebnisse einzelner Modelle eintauchen, um deren Stärken und Schwächen besser zu verstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Funktion, um die besten und schlechtesten Beispiele für ein Modell zu extrahieren\n",
    "def analyze_model_examples(model_name):\n",
    "    print(f\"\\n=== Analyse für Modell: {model_name} ===\\n\")\n",
    "    \n",
    "    # Beste Beispiele nach ROUGE-L\n",
    "    print(\"Top 3 beste Beispiele (nach ROUGE-L F1-Score):\")\n",
    "    best_examples = extract_top_examples(results, model_name, \"rougeL_fmeasure\", top_n=3, highest=True)\n",
    "    \n",
    "    for i, example in enumerate(best_examples, 1):\n",
    "        print(f\"\\nBeispiel {i} (Score: {example['score']:.4f})\")\n",
    "        print(\"\\nReferenztext (Ausschnitt):\")\n",
    "        print(example['reference_text'][:200] + \"...\" if len(example['reference_text']) > 200 else example['reference_text'])\n",
    "        print(\"\\nGenerierter Text (Ausschnitt):\")\n",
    "        print(example['generated_text'][:200] + \"...\" if len(example['generated_text']) > 200 else example['generated_text'])\n",
    "    \n",
    "    # Schlechteste Beispiele nach ROUGE-L\n",
    "    print(\"\\n\\nTop 3 schlechteste Beispiele (nach ROUGE-L F1-Score):\")\n",
    "    worst_examples = extract_top_examples(results, model_name, \"rougeL_fmeasure\", top_n=3, highest=False)\n",
    "    \n",
    "    for i, example in enumerate(worst_examples, 1):\n",
    "        print(f\"\\nBeispiel {i} (Score: {example['score']:.4f})\")\n",
    "        print(\"\\nReferenztext (Ausschnitt):\")\n",
    "        print(example['reference_text'][:200] + \"...\" if len(example['reference_text']) > 200 else example['reference_text'])\n",
    "        print(\"\\nGenerierter Text (Ausschnitt):\")\n",
    "        print(example['generated_text'][:200] + \"...\" if len(example['generated_text']) > 200 else example['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielanalyse für ein bestimmtes Modell durchführen\n",
    "if results_df is not None and not results_df.empty:\n",
    "    model_to_analyze = results_df['model'].iloc[0]  # Erstes Modell in den Ergebnissen\n",
    "    analyze_model_examples(model_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9204c2c",
   "metadata": {},
   "source": [
    "## 6. Statistische Signifikanz\n",
    "\n",
    "Wir können auch testen, ob die Unterschiede zwischen den Modellen statistisch signifikant sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistische Signifikanz der Unterschiede zwischen Modellen berechnen\n",
    "try:\n",
    "    significance_df = compute_statistical_significance(results, metric=\"rougeL_fmeasure\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(significance_df, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.title(\"P-Werte für paarweise Vergleiche (ROUGE-L F1)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"significance_heatmap.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nStatistische Signifikanz (p-Werte):\")\n",
    "    print(\"Werte < 0.05 deuten auf signifikante Unterschiede hin\\n\")\n",
    "    print(significance_df)\n",
    "except Exception as e:\n",
    "    print(f\"Fehler bei der Berechnung der statistischen Signifikanz: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c71c5",
   "metadata": {},
   "source": [
    "## 7. Zusammenfassung\n",
    "\n",
    "Hier fassen wir die wichtigsten Erkenntnisse aus der Evaluierung zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rangliste der Modelle nach ROUGE-L F1-Score\n",
    "if 'rougeL' in results_df.columns:\n",
    "    ranked_models = results_df.sort_values(by='rougeL', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='model', y='rougeL', data=ranked_models)\n",
    "    plt.title(\"Rangliste der Modelle nach ROUGE-L F1-Score\", fontsize=16)\n",
    "    plt.xlabel(\"Modell\")\n",
    "    plt.ylabel(\"ROUGE-L F1-Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"model_ranking.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRangliste der Modelle nach ROUGE-L F1-Score:\")\n",
    "    for i, (idx, row) in enumerate(ranked_models.iterrows(), 1):\n",
    "        print(f\"{i}. {row['model']}: {row.get('rougeL', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0199b",
   "metadata": {},
   "source": [
    "## Fazit und nächste Schritte\n",
    "\n",
    "In diesem Notebook haben wir die Evaluierungspipeline für verschiedene LLMs zur Generierung von Pressemitteilungen aus Gerichtsurteilen ausgeführt und visualisiert. Die Ergebnisse zeigen die Leistung der verschiedenen Modelle anhand mehrerer Metriken, insbesondere ROUGE-Scores.\n",
    "\n",
    "**Erkenntnisse:**\n",
    "- [Hier können Erkenntnisse zur Modellleistung ergänzt werden]\n",
    "- [Vergleich der Modelle und ihre jeweiligen Stärken/Schwächen]\n",
    "\n",
    "**Nächste Schritte:**\n",
    "- Erweitern der Evaluierung um zusätzliche Metriken (z.B. BERTScore, menschliche Bewertungen)\n",
    "- Optimierung der Prompts für die besten Modelle\n",
    "- Detailliertere Fehleranalyse der schlechtesten Fälle\n",
    "- Anpassung der Generierungsparameter (Temperatur, max_tokens, etc.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
