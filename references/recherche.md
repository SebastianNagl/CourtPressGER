Alles klar. Ich suche nach Forschungsprojekten und wissenschaftlichen Arbeiten, die sich mit der automatisierten Generierung von Pressemitteilungen (und ggf. allgemeinen Zusammenfassungen) juristischer Urteile mithilfe von KI beschäftigen. Der Schwerpunkt liegt auf deutschsprachigen Rechtssystemen, mit europäischem und internationalem Fokus als Fallback. Ich werde auch darauf achten, welche Evaluationsmetriken zur Bewertung der generierten Texte verwendet wurden – von Standardmetriken wie BLEU, ROUGE und BERTScore bis hin zu juristisch-inhaltlichen Maßstäben wie Korrektheit und Verständlichkeit.

Ich melde mich, sobald ich Ergebnisse für dich habe.

# KI-gestützte Generierung von Pressemitteilungen und Urteilszusammenfassungen

## Hintergrund und Motivation

Gerichtsentscheidungen sind oft sehr umfangreich und komplex formuliert. Um juristische Informationen einem breiteren Publikum zugänglich zu machen – sei es in **Pressemitteilungen** der Gerichte oder als **kurze Zusammenfassungen (Leitsätze)** für Jurist:innen – werden zunehmend Verfahren der **automatischen Textzusammenfassung** erprobt. Insbesondere im deutschsprachigen Rechtsraum (Deutschland, Österreich, Schweiz) gewinnt dieses Forschungsfeld an Bedeutung ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=Historically%20speaking%2C%20the%20German%20legal,German%20court)). Ziel ist es, mit *künstlicher Intelligenz (KI)* lange Urteilstexte in verständliche Kurztexte zu verdichten, ohne wichtige Inhalte oder juristische Korrektheit zu verlieren. 

Dabei stellen sich zwei Hauptfragen: **(1)** Wie gut lassen sich juristische Entscheidungen mittels aktueller NLP-Methoden zusammenfassen (z.B. welche Daten und Modelle gibt es)? **(2)** Wie bewertet man die Qualität solcher automatisch generierten Zusammenfassungen, insbesondere im Hinblick auf juristische **Faktentreue**, **Verständlichkeit** und **Nützlichkeit** für Laien?

Im Folgenden geben wir einen strukturierten Überblick über wichtige Forschungsarbeiten und Projekte zu diesem Thema, mit Schwerpunkt auf dem deutschsprachigen Raum. Anschließend beleuchten wir die gängigen **Evaluationsmethoden** – von automatischen Metriken bis zu juristischen Qualitätskriterien. Eine Tabelle fasst zentrale Publikationen und Projekte mit ihren Eigenschaften zusammen.

## Forschungsstand im deutschsprachigen Raum

Im deutschen Rechtskreis wurden in den letzten Jahren einige bedeutende Schritte unternommen, um KI-gestützte Zusammenfassungen von Urteilen zu ermöglichen:

- **Neues Datenset und Basislinien (Deutschland)** – Forscher der TU München stellten 2021 erstmals ein großes Datenset von **100.000 deutschen Gerichtsentscheidungen mit Kurz-Zusammenfassungen** vor ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=legal%20case,laying%20the%20crucial%20groundwork%20for)). Diese Kurztexte stammen aus vorhandenen **Leitsätzen** bzw. redaktionellen Zusammenfassungen. Die Studie definierte damit die Aufgabe der automatischen **Urteilszusammenfassung** für deutschsprachige Texte und entwickelte sowohl **extraktive** als auch **abstraktive** Basis-Modelle ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=compression%20ratio%20among%20the%20most,laying%20the%20crucial%20groundwork%20for)). Das beste Modell (Transformer-basiert) erreichte einen ROUGE-1 Score von ~30.5 ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=pipeline%20tailored%20explicitly%20to%20the,laying%20the%20crucial%20groundwork%20for)) – ein erster Referenzwert für die Zusammenfassungsqualität. Dieses Werk („Summarization of German Court Rulings“) bildet einen Grundstein und zeigt zugleich die Herausforderungen auf: die **komplexe Struktur** deutscher Urteile (Rubrum, Tenor, Gründe, etc.) erfordert spezielle Vorverarbeitung und Modelle ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=legal%20case,research%20on%20German%20summarization%20systems)).

- **Leitsatz-Extraktion mit Argumentationsstruktur (Deutschland)** – Eine Folgestudie (Steffes & Rataj, 2022) konzentrierte sich auf Entscheidungen des **Bundesgerichtshofs (BGH)** und deren **amtliche Leitsätze**. Hier wurde untersucht, wie sich die **argumentative Struktur** eines Urteils für die Zusammenfassung nutzen lässt. Konkret selektiert das Verfahren anhand von Argumentationsrollen diejenigen **Schlüsselsätze**, die den Leitsatz bilden. Die Autoren zeigten, dass dieser Ansatz die Auswahl der richtigen Leitsatz-Sätze verbessert (gemessen an ROUGE) gegenüber rein statistischen Methoden. Dieses Verfahren ist extraktiv und nutzt Merkmale der juristischen Argumentation, um faktisch relevante Inhalte herauszufiltern.

- **Multilinguale Urteils-Zusammenfassung (Schweiz)** – Sehr aktuell ist ein Projekt aus der Schweiz (Rolshoven et al., 2024), das sich dem mehrsprachigen schweizerischen Rechtssystem widmet. Vorgestellt wurde das **SLDS-Datenset** („Swiss Leading Decision Summarization“), das **18.000 Leitentscheide des Schweizer Bundesgerichts** in den Sprachen Deutsch, Französisch und Italienisch umfasst – mitsamt **deutschen Regesten** (Zusammenfassungen) zu jedem Fall. Dieses Corpus erlaubt *Cross*-linguale Zusammenfassung: Ein Urteil etwa in Französisch soll automatisch in deutscher Zusammenfassung wiedergegeben werden. Erste Experimente fine-tunten dafür **mehrsprachige Transformer-Modelle (mT5)** auf den Datensätzen und vergleichen sie mit großen vortrainierten Modellen (z.B. GPT-4). Interessanterweise zeigte sich, dass feinjustierte **kleinere Modelle** ähnlich gut performen wie deutlich größere Modelle im Prompting-Modus. Als Bewertungsmetriken wurden hier neben ROUGE auch **BLEU**, **METEOR** und **BERTScore** herangezogen (siehe unten). Dieses Schweizer Projekt ist besonders relevant, da es **mehrsprachige juristische Texte** und deren konsistente Zusammenfassung adressiert – ein Szenario, das in der EU (mit mehreren Amtssprachen) ebenfalls auftritt.

- **Situation in Österreich:** Spezifische Veröffentlichungen zur KI-basierten Zusammenfassung österreichischer Gerichtsentscheidungen sind derzeit seltener. Allerdings sind die Herausforderungen vergleichbar, da auch österreichische Höchstgerichte **Leitsätze** zu Entscheidungen formulieren. Die Methoden aus Deutschland/Schweiz lassen sich prinzipiell auf österreichische Urteile übertragen, zumal die Sprache Deutsch ist. Bisher gibt es jedoch keine bekannten großen Forschungsprojekte eigens für österreichische Urteilszusammenfassungen – ein mögliches zukünftiges Feld für Legal Tech im österreichischen Justizwesen.

**Hinweis zu Pressemitteilungen:** Die oben genannten Arbeiten zielen primär auf **amtliche Leitsätze oder Kurzfassungen** für juristisches Fachpublikum ab. Eine **Pressemitteilung** zu einem Gerichtsurteil ist zwar ebenfalls eine Zusammenfassung, aber meist freier formuliert und für **Laien** gedacht. Spezifische Forschungsarbeiten, die explizit die Generierung von Pressemitteilungen simulieren, sind im deutschsprachigen Raum bislang kaum dokumentiert. Allerdings kann man annehmen, dass ein System, das Leitsätze faktentreu zusammenfassen kann, mit zusätzlicher **Sprachvereinfachung** auch pressetaugliche Texte erzeugen könnte. Die Verständlichkeit für Laien wird dabei zu einem wichtigen Kriterium (siehe Evaluierung).

## Europäische und internationale Ansätze

Sollte die deutschsprachige Literatur nicht ausreichen, lohnt der Blick auf **internationale Forschungsarbeiten** zur automatischen Zusammenfassung juristischer Texte. In den USA und Common-Law-Staaten liegt der Fokus oft auf der **Fallrecht-Zusammenfassung** (case summarization). Ein Beispiel ist Zhong et al. (2019), die mittels eines iterativen Algorithmus Schlüsselsätze identifizierten, um **US-Gerichtsurteile prägnant zusammenzufassen** ([A Comprehensive Survey on Legal Summarization: Challenges and Future Directions](https://arxiv.org/html/2501.17830v1#:~:text=Kevin%C2%A0D,Association%20for%20Computing%20Machinery%2C%20New)). Jüngst verarbeitete eine Studie sogar **430.000 US-Gerichtsentscheidungen** und erzeugte daraus Kurzfassungen; ein auf Reinforcement Learning basierendes Modell („MemSum“) schnitt dabei besser ab als Transformer-basierte Summarizer. In Großbritannien oder Australien existieren ebenfalls Arbeiten, meist mit extraktiven Methoden und kleineren Datensätzen aus veröffentlichten Urteilszusammenfassungen.

Auch in anderen Rechtssystemen gibt es vergleichbare Entwicklungen: So wurde mit *RulingBR* ein Datensatz mit über **10.000 brasilianischen Gerichtsentscheidungen des Obersten Gerichts** und dazugehörigen portugiesischen Zusammenfassungen publiziert. Für Indien wurde mit *ILDC* ein Korpus erstellt, um englische Zusammenfassungen hoher Gerichtsurteile zu generieren. Damit rücken auch nicht-englische Sprachen ins Blickfeld: Neben Deutsch etwa **Portugiesisch, Hindi, Italienisch** u.a.. Ein EU-weites Beispiel ist *EUR-Lex-Sum* (Aumiller et al., 2022), ein mehrsprachiger Datensatz zur Zusammenfassung von **EU-Gesetzestexten** – hier geht es zwar um Gesetzeskommentare statt Urteile, doch es zeigt den Trend zu multilingualen legal-NLP-Ressourcen. Insgesamt befinden sich viele dieser internationalen Ansätze noch im Forschungsstadium, liefern aber wertvolle Impulse (z.B. spezielle Modell-Architekturen oder Trainingsstrategien), von denen deutschsprachige Projekte profitieren können.

## Evaluierung der generierten Texte

Die **Bewertung** von automatisch generierten Urteilszusammenfassungen ist ein zentrales Thema, da klassische Metriken nicht ohne weiteres die *juristische Qualität* erfassen. Man unterscheidet grob **automatische Metriken** und **humane Evaluation nach fachlichen Kriterien**:

- **Automatische Metriken:** In fast allen Studien werden zunächst etablierte NLP-Metriken genutzt, um die KI-Zusammenfassung mit einer Referenz-Zusammenfassung zu vergleichen. **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) ist die meistverwendete Kennzahl, die auf n-Gram-Überschneidungen basiert. So reportierten z.B. Glaser et al. (2021) ROUGE-1=30.5 für ihr bestes Modell ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=pipeline%20tailored%20explicitly%20to%20the,laying%20the%20crucial%20groundwork%20for)). Auch **BLEU** (aus maschineller Übersetzung) wird manchmal herangezogen, insbesondere für mehrsprachige Zusammenfassungen (im Schweizer SLDS-Projekt etwa) neben **METEOR**. Da n-Gram-Metriken jedoch nur grobe Übereinstimmungen messen, greifen neuere Arbeiten zu **semantischen Metriken**: Ein Beispiel ist **BERTScore**, welches mittels BERT-Embeddings die semantische Ähnlichkeit zwischen generiertem Text und Referenz bewertet ([A Comprehensive Survey on Legal Summarization: Challenges and Future Directions](https://arxiv.org/html/2501.17830v1#:~:text=Among%20these%2C%20BERTScore%C2%A0%28Zhang%20et%C2%A0al,2022%3B%20Bannihatti%C2%A0Kumar%20et%C2%A0al)). Tatsächlich nutzen immer mehr Legal-NLP-Studien BERTScore, um über bloße Wortüberlappung hinaus die Inhaltsnähe zu messen ([A Comprehensive Survey on Legal Summarization: Challenges and Future Directions](https://arxiv.org/html/2501.17830v1#:~:text=Among%20these%2C%20BERTScore%C2%A0%28Zhang%20et%C2%A0al,2022%3B%20Bannihatti%C2%A0Kumar%20et%C2%A0al)). Trotz dieser Fortschritte haben automatische Metriken ihre Tücken: **In juristischen Zusammenhängen kann eine Zusammenfassung inhaltlich falsch oder unvollständig sein, obwohl ROUGE/BLEU hoch sind.** Steffes et al. (2023) zeigten explizit, dass **ROUGE als alleiniger Qualitätsindikator unzuverlässig ist**, da er juristisch relevante Inhalte nicht zuverlässig bewertet ([On evaluating legal summaries with ROUGE](https://dl.acm.org/doi/pdf/10.1145/3594536.3595150#:~:text=On%20evaluating%20legal%20summaries%20with,the%20quality%20of%20summarization%20algorithms)). Mit anderen Worten: Ein System könnte hohe ROUGE-Werte erzielen, aber dennoch wesentliche rechtliche Aussagen verfehlen. Um solche Fälle aufzudecken, wird an neuen Evaluationsmethoden geforscht (siehe unten).

- **Kriterien fachlicher Qualität:** Letztlich muss eine juristische Zusammenfassung von Expert:innen geprüft werden. Wichtige **Qualitätskriterien** sind dabei: **Faktentreue** (Stimmt der Inhalt mit den Fakten und dem Tenor des Originals überein? Enthält der Text keine Halluzinationen oder Verdrehungen?  ([Legal summarization - Anthropic API](https://docs.anthropic.com/en/docs/about-claude/use-case-guides/legal-summarization#:~:text=Legal%20summarization%20,%C2%B7%20Legal%20precision%20%C2%B7%20Conciseness))), **juristische Korrektheit/Präzision** (werden Rechtsnormen und Ergebnisse korrekt und präzise wiedergegeben? ([Legal summarization - Anthropic API](https://docs.anthropic.com/en/docs/about-claude/use-case-guides/legal-summarization#:~:text=Legal%20summarization%20,%C2%B7%20Legal%20precision%20%C2%B7%20Conciseness))), **Vollständigkeit wesentlicher Punkte** (deckt die Zusammenfassung alle entscheidungstragenden Aspekte ab?), **Verständlichkeit** (ist der Text klar und verständlich formuliert, ggf. in Laiensprache?) und **Nützlichkeit** für die Zielgruppe (kann ein/e Jurist:in bzw. Laie die relevanten Infos schnell entnehmen?). Die Bewertung entlang dieser Dimensionen erfordert häufig **Humanurteile**. Allerdings zeigt eine aktuelle Übersichtsarbeit, dass nur ca. *9%* der Studien tatsächlich Fachexpert:innen für Evaluation einbeziehen ([A Comprehensive Survey on Legal Summarization: Challenges and Future Directions](https://arxiv.org/html/2501.17830v1#:~:text=studies%20have%20backed%20up%20their,evaluation%20in%20the%20legal%20domain)) – vor allem aus Aufwand- und Kosten­gründen. Wenn humanes Rating erfolgt, dann oft durch Jurist:innen, die z.B. Schulnoten für Verständlichkeit und Korrektheit vergeben, oder durch Vergleich von vom System generierten Leitsätzen mit den offiziellen Leitsätzen. Manche Arbeiten schlagen ausgefeilte Verfahren vor: Xu und Ashley (2023) etwa präsentieren ein **Frage-Antwort-basiertes Evaluationsframework** mit GPT-4 ([[2309.15016] Question-Answering Approach to Evaluating Legal Summaries](https://arxiv.org/abs/2309.15016#:~:text=,4%20grading%20with%20human%20grading)). Dabei erzeugt GPT-4 zunächst Verständnisfragen zum Referenztext und beantwortet diese sowohl auf Basis der Referenz-Zusammenfassung als auch der KI-Zusammenfassung. Anschließend werden die Antworten verglichen und von GPT-4 oder Menschen bewertet ([[2309.15016] Question-Answering Approach to Evaluating Legal Summaries](https://arxiv.org/abs/2309.15016#:~:text=,4%20grading%20with%20human%20grading)). Erste Ergebnisse deuten an, dass solche *Q&A-Checks* besser mit dem Urteil von Jurist:innen korrelieren als simple ROUGE-Scores ([[2309.15016] Question-Answering Approach to Evaluating Legal Summaries](https://arxiv.org/abs/2309.15016#:~:text=structure%20into%20account%2C%20which%20is,4%20grading%20with%20human%20grading)). Ähnlich wurde vorgeschlagen, die **Abdeckung von Argumentationsstrukturen** als Maßstab zu nehmen – sprich zu prüfen, ob die Zusammenfassung alle Argumente (z.B. pro/contra) aus dem Original abbildet (vgl. Ashley & Litman, 2024). Insgesamt geht der Trend dahin, **mehrdimensionale Evaluationsansätze** zu verfolgen: Kombination mehrerer automatischer Metriken (um unterschiedliche Aspekte zu erfassen) und ergänzend gezielte **Expertenbewertungen** für kritische Kriterien wie Faktentreue.

## Laufende Projekte und Anwendung in der Praxis

Neben wissenschaftlichen Publikationen gibt es in deutschsprachigen Ländern auch **Praxisprojekte**, oft in Zusammenarbeit zwischen Justiz und Forschung, um KI-Tools für Urteilsveröffentlichungen zu entwickeln:

- **Projekt ALeKS (Deutschland)** – Unter dem Titel *„Anonymisierungs- und Leitsatzerstellungs-Kit zur smarten Veröffentlichung von Gerichtsentscheidungen“* (ALeKS) arbeiten seit 2023 mehrere Bundesländer und das Bundesjustizministerium an einer KI-Lösung, um mehr Urteile automatisiert veröffentlichen zu können ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=K%C3%BCnftig%20sollen%20mehr%20Gerichtsentscheidungen%20in,Automatisierung%20vereinfacht)). Das Projekt hat zwei Kernkomponenten: Zum einen ein **Anonymisierungstool**, das Personendaten in Urteilen erkennt und unkenntlich macht (hierfür wurde in Bayern mit der Universität Erlangen-Nürnberg bereits ein Prototyp entwickelt) ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=In%20einem%20ersten%20Schritt%20wird,bereits%20einen%20ersten%20Prototyp%20entwickelt)) ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=Prototyps%20aus%20dem%20bayerischen%20Forschungsprojekt,auch%20die%20Benutzerfreundlichkeit%20der%20Software)). Zum anderen – und für dieses Thema relevant – soll der Prozess der **Leitsatzerstellung automatisiert** werden. Geplant ist der Einsatz von **Large Language Models (LLMs)**, um aus dem Urteilstext einen aussagekräftigen Leitsatz zu generieren ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=In%20einem%20zweiten%20Schritt%20soll,der%20Entscheidung%20sollen%20automatisiert%20erfolgen)). Diese automatisch erstellten Zusammenfassungen würden den Richtern als Entwurf dienen, die dann redaktionell nacharbeiten könnten. ALeKS befindet sich aktuell in der Entwicklungsphase; erste Tests der Anonymisierung verliefen vielversprechend, und die Integration der LLM-Komponente für Zusammenfassungen ist als nächstes vorgesehen ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=In%20einem%20zweiten%20Schritt%20soll,der%20Entscheidung%20sollen%20automatisiert%20erfolgen)). Dieses Vorhaben zeigt das Interesse der Justiz, KI praktisch einzusetzen, um **Publikationsquoten** von Entscheidungen zu steigern – ohne Qualitätsabstriche bei Datenschutz und inhaltlicher Richtigkeit.

- **Open Access und Dateninitiativen** – In der Schweiz werden die Bundesgerichtsurteile bereits mit offiziellen Regesten veröffentlicht, was Projekte wie das oben erwähnte SLDS-Datenset ermöglicht hat. Das Team der Universität Bern kooperiert hier indirekt mit dem Bundesgericht, indem es die öffentlich verfügbaren Entscheidungen von der Plattform **Entscheidsuche** nutzt. Ähnlich öffnen immer mehr Gerichte ihre Rechtsprechungsdaten (Deutschland plant z.B. eine Erhöhung von unter 3% Veröffentlichung Richtung mehr Transparenz ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=Gegenw%C3%A4rtig%20werden%20in%20Deutschland%20%E2%80%93,ver%C3%B6ffentlichten%20Gerichtsentscheidungen%20sogar%20noch%20darunter))). Solche offenen Daten sind die Grundlage, auf der **Forschungsprojekte an Universitäten** Modelle trainieren können. Eine enge Verzahnung von Gerichten, Ministerien und Forschungseinrichtungen ist hierbei ideal: Fachleute aus der Praxis definieren Anforderungen (z.B. **Verständlichkeit für Laien** bei Pressemitteilungen), während KI-Expert:innen die technischen Lösungen entwickeln. 

- **Weiterer Ausblick:** International laufen vergleichbare Projekte. So testet etwa das britische Justizministerium KI-gestützte Tools, um Urteile für Bürger verständlicher aufzubereiten (ähnlich einer Pressemitteilung in einfacher Sprache). In den USA experimentieren kommerzielle Anbieter (Westlaw, LexisNexis) mit automatischen Case Summaries für Anwälte. Diese Praxisprojekte bestätigen den Forschungsbefund, dass **automatisierte Zusammenfassungen** enormes Potenzial zur Effizienzsteigerung bieten – jedoch nur, wenn die **inhaltliche Qualität** sichergestellt ist.

## Fazit

**KI-generierte Urteilszusammenfassungen** – seien es presseähnliche Mitteilungen oder prägnante Leitsätze – sind ein aktives Forschungsfeld im deutschsprachigen Raum. Erste Arbeiten für deutsche und schweizerische Urteile zeigen die Machbarkeit auf großen Datensätzen ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=legal%20case,research%20on%20German%20summarization%20systems)). Moderne Transformermodelle können juristische Texte bereits einigermaßen sinnvoll kondensieren, müssen aber weiter verbessert werden, um mit den von Menschen verfassten Zusammenfassungen gleichzuziehen. Besonders die **Evaluation** bleibt eine Herausforderung: Automatische Kennzahlen wie ROUGE oder BLEU liefern Anhaltspunkte, greifen aber zu kurz, wenn es um juristische Richtigkeit und Verständlichkeit geht ([On evaluating legal summaries with ROUGE](https://dl.acm.org/doi/pdf/10.1145/3594536.3595150#:~:text=On%20evaluating%20legal%20summaries%20with,the%20quality%20of%20summarization%20algorithms)) ([Legal summarization - Anthropic API](https://docs.anthropic.com/en/docs/about-claude/use-case-guides/legal-summarization#:~:text=Legal%20summarization%20,%C2%B7%20Legal%20precision%20%C2%B7%20Conciseness)). Hier sind hybride Ansätze gefragt – Kombinationen aus intelligenten Metriken (etwa BERTScore, Q&A-Tests) und Expertenfeedback.

Für den deutschsprachigen Kontext sind die Aussichten positiv: Durch Projekte wie ALeKS werden Forschungsergebnisse in realen Gerichtsalltag überführt, und die wachsende Verfügbarkeit von juristischen Textdaten (Open Data) begünstigt weitere Durchbrüche. Künftige Systeme könnten Pressestellen von Gerichten entlasten, indem sie **auf Knopfdruck eine erste Entwurfspressemitteilung** zu einem Urteil liefern, die dann nur noch feinjustiert werden muss. Bis dahin gilt es jedoch, in enger Abstimmung zwischen Jurist:innen und Informatiker:innen die Modelle zu trainieren – mit Augenmerk auf **Faktentreue**, **Klarheit** und **Mehrsprachigkeit**. Die interdisziplinären Forschungsprojekte, die derzeit im Gange sind, legen hierfür den Grundstein.

## Wichtige Publikationen und Projekte (Übersichtstabelle)

| **Titel / Projekt**                                                                                          | **Jahr** | **Autor:innen / Beteiligte**                              | **Institution(en)**                           | **Rechtssystem-Fokus**             | **Texttyp**                               | **KI-Methode**                                    | **Evaluationsmetriken**                            |
|--------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------|-----------------------------------------------|------------------------------------|-----------------------------------------|----------------------------------------------------|----------------------------------------------------|
| *Summarization of German Court Rulings* ([Summarization of German Court Rulings - ACL Anthology](https://aclanthology.org/2021.nllp-1.19/#:~:text=legal%20case,research%20on%20German%20summarization%20systems))                                                  | 2021     | Ingo Glaser, Sebastian Moser, Florian Matthes             | TU München                                     | Deutschland (Zivil-/Strafgerichte) | Urteil **Kurzfassung** (Leitsatz)       | Extraktiv + Abstraktiv (Transformer-basierte Modelle, Pointer-Netzwerke) | ROUGE (1/2/L), teilweise Vergleich mit Referenztexten |
| *Legal Text Summarization Using Argumentative Structures*                                | 2022     | Bianca Steffes, Piotr Rataj                                | Universität des Saarlandes, ZRD Saarbrücken   | Deutschland (BGH Zivilsenate)      | Urteil **Leitsatz** (Guiding Principle) | Extraktiv (satzweise Auswahl per Argumentations-Score) | ROUGE (Bewertung der Auswahlqualität)              |
| *Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland* | 2024     | Luca Rolshoven *et al.*                                    | Univ. Bern; Berner Fachhochschule u.a.         | Schweiz (Bundesgericht, mehrsprachig) | Urteil **Regeste** (Headnote)         | Abstraktiv (feinjustiertes mT5; Vergleich mit GPT-4) | ROUGE, BLEU, **METEOR**, **BERTScore**              |
| *On evaluating legal summaries with ROUGE* ([On evaluating legal summaries with ROUGE](https://dl.acm.org/doi/pdf/10.1145/3594536.3595150#:~:text=On%20evaluating%20legal%20summaries%20with,the%20quality%20of%20summarization%20algorithms))                                              | 2023     | Bianca Steffes *et al.*                                    | Universität des Saarlandes                    | (Allg.) – getestet an deutschen BGH-Leitsätzen     | **Evaluationsstudie** (Meta-Paper)    | – (Vergleich verschiedener ROUGE-Varianten)         | Abgleich mit **menschl. Bewertung** (Korrelation)   |
| *Question-Answering Approach to Evaluating Legal Summaries* ([[2309.15016] Question-Answering Approach to Evaluating Legal Summaries](https://arxiv.org/abs/2309.15016#:~:text=,4%20grading%20with%20human%20grading))                             | 2023     | Huihui Xu, Kevin D. Ashley                                 | Univ. Pittsburgh (USA)                        | (Allg.) – legal opinions (Common Law) | **Evaluationsframework**               | GPT-4 generiert Q&A zur Inhaltsüberprüfung          | Korrelation mit menschl. Urteil; kein klassischer Score |
| **Projekt ALeKS** – *Anonymisierungs- und Leitsatzerstellungs-Kit* ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=K%C3%BCnftig%20sollen%20mehr%20Gerichtsentscheidungen%20in,Automatisierung%20vereinfacht)) ([BMJ  -  Digitalisierungsvorhaben der Länder - Gerichtsentscheidungen durch KI leichter veröffentlichen](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=In%20einem%20zweiten%20Schritt%20soll,der%20Entscheidung%20sollen%20automatisiert%20erfolgen))      | 2023–25  | Bayerisches Jusitzministerium; FAU Erlangen; Bund u.a.    | Justizbehörden DE (Länder + BMJ); Uni-Forschung | Deutschland (alle Gerichte)        | Urteil **Leitsatz** / Presse-Notiz      | Geplant: Large Language Models (Generierung + Verschlagwortung) | Geplant: Expertenreview, Pilot mit Justizpraktikern | 

*p*: Pressemitteilung; *Leitsatz*: amtlicher Leitsatz; *Regeste*: Entscheidungsregeste (Schweiz); *Evaluationsstudie*: untersucht Bewertungsmethoden statt eigenes Summarisierungsmodell.