{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa951390",
   "metadata": {},
   "source": [
    "# CourtPressGER: German Court Press Release Dataset\n",
    "\n",
    "- Ein Datensatz bestehend aus :\n",
    "-   6,5k deutscher höchstgerichtlicher Urteile und zugehöriger Pressemitteilungen und\n",
    "-   zugehörige synthetische Prompts, um aus den Urteilen Pressemitteilungen zu generieren\n",
    "- Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4dc6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Beschleunigung Setup\n",
    "try:\n",
    "    import cudf\n",
    "    import cuml\n",
    "    import cupy as cp\n",
    "    from cuml.metrics import pairwise_distances\n",
    "    from cuml.preprocessing import normalize\n",
    "    USE_GPU = True\n",
    "    print(\"GPU-Beschleunigung ist aktiviert (RAPIDS-Bibliotheken geladen)\")\n",
    "except ImportError:\n",
    "    USE_GPU = False\n",
    "    print(\"GPU-Beschleunigung ist nicht verfügbar, verwende CPU-Version\")\n",
    "\n",
    "# Standard Data Science Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Transformer & NLP\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setze Warnungen und Seed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "if USE_GPU:\n",
    "    cp.random.seed(42)\n",
    "\n",
    "# Lade die Daten\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        df = cudf.read_csv('data/german_courts.csv')\n",
    "        print(\"Daten mit GPU-Beschleunigung geladen\")\n",
    "    else:\n",
    "        df = pd.read_csv('data/german_courts.csv')\n",
    "        print(\"Daten mit CPU geladen\")\n",
    "    print(f\"Datensatzgröße: {len(df)} Einträge\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden der Daten: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d44be7",
   "metadata": {},
   "source": [
    "## Model Setup und Training\n",
    "\n",
    "In diesem Abschnitt laden und trainieren wir die Modelle für die Verarbeitung der Gerichtsurteile und Pressemitteilungen.\n",
    "Wir nutzen die GPU-Beschleunigung wo möglich, mit Fallback auf CPU-Implementierungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade bereinigte Daten\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        clean_df = cudf.read_csv('cleaned_data/cleaned_combined_methods.csv')\n",
    "    else:\n",
    "        clean_df = pd.read_csv('cleaned_data/cleaned_combined_methods.csv')\n",
    "    print(f\"Bereinigte Daten geladen: {len(clean_df)} Einträge\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden der bereinigten Daten: {e}\")\n",
    "\n",
    "# Vorbereitung der Modelle\n",
    "def prepare_model_inputs(texts, tokenizer, max_length=512):\n",
    "    \"\"\"Bereitet Texte für die Modellverarbeitung vor\"\"\"\n",
    "    # Tokenisierung mit Padding und Truncation\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    if USE_GPU:\n",
    "        return {k: v.cuda() for k, v in inputs.items()}\n",
    "    return inputs\n",
    "\n",
    "# Lade Modell und Tokenizer\n",
    "model_name = 'deepset/gbert-large'  # Deutsches BERT Modell\n",
    "print(f\"Lade {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "    print(\"Modell auf GPU geladen\")\n",
    "else:\n",
    "    print(\"Modell auf CPU geladen\")\n",
    "\n",
    "# Sentence Transformer für Ähnlichkeitsvergleiche\n",
    "st_model = SentenceTransformer('T-Systems-onsite/german-roberta-sentence-transformer-v2')\n",
    "if USE_GPU:\n",
    "    st_model = st_model.cuda()\n",
    "\n",
    "# Beispiel-Inference\n",
    "sample_size = min(5, len(clean_df))\n",
    "sample_texts = clean_df['summary'].head(sample_size).to_pandas() if USE_GPU else clean_df['summary'].head(sample_size)\n",
    "\n",
    "print(\"\\nFühre Beispiel-Inference durch...\")\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_model_inputs(sample_texts.tolist(), tokenizer)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "print(f\"Embedding-Dimensionen: {embeddings.shape}\")\n",
    "\n",
    "if USE_GPU:\n",
    "    # Berechne Ähnlichkeitsmatrix mit cupy\n",
    "    embeddings_gpu = cp.array(embeddings.cpu().numpy())\n",
    "    similarity_matrix = cp.matmul(embeddings_gpu, embeddings_gpu.T)\n",
    "    print(\"\\nÄhnlichkeitsmatrix (GPU-beschleunigt):\")\n",
    "    print(cp.asnumpy(similarity_matrix))\n",
    "else:\n",
    "    # Berechne Ähnlichkeitsmatrix auf CPU\n",
    "    similarity_matrix = np.matmul(embeddings.numpy(), embeddings.numpy().T)\n",
    "    print(\"\\nÄhnlichkeitsmatrix (CPU):\")\n",
    "    print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862d90a",
   "metadata": {},
   "source": [
    "## Training und Evaluation\n",
    "\n",
    "In diesem Abschnitt trainieren wir das Modell auf den Trainingsdaten und evaluieren die Performance.\n",
    "Die GPU-Beschleunigung wird für Batch-Verarbeitung und Matrixoperationen genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Batch-Größe basierend auf verfügbarem GPU-Speicher anpassen\n",
    "BATCH_SIZE = 32 if USE_GPU else 16\n",
    "\n",
    "def create_dataloader(texts, labels, tokenizer):\n",
    "    \"\"\"Erstellt einen DataLoader für das Training\"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    if USE_GPU:\n",
    "        input_ids = inputs['input_ids'].cuda()\n",
    "        attention_mask = inputs['attention_mask'].cuda()\n",
    "        labels = torch.tensor(labels).cuda()\n",
    "    else:\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Erstelle Train/Val Split\n",
    "train_texts = clean_df['judgement'].sample(frac=0.8, random_state=42)\n",
    "val_texts = clean_df['judgement'].drop(train_texts.index)\n",
    "train_labels = clean_df['summary'][train_texts.index]\n",
    "val_labels = clean_df['summary'][val_texts.index]\n",
    "\n",
    "# Konvertiere zu Listen für die Tokenisierung\n",
    "if USE_GPU:\n",
    "    train_texts = train_texts.to_pandas().tolist()\n",
    "    val_texts = val_texts.to_pandas().tolist()\n",
    "    train_labels = train_labels.to_pandas().tolist()\n",
    "    val_labels = val_labels.to_pandas().tolist()\n",
    "else:\n",
    "    train_texts = train_texts.tolist()\n",
    "    val_texts = val_texts.tolist()\n",
    "    train_labels = train_labels.tolist()\n",
    "    val_labels = val_labels.tolist()\n",
    "\n",
    "# Erstelle DataLoader\n",
    "train_dataloader = create_dataloader(train_texts, train_labels, tokenizer)\n",
    "val_dataloader = create_dataloader(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Training Loop mit GPU-Beschleunigung\n",
    "def train_epoch(model, dataloader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return (\n",
    "        total_loss / len(dataloader),\n",
    "        classification_report(all_labels, all_predictions)\n",
    "        confusion_matrix(all_labels, all_predictions)\n",
    "    )\n",
    "\n",
    "# Speichern der Modelle\n",
    "def save_model(model, epoch, optimizer, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "\n",
    "# Haupttrainingsschleife\n",
    "print(\"Starte Training...\")\n",
    "NUM_EPOCHS = 3\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoche {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    val_loss, val_report, val_cm = evaluate(model, val_dataloader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(\"\\nKlassifikationsbericht:\")\n",
    "    print(val_report)\n",
    "    \n",
    "    # Speichere bestes Modell\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_model(model, epoch, optimizer, val_loss, 'best_model.pt')\n",
    "        print(\"Neues bestes Modell gespeichert!\")\n",
    "\n",
    "print(\"Training abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
