@article{banerjee2005,
  title = {{{METEOR}}: {{An Automatic Metric}} for {{MT Evaluation}} with {{Improved Correlation}} with {{Human Judgments}}},
  author = {Banerjee, Satanjeev and Lavie, Alon},
  year = {2005},
  abstract = {We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.},
  langid = {english},
  keywords = {,notion},
  file = {/Users/sebastiannagl/Zotero/storage/92VVMK4X/Proceedings of the....pdf}
}

@misc{chang2024,
  title = {{{BooookScore}}: {{A}} Systematic Exploration of Book-Length Summarization in the Era of {{LLMs}}},
  shorttitle = {{{BooookScore}}},
  author = {Chang, Yapei and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
  year = {2024},
  month = apr,
  number = {arXiv:2310.00785},
  eprint = {2310.00785},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.00785},
  urldate = {2025-04-09},
  abstract = {Summarizing book-length documents ({$>$}100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BOOOOKSCORE, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BOOOOKSCORE has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \$15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BOOOOKSCORE than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BOOOOKSCORE but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/Users/sebastiannagl/Zotero/storage/XPI2CCIU/Chang et al. - 2024 - BooookScore A systematic exploration of book-length summarization in the era of LLMs.pdf}
}

@inproceedings{glaser2021,
  title = {Summarization of {{German Court Rulings}}},
  booktitle = {Proceedings of the {{Natural Legal Language Processing Workshop}} 2021},
  author = {Glaser, Ingo and Moser, Sebastian and Matthes, Florian},
  year = {2021},
  pages = {180--189},
  publisher = {Association for Computational Linguistics},
  address = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.nllp-1.19},
  urldate = {2025-05-05},
  abstract = {Historically speaking, the German legal language is widely neglected in NLP research, especially in summarization systems, as most of them are based on English newspaper articles. In this paper, we propose the task of automatic summarization of German court rulings. Due to their complexity and length, it is of critical importance that legal practitioners can quickly identify the content of a verdict and thus be able to decide on the relevance for a given legal case. To tackle this problem, we introduce a new dataset consisting of 100k German judgments with short summaries. Our dataset has the highest compression ratio among the most common summarization datasets. German court rulings contain much structural information, so we create a pre-processing pipeline tailored explicitly to the German legal domain. Additionally, we implement multiple extractive as well as abstractive summarization systems and build a wide variety of baseline models. Our best model achieves a ROUGE-1 score of 30.50. Therefore with this work, we are laying the crucial groundwork for further research on German summarization systems.},
  langid = {english},
  keywords = {,notion},
  file = {/Users/sebastiannagl/Zotero/storage/BII3XCF9/Glaser et al. - 2021 - Summarization of German Court Rulings.pdf}
}

@misc{kryscinski2019,
  title = {Evaluating the {{Factual Consistency}} of {{Abstractive Text Summarization}}},
  author = {Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  year = {2019},
  month = oct,
  number = {arXiv:1910.12840},
  eprint = {1910.12840},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.12840},
  urldate = {2025-04-08},
  abstract = {Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,notion},
  file = {/Users/sebastiannagl/Zotero/storage/MGY6ILDW/Kryściński et al. - 2019 - Evaluating the Factual Consistency of Abstractive Text Summarization.pdf}
}

@article{lin2004,
  title = {{{ROUGE}}: {{A Package}} for {{Automatic Evaluation}} of {{Summaries}}},
  author = {Lin, Chin-Yew},
  year = {2004},
  abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.},
  langid = {english},
  keywords = {,notion},
  file = {/Users/sebastiannagl/Zotero/storage/8AQQYGY8/Lin - ROUGE A Package for Automatic Evaluation of Summa.pdf}
}

@inproceedings{papineni2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {{{BLEU}}},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}  - {{ACL}} '02},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2002},
  pages = {311},
  publisher = {Association for Computational Linguistics},
  address = {Philadelphia, Pennsylvania},
  doi = {10.3115/1073083.1073135},
  urldate = {2025-03-31},
  langid = {english},
  keywords = {notion},
  file = {/Users/sebastiannagl/Zotero/storage/BUG4L5W7/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf}
}

@misc{rolshoven2024,
  title = {Unlocking {{Legal Knowledge}}: {{A Multilingual Dataset}} for {{Judicial Summarization}} in {{Switzerland}}},
  shorttitle = {Unlocking {{Legal Knowledge}}},
  author = {Rolshoven, Luca and Rasiah, Vishvaksenan and Bose, Srinanda Br{\"u}gger and St{\"u}rmer, Matthias and Niklaus, Joel},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13456},
  eprint = {2410.13456},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13456},
  urldate = {2025-05-05},
  abstract = {Legal research is a time-consuming task that most lawyers face on a daily basis. A large part of legal research entails looking up relevant caselaw and bringing it in relation to the case at hand. Lawyers heavily rely on summaries (also called headnotes) to find the right cases quickly. However, not all decisions are annotated with headnotes and writing them is time-consuming. Automated headnote creation has the potential to make hundreds of thousands of decisions more accessible for legal research in Switzerland alone. To kickstart this, we introduce the Swiss Leading Decision Summarization (SLDS) dataset, a novel cross-lingual resource featuring 18K court rulings from the Swiss Federal Supreme Court (SFSC), in German, French, and Italian, along with German headnotes. We fine-tune and evaluate three mT5 variants, along with proprietary models. Our analysis highlights that while proprietary models perform well in zero-shot and one-shot settings, fine-tuned smaller models still provide a strong competitive edge. We publicly release the dataset to facilitate further research in multilingual legal summarization and the development of assistive technologies for legal professionals.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/Users/sebastiannagl/Zotero/storage/S5MA74CZ/Rolshoven et al. - 2024 - Unlocking Legal Knowledge A Multilingual Dataset for Judicial Summarization in Switzerland.pdf}
}

@incollection{steffes2022,
  title = {Legal {{Text Summarization Using Argumentative Structures}}},
  booktitle = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  author = {Steffes, Bianca and Rataj, Piotr},
  editor = {Francesconi, Enrico and Borges, Georg and Sorge, Christoph},
  year = {2022},
  month = dec,
  publisher = {IOS Press},
  doi = {10.3233/FAIA220474},
  urldate = {2025-05-05},
  abstract = {Legal text summarization focuses on the automated creation of summaries for legal texts. We show that the argumentative structure of judgments can improve the selection of guiding principles as a specific kind of summary using judgments of the German Federal Court of Justice as measured by the ROUGE metric. We evaluate our first results and put them in the context of our ongoing work.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  isbn = {978-1-64368-364-5 978-1-64368-365-2},
  langid = {english},
  keywords = {,notion},
  file = {/Users/sebastiannagl/Zotero/storage/UF9X2SPY/Steffes and Rataj - 2022 - Legal Text Summarization Using Argumentative Structures.pdf}
}

@inproceedings{steffes2023,
  title = {On Evaluating Legal Summaries with {{ROUGE}}},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Steffes, Bianca and Rataj, Piotr and Burger, Luise and Roth, Lukas},
  year = {2023},
  month = jun,
  pages = {457--461},
  publisher = {ACM},
  address = {Braga Portugal},
  doi = {10.1145/3594536.3595150},
  urldate = {2025-05-05},
  abstract = {ROUGE is the most commonly used measure for evaluating summarization algorithms in practice. However, it is questionable whether ROUGE scores adequately reflect the quality of summaries in terms of content. We introduce a metric to measure (legal) content of summaries based on exhaustiveness and concreteness and compare ROUGE scores with values generated by legal experts according to our metric on two tasks. Our results show that ROUGE does not reliably gauge legal content and thus should not be used as a single indicator for the quality of summarization algorithms. For our particular use case we furthermore show one way to increase the reliability of ROUGE by pre-selecting sentences.},
  isbn = {979-8-4007-0197-9},
  langid = {english},
  keywords = {,notion},
  file = {/Users/sebastiannagl/Zotero/storage/R3NPYHYV/Steffes et al. - 2023 - On evaluating legal summaries with ROUGE.pdf}
}

@misc{wang2020,
  title = {Asking and {{Answering Questions}} to {{Evaluate}} the {{Factual Consistency}} of {{Summaries}}},
  author = {Wang, Alex and Cho, Kyunghyun and Lewis, Mike},
  year = {2020},
  month = apr,
  number = {arXiv:2004.04228},
  eprint = {2004.04228},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.04228},
  urldate = {2025-04-09},
  abstract = {Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose an automatic evaluation protocol called QAGS1 that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,notion},
  file = {/Users/sebastiannagl/Zotero/storage/TF76RB3L/Wang et al. - 2020 - Asking and Answering Questions to Evaluate the Factual Consistency of Summaries.pdf}
}

@incollection{xu2023a,
  title = {Question-{{Answering Approach}} to {{Evaluating Legal Summaries}}},
  author = {Xu, Huihui and Ashley, Kevin},
  year = {2023},
  eprint = {2309.15016},
  primaryclass = {cs},
  doi = {10.3233/FAIA230977},
  urldate = {2025-05-05},
  abstract = {Traditional evaluation metrics like ROUGE compare lexical overlap between the reference and generated summaries without taking argumentative structure into account, which is important for legal summaries. In this paper, we propose a novel legal summarization evaluation framework that utilizes GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary. GPT-4 is then used to produce answers based on the generated summary for the questions from the reference summary. Finally, GPT-4 grades the answers from the reference summary and the generated summary. We examined the correlation between GPT-4 grading and human grading. The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,notion},
  file = {/Users/sebastiannagl/Zotero/storage/T2F8BWHL/Xu and Ashley - 2023 - Question-Answering Approach to Evaluating Legal Summaries.pdf}
}

@misc{zhang2020,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  shorttitle = {{{BERTScore}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09675},
  eprint = {1904.09675},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.09675},
  urldate = {2025-03-31},
  abstract = {We propose BERTSCORE, an automatic evaluation metric for text generation. Analogously to common metrics, BERTSCORE computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTSCORE correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTSCORE is more robust to challenging examples when compared to existing metrics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,notion},
  file = {/Users/sebastiannagl/Zotero/storage/39D323KY/Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf}
}
