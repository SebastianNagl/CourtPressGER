{
 "cells": [
  {
   "cell_type": "raw",
   "id": "afbc90b4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"CourtPressGER\"\n",
    "author:\n",
    "  - name: Sebastian Nagl\n",
    "    affiliation: Technical University of Munich\n",
    "  - name: Mohamed Elganayni\n",
    "    affiliation: Technical University of Munich\n",
    "  - name: Melanie Pospisil\n",
    "    affiliation: Technical University of Munich\n",
    "  - name: Rusheel Iyer\n",
    "    affiliation: Technical University of Munich\n",
    "  - name: Matthias Grabmair\n",
    "    affiliation: Technical University of Munich\n",
    "format:\n",
    "  pdf:\n",
    "    documentclass: article\n",
    "    include-in-header:\n",
    "      text: |\n",
    "        \\usepackage[review]{acl}\n",
    "        \\usepackage{times}\n",
    "        \\usepackage{latexsym}\n",
    "        \\usepackage{booktabs}\n",
    "        \\usepackage{longtable}\n",
    "        \\usepackage{pdflscape}\n",
    "        \\usepackage{makecell}\n",
    "        \\usepackage[T1]{fontenc}\n",
    "        \\usepackage[utf8]{inputenc}\n",
    "        \\usepackage{microtype}\n",
    "        \\usepackage{inconsolata}\n",
    "        \\usepackage{graphicx}\n",
    "        \\graphicspath{{./figures}}\n",
    "    keep-tex: true\n",
    "bibliography: CourtPressGER.bib\n",
    "cite-method: natbib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca81c97",
   "metadata": {},
   "source": [
    "```{=latex}\n",
    "\\begin{abstract}\n",
    "We present CourtPressGER, a system for automatically generating German court press releases with Large Language Models (LLMs). We compile a curated dataset of \\textbf{6.4\\ k\\ pairs}  of court decisions and their officially published press releases from Germany's highest federal courts and the Federal Constitutional Court. Each pair is accompanied by a \\emph{synthetic prompt} that enables the automatic generation of press releases from the full decision text. We describe a modular pipeline that queries state-of-the-art models of different sizes and evaluate the outputs with a multidimensional protocol combining reference-based metrics, factual-consistency checks and an LLM-as-judge approach that approximates expert review. The results show that large general-purpose LLMs can already deliver press releases that approach the quality of human drafts, while a hierarchical summarisation strategy allows smaller models to remain competitive. CourtPressGER illustrates the potential of LLMs to support judicial communication and provides a public benchmark for future research.\n",
    "\\end{abstract}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f581db",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4dd6f",
   "metadata": {},
   "source": [
    "The German legal system consists of a complex network of courts that regularly publish extensive decisions. To make these decisions accessible to the public, the highest courts create press releases that summarize the essential aspects and implications of the decisions in an understandable form. These press releases serve as an important interface between the judicial system and the general public by explaining complex legal matters in an accessible way and serve as a proxy for the task of legal case summarization, for which manually created gold data is typically sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac869a8",
   "metadata": {},
   "source": [
    "However, the manual creation of such press releases requires significant resources. Recent progress in LLMs suggests that high‑quality automatic drafts are within reach, provided adequate training data and evaluation protocols are available. CourtPressGER addresses this gap by:\n",
    "\n",
    "1. Collecting the largest aligned corpus of German decisions and press releases to date, \n",
    "2. deriving decision‑specific instruction prompts, \n",
    "3. benchmarking a range of open and commercial LLMs, and\n",
    "4. analysing their outputs through complementary automatic and expert‑level measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b805e",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ae815",
   "metadata": {},
   "source": [
    "Legal text summarization has been an active area of research for several decades. Early approaches relied on statistical methods and extractive summarization techniques to select the most important sentences from legal documents. With the advent of neural network models, more sophisticated abstractive summarization methods became possible, allowing for the generation of new text that captures the essence of the original document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6bca5",
   "metadata": {},
   "source": [
    "In the German legal domain, several notable research efforts have focused on court decision summarization. The focus of these studies has been on official headnotes (\"Leitsätze\") as they are mainly extractive summaries from the judgement that are written by the judges themselves. These headnotes are typically short and concise, making them suitable for extractive summarization tasks and can in general be found verbatim in the body of the decision. However, they do not provide a comprehensive overview of the entire decision and are not intended for public communication. In contrast, press releases are designed to be more accessible to the general public and provide a broader context for the decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ec908",
   "metadata": {},
   "source": [
    "@glaser2021 presented the first large dataset of 100.000 German court decisions with corresponding summaries, establishing baseline models for German legal summarization. Their transformer-based approach achieved a ROUGE-1 F1 score of approximately 30.5%, demonstrating both the feasibility and challenges of the task. The complex structure of German court decisions (including sections like \"Rubrum,\" \"Tenor,\" and \"Gründe\") requires specialized preprocessing and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4eb9e6",
   "metadata": {},
   "source": [
    "@steffes2022 focused on extracting official headnotes (\"Leitsätze\") from Federal Court of Justice (BGH) decisions by utilizing the argumentative structure of rulings. Their approach selected key sentences based on their argumentative roles, improving the selection of headnote sentences compared to purely statistical methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0beddcf",
   "metadata": {},
   "source": [
    "\n",
    "For multilingual court summarization, @rolshoven2024 introduced the SLDS dataset (Swiss Leading Decision Summarization) containing 18,000 Swiss Federal Court decisions in German, French, and Italian, along with German summaries (\"Regesten\"). Their work on cross-lingual summarization demonstrated that fine-tuned smaller models could perform similarly to large pre-trained models in prompt mode. They evaluated their approach using ROUGE, BLEU, METEOR, and BERTScore metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1db09",
   "metadata": {},
   "source": [
    "Regarding evaluation methodologies, @steffes2023 explicitly showed that ROUGE is unreliable as a sole quality indicator for legal summaries since it fails to reliably assess legally relevant content. Their study demonstrated that a system might achieve high ROUGE scores while missing essential legal statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16348a95",
   "metadata": {},
   "source": [
    "For more robust evaluation, @xu2023a presented a question-answering framework using LLMs to assess the factual correctness of legal summaries. Their approach generates understanding questions about the reference text and compares answers derived from both reference and generated summaries, showing better correlation with expert judgments than simple ROUGE scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4efdfb",
   "metadata": {},
   "source": [
    "In practical applications, the [ALeKS project](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=K%C3%BCnftig%20sollen%20mehr%20Gerichtsentscheidungen%20in,Automatisierung%20vereinfacht) (Anonymisierungs- und Leitsatzerstellungs-Kit) is being developed in Germany to automatically anonymize court decisions and generate headnotes using LLMs. This collaboration between judicial authorities and research institutions aims to increase the publication rate of court decisions while maintaining content accuracy and data protection standards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a74bc",
   "metadata": {},
   "source": [
    "\n",
    "Our work extends these efforts by specifically focusing on press release generation (rather than technical headnotes) for German court decisions, emphasizing both factual correctness and accessibility for non-legal audiences. We employ a comprehensive evaluation framework that combines reference-based metrics, embedding-based metrics, and factual consistency checks through both automated methods and LLM-as-judge assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe9ae8",
   "metadata": {},
   "source": [
    "\n",
    "It is important to note that court press releases often contain additional context not found in the original decision, such as procedural history, background information, or quotes from spokespersons. This characteristic distinguishes press releases from pure summaries and presents additional challenges for automated evaluation of factual consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8f00f",
   "metadata": {},
   "source": [
    "# CourtPressGER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d3057",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80d9a7",
   "metadata": {},
   "source": [
    "```{=latex}\n",
    "\\begin{table*}[ht]\n",
    "\\centering\n",
    "\\begin{tabular*}{\\textwidth}{\n",
    "    @{\\extracolsep{\\fill}}\n",
    "    lrrrrrr\n",
    "    }\n",
    "\\toprule\n",
    " & \\multicolumn{3}{c}{Press Release} & \\multicolumn{3}{c}{Judgment} \\\\\n",
    "\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\n",
    "Court & Mean & Std & Count & Mean & Std & Count \\\\\n",
    "\\midrule\n",
    "Bundesarbeitsgericht & 1056.37 & 407.50 & 177 & 14148.00 & 7913.64 & 177 \\\\\n",
    "Bundesfinanzhof & 800.28 & 213.58 & 761 & 7378.97 & 4410.79 & 761 \\\\\n",
    "Bundesgerichtshof & 1386.84 & 680.10 & 2407 & 8216.82 & 5686.26 & 2407 \\\\\n",
    "Bundessozialgericht & 1146.66 & 484.69 & 161 & 11790.02 & 4850.29 & 161 \\\\\n",
    "Bundesverfassungsgericht & 2039.50 & 1353.63 & 1771 & 14781.53 & 16844.62 & 1771 \\\\\n",
    "Bundesverwaltungsgericht & 942.91 & 336.86 & 1155 & 11734.63 & 8110.92 & 1155 \\\\\n",
    "\\midrule\n",
    "\\textbf{Overall average} & 1402.32 & 954.52 & -- & 10809.58 & 10739.27 & -- \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular*}\n",
    "\\caption{Statistical summary of press releases and judgments by court}\n",
    "\\label{tab:descriptive_statistics}\n",
    "\\end{table*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234752b",
   "metadata": {},
   "source": [
    "\n",
    "Our dataset includes court decisions and corresponding press releases from Germany's highest courts (Bundesgerichte) as well as the federal constitutional court (Bundesverfassungsgericht - under german law not a Bundesgericht) :\n",
    "\n",
    "- Federal Labor Law Court (Bundesarbeitsgericht - BAG)\n",
    "- Federal Fiscal Court (Bundesfinanzhof - BFH)\n",
    "- Federal Court of Justice (Bundesgerichtshof - BGH)\n",
    "- Federal Social Court (Bundessozialgericht - BSG)\n",
    "- Federal Constitutional Court (Bundesverfassungsgericht - BVerfG)\n",
    "- Federal Administrative Court (Bundesverwaltungsgericht - BVerwG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcb9ff",
   "metadata": {},
   "source": [
    "The cleaned dataset contains 6.4k pairs of court decisions and press releases. The average length of decisions is 10.810 BPE tokens , while press releases average 1.402 BPE tokens. We report BPE token counts as used by modern LLMs rather than raw word or character counts for better compatibility with model context window considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47267c",
   "metadata": {},
   "source": [
    "## Splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f7df8",
   "metadata": {},
   "source": [
    "\n",
    "For our experiments, we divided the dataset into training, validation, and test splits in an 72.2/11.6/16.3 ratio. The training set contains 4643 pairs, while the validation set contains 744 test sets contain 1045 pairs. The split was done chronogically with the following year distribution: ((…)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d32a5",
   "metadata": {},
   "source": [
    "\n",
    "We decided to split chronologically because otherwise the distribution shifts incurred by rotating press office personnel over time would not be captured in the data split, leading to a potential overestimation of performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdc175",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927fedc",
   "metadata": {},
   "source": [
    "Our dataset analysis reveals variation in document lengths across different courts. Federal Constitutional Court decisions tend to be the longest with an average of 14.782 BPE tokens, while Federal Fiscal Court decisions average 7.379 BPE tokens. Press release lengths also vary, with Federal Constitutional Court releases averaging 2,230 BPE tokens and Federal Court of Justice releases averaging 1,620 BPE tokens. The standard deviation for court decision length is 10.739 BPE tokens, indicating considerable variation in document size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f08c9e",
   "metadata": {},
   "source": [
    "The descriptive statistics of the cleaned dataset can be seen in [Table 1](#tab:descriptive_statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a1352",
   "metadata": {},
   "source": [
    "In addition, the distribution of press release and judgement length and year distribution can be seen in [Figure 1](#fig:length_distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea27310",
   "metadata": {},
   "source": [
    "```{=latex}\n",
    "\\begin{figure*}[ht]\n",
    "\\centering\n",
    "\\includegraphics[width=\\textwidth]{output.png}\n",
    "\\caption{Distribution of press release and judgment lengths across different courts}\n",
    "\\label{fig:length_distribution}\n",
    "\\end{figure*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b44aee",
   "metadata": {},
   "source": [
    "# Experimental Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e30c6",
   "metadata": {},
   "source": [
    "\n",
    "## Synthetic Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90963816",
   "metadata": {},
   "source": [
    "\n",
    "For each decision-press release pair, we generated synthetic prompts through the Anthropic API (Claude Sonnet 3.7) to serve as input for LLMs to generate press releases. These prompts were designed to highlight the key aspects of the decision and to train the models to create relevant and precise press releases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78cb6c",
   "metadata": {},
   "source": [
    "\n",
    "To create synthetic prompts, we utilized Claude 3.7 Sonnet with a system prompt [→Appendix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac91070",
   "metadata": {},
   "source": [
    "## Press Release Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7b098",
   "metadata": {},
   "source": [
    "Our pipeline includes various LLMs, which can be categorized into two groups:\n",
    "\n",
    "1. Large Models: GPT-4o (mainstream and economical closed source model at time of experiments), Llama-3-70B (large & SotA open weights model at time of running experiments)\n",
    "2. Small Models: Teuken-7B, Llama-3-8B, EuroLLM-9B, Mistral-7B (all open weights in smaller class, typical base models for research finetuning experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a388ba",
   "metadata": {},
   "source": [
    "The pipeline is designed to send the synthetic prompts to the models, collect the generated press releases, and store them alongside the actual press releases. A checkpoint system allows for the continuation of interrupted generation processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49d533",
   "metadata": {},
   "source": [
    "\n",
    "### Context Limitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc79ca",
   "metadata": {},
   "source": [
    "We found that the context window size of the models has a significant impact on their ability to generate high-quality press releases. Models with larger context windows (e.g., GPT-4o with a theoretical limit of 128k tokens, though in our implementation we used the API with a practical limit of 64k tokens) can process the entire court decision at once, while smaller models require document chunking and hierarchical summarization approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde87b6",
   "metadata": {},
   "source": [
    "For decisions that exceed the context window of a model, we implemented a hierarchical summarization approach (described in the next section) that allows the model to consider the entire document while respecting context limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f033344",
   "metadata": {},
   "source": [
    "### Generation Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba1f2a",
   "metadata": {},
   "source": [
    "For consistency across models, we use a standardized german prompt template that can be found in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6d26",
   "metadata": {},
   "source": [
    "For OpenAI models (GPT-4o), the request format uses the above template as the user message with a system message that instructs the model to act as an expert in legal texts who writes press releases based on court decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7b1ab",
   "metadata": {},
   "source": [
    "For local models (Teuken-7B, Llama-3-8B, EuroLLM-9B), we use a similar approach but without separate system messages, including the instructions directly in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276137a5",
   "metadata": {},
   "source": [
    "## Hierarchical Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8138159",
   "metadata": {},
   "source": [
    "For court decisions that exceed the context window of a model, we implemented a hierarchical summarization approach. This method involves the following steps:\n",
    "\n",
    "1. Chunking: The court decision is divided into chunks that fit within the model's context window.\n",
    "2. Level 0 Summarization: Each chunk is independently summarized.\n",
    "3. Higher Level Summarization: The summaries are combined and recursively summarized until a single summary is created.\n",
    "4. Final Press Release Generation: The final summary is used as input for the press release generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e175ce",
   "metadata": {},
   "source": [
    "This hierarchical approach allows smaller models to process long documents while maintaining the context and coherence of the original text. The implementation involves a recursive algorithm that estimates the number of levels needed based on the document length and the model's context window size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c985dd6",
   "metadata": {},
   "source": [
    "Each level of summarization uses specially designed prompts that instruct the model to focus on different aspects of the text, with higher levels emphasizing cohesion and integration of information from multiple chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f68262",
   "metadata": {},
   "source": [
    "## FT Teuken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02570c0",
   "metadata": {},
   "source": [
    "\n",
    "#todo ME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773e978",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409506ca",
   "metadata": {},
   "source": [
    "Our evaluation framework was designed to address the known limitations of traditional NLP metrics for legal text summarization. As highlighted by @steffes2023, metrics like ROUGE can be unreliable as sole quality indicators because they may not adequately capture legally relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60268cc5",
   "metadata": {},
   "source": [
    "Therefore, we developed a comprehensive evaluation approach using multiple complementary metrics:\n",
    "\n",
    "- ROUGE (@lin2004)\n",
    "- BLEU (@papineni2002)\n",
    "- METEOR (@banerjee2005)\n",
    "- BERTScore (@zhang2020)\n",
    "- QAGS (Question Answering for evaluating Generated Summaries) (@wang2020)\n",
    "- FactCC (Factual Consistency Check) (@Kryscinski2019)\n",
    "- LLM-as-a-Judge (evaluation using Claude 3.7 Sonnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef1084",
   "metadata": {},
   "source": [
    "While BLEU is less commonly used for summarization tasks due to its sensitivity to word order and sentence length, we include it to maintain comparability with multilingual studies like @rolshoven2024 to provide a more comprehensive assessment through multiple metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a76bff",
   "metadata": {},
   "source": [
    "This multi-faceted approach aligns with recent trends in legal summarization evaluation, which emphasize combining different automated metrics with expert judgment to assess different quality dimensions of generated legal texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc566036",
   "metadata": {},
   "source": [
    "In addition to these metrics, we engaged a legal professional who ranked a subset ofthe model outputs for \\textbf{10 decisions from each court} (60 cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386fe9d",
   "metadata": {},
   "source": [
    "### Factual Consistency Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25683a2a",
   "metadata": {},
   "source": [
    "Our project utilizes advanced metrics to evaluate the factual consistency between court decisions and generated press releases:\n",
    "\n",
    "- QAGS (Question Answering for evaluating Generated Summaries): This metric first generates questions from the press releases, then answers these questions with the court decisions as context, and finally compares the answers to verify if the press release is factually correct. This approach is similar to the framework proposed by @xu2023a, which showed better correlation with expert judgments than traditional metrics.\n",
    "- FactCC (Factual Consistency Check): This metric extracts claims from the press releases and checks each claim for consistency with the court decision. A total score for factual consistency is calculated from these checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6c733",
   "metadata": {},
   "source": [
    "For both QAGS and FactCC, we acknowledge a significant limitation: These metrics were originally developed and trained on English news datasets, not German legal texts. Their application to our German court texts relies on the multilingual capabilities of the underlying models, but has not been specifically validated for German legal text. This limitation likely affects the absolute scores and may partially explain why smaller German-specific models like Teuken-7B achieve factual consistency scores comparable to larger models despite lower performance on other metrics. The scores should be interpreted as relative comparisons rather than absolute measures of factual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82be8a0",
   "metadata": {},
   "source": [
    "For additional context information in press releases that doesn't directly appear in the court decision, these metrics may incorrectly flag such information as inconsistent, leading to potentially lower scores even for high-quality press releases. We address this limitation partially through our LLM-as-a-Judge approach and the human evaluation process, which can better distinguish between contradictory information and benign additional context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981033a2",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651eb1f0",
   "metadata": {},
   "source": [
    "We use Claude 3.7 Sonnet to evaluate the generated press releases based on various criteria such as factual correctness, completeness, clarity, and structure. Optionally, the generated press release can be compared with the reference press release. The metric provides both numerical ratings (1-10) and detailed justifications, calculating an overall score across all evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338001a",
   "metadata": {},
   "source": [
    "To evaluate the quality of the generated press releases, we use Claude 3.7 Sonnet with the following system prompt [→Appendix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d49170",
   "metadata": {},
   "source": [
    "It is important to note that our full evaluation relied on LLM-as-a-Judge rather than human legal experts. While this approach provides valuable insights and scales to large datasets, it serves as a proxy for human evaluation and would benefit from validation through extendedtargeted expert reviews in future work. Claude 3.7 Sonnet was selected for this task due to its strong performance in understanding complex legal texts in multiple languages as well as its selection for synthetic prompt generation which made it a natural choice for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd32446",
   "metadata": {},
   "source": [
    "## Human Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4899a8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#todo MP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4834682b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a080a",
   "metadata": {},
   "source": [
    "Based on our evaluation, we present the results organized by evaluation type (hierarchical vs. full document processing) and model. We structured our analysis to examine reference-based metrics, embedding-based metrics, factual consistency metrics, and human-like evaluation through LLM-as-judge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7902e0",
   "metadata": {},
   "source": [
    "The full‑text condition reveals the upper bound a model can reach when context is not truncated, whereas the hierarchical setting approximates a local‑deployment scenario. GPT‑4o and Llama‑3‑70B are statistically tied on most automatic metrics, yet human‑style LLM judging clearly prefers GPT‑4o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd17cf",
   "metadata": {},
   "source": [
    "```{=latex}\n",
    "\\begin{table*}[t]\n",
    "\\centering\n",
    "\\scriptsize\n",
    "\\setlength{\\tabcolsep}{4pt}      % etwas enger, falls nötig\n",
    "\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}l*{12}{r}}\n",
    "\\toprule\n",
    "Modell &\n",
    "ROUGE-1 & BLEU-1 & METEOR & BERT &\n",
    "FactCC & QAGS &\n",
    "llm\\_fact &\n",
    "llm\\_compl &\n",
    "llm\\_clar &\n",
    "llm\\_struc &\n",
    "llm\\_ref &\n",
    "llm\\_total\n",
    "\\\\ \\midrule\n",
    "gpt 4o &\n",
    "0.3584 & 0.2275 & 0.1836 & 0.7711 & 0.4915 & 0.2637 &\n",
    "\\textbf{8.1070} & \\textbf{7.0885} & \\textbf{8.7451} &\n",
    "\\textbf{8.4076} & \\textbf{6.8414} & \\textbf{7.8379}\n",
    "\\\\\n",
    "llama 3\\_3 70B &\n",
    "\\textbf{0.3746} & \\textbf{0.2327} & \\textbf{0.1931} & 0.7730 & 0.4987 & \\textbf{0.2863} &\n",
    "7.3417 & 6.3637 & 8.1545 & 7.6200 & 5.9002 & 7.0760\n",
    "\\\\\n",
    "eurollm 9B &\n",
    "0.2800 & 0.1856 & 0.1451 & 0.7459 & 0.5065 & 0.1875 &\n",
    "4.9739 & 4.4255 & 6.4043 & 6.6876 & 3.5435 & 5.2070\n",
    "\\\\\n",
    "llama 3 8B &\n",
    "0.2927 & 0.1829 & 0.1472 & 0.7373 & 0.5082 & 0.2289 &\n",
    "5.2780 & 4.5405 & 6.3069 & 6.4295 & 3.7751 & 5.2660\n",
    "\\\\\n",
    "mistral v03 &\n",
    "0.3571 & 0.2304 & 0.1871 & \\textbf{0.7777} & \\textbf{0.5122} & 0.2386 &\n",
    "5.5376 & 4.9653 & 5.5578 & 5.2447 & 3.7370 & 5.0085\n",
    "\\\\\n",
    "teuken &\n",
    "0.1630 & 0.0794 & 0.0781 & 0.6600 & 0.5051 & 0.1607 &\n",
    "3.0635 & 2.1606 & 4.2356 & 4.4077 & 1.8269 & 3.1388\n",
    "\\\\ \\bottomrule\n",
    "\\end{tabular*}\n",
    "\\caption{Press release comparison on hierarchical summarized judgements}\n",
    "\\label{tab:comparison_hier}\n",
    "\\end{table*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449c9f3",
   "metadata": {},
   "source": [
    "```{=latex}\n",
    "\\begin{table*}[t]\n",
    "\\centering\n",
    "\\scriptsize\n",
    "\\setlength{\\tabcolsep}{4pt}          % etwas enger, falls nötig\n",
    "\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}l*{12}{r}}\n",
    "\\toprule\n",
    "Model &\n",
    "ROUGE-1 & BLEU-1 & METEOR & BERT &\n",
    "FactCC & QAGS &\n",
    "llm\\_fact & llm\\_compl & llm\\_clar & llm\\_struc & llm\\_ref & llm\\_total\n",
    "\\\\ \\midrule\n",
    "gpt 4o &\n",
    "0.3627 & 0.2105 & 0.1845 & 0.7563 & 0.4991 & 0.2777 &\n",
    "\\textbf{8.3933} & \\textbf{7.1615} & \\textbf{8.8192} &\n",
    "\\textbf{8.5385} & \\textbf{7.0115} & \\textbf{7.9848}\n",
    "\\\\\n",
    "llama 3\\_3 70B &\n",
    "\\textbf{0.3823} & \\textbf{0.2248} & \\textbf{0.1986} & \\textbf{0.7691} & \\textbf{0.5082} & 0.2898 &\n",
    "8.1721 & 6.8661 & 8.6333 & 8.1552 & 6.6603 & 7.6974\n",
    "\\\\\n",
    "mistral v03 &\n",
    "0.3612 & 0.2126 & 0.1901 & 0.7465 & 0.5021 & \\textbf{0.3252} &\n",
    "6.9612 & 5.7141 & 7.1395 & 6.8110 & 5.0271 & 6.3306\n",
    "\\\\ \\bottomrule\n",
    "\\end{tabular*}\n",
    "\\caption{Press release comparison on full judgements}\n",
    "\\label{tab:comparison_full}\n",
    "\\end{table*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339952e",
   "metadata": {},
   "source": [
    "#todo MP - add in results for human evaluation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec1982",
   "metadata": {},
   "source": [
    "Note that we evaluate Mistral_v03 also on the full ruling text even though it’s context is limited to 32k tokens. In our experiments, 1% of documents needed to be truncated for evaluation in this narrower context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075070e",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd785e",
   "metadata": {},
   "source": [
    "These results are consistent with findings from @glaser2021, who reported ROUGE-1 scores of around 30.5% for their best models on German court decision summarization. Our best models exceed this performance slightly, which may be attributed to the advancement in LLMs since their study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48255079",
   "metadata": {},
   "source": [
    "Our findings confirm the intuitive trade‑off between model capacity and inference cost: large models (\\emph{GPT\\ 4o, Llama\\ 3\\ 70B}) heavily outperform smaller ones on fidelity, completeness and clarity, but the differential shrinks when hierarchical summarisation is used. The surprisingly high FactCC scores for small German models stem from the English‑centric nature of the metric; annotation artefacts lead to partial credit even for hallucinated statements. Conversely, QAGS questions often target details absent from official releases, penalising otherwise sound outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81208a",
   "metadata": {},
   "source": [
    "These results demonstrate that while larger models generally produce press releases that are more factually correct, complete, clear, and well-structured, the hierarchical summarization approach allows smaller models to produce reasonably good summaries, particularly in terms of clarity and structure. Interestingly, the improvement from hierarchical summarisation to full summarisation is marginal for the largest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bb445",
   "metadata": {},
   "source": [
    "The LLM-as-a-judge protocol aligns well with expert feedback collected on a subset of 60 cases ((#todo MP - is this correct?)), supporting its use as a low‑cost proxy. However, qualitative analysis shows that LLM evaluators struggle with nuanced legal misinterpretations (ratio decidendi vs. obiter dicta). A hybrid pipeline that flags such edge cases for manual review is therefore advisable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3caac64",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e99364",
   "metadata": {},
   "source": [
    "Our comprehensive evaluation of the CourtPressGER system demonstrates that modern LLMs can effectively generate German court press releases, with performance varying according to model size and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba054e59",
   "metadata": {},
   "source": [
    "Key findings include:\n",
    "\n",
    "1. Model size matters: Larger models consistently outperform smaller models across all evaluation metrics.\n",
    "2. Hierarchical summarization is effective: Our hierarchical approach enables smaller models to process long documents while maintaining reasonable quality.\n",
    "3. Factual consistency challenges: Even the best models struggle with perfect factual consistency, indicating room for improvement.\n",
    "4. Language-specific models: German-specific models like EuroLLM show competitive performance for their size compared to larger multilingual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2c54f",
   "metadata": {},
   "source": [
    "While our fine-tuned Teuken model showed some improvement over the base version, ((ME update this when done)) it still performs significantly below larger models, suggesting that parameter count remains a decisive factor for this complex task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487d3c9",
   "metadata": {},
   "source": [
    "Our work provides a contribution to the emerging field of automated legal text summarization in the German language, extending the work of @glaser2021, @steffes2022, and @rolshoven2024. The multidimensional evaluation approach we employed addresses the limitations of traditional metrics highlighted by @steffes2023 and incorporates newer evaluation methods like question-answering based assessment proposed by @xu2023a.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2a4dc",
   "metadata": {},
   "source": [
    "Our system has potential practical applications similar to the [ALeKS project](https://www.bmj.de/DE/themen/digitales/digitalisierung_justiz/digitalisierungsinitiative/laendervorhaben/_doc/artikel_vorhaben_14_ALeKS.html#:~:text=K%C3%BCnftig%20sollen%20mehr%20Gerichtsentscheidungen%20in,Automatisierung%20vereinfacht) currently under development in Germany, which aims to automate the generation of court decision headnotes. While ALeKS focuses on technical headnotes, our work specifically addresses press releases that need to be accessible to non-legal audiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f8c66",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd516c1",
   "metadata": {},
   "source": [
    "We acknowledge several limitations of our approach:\n",
    "\n",
    "1. Evaluation metrics: Our use of QAGS and FactCC metrics, which were developed and validated on English datasets, introduces uncertainty when applied to German legal texts. Future work should explore German-specific factual consistency metrics.\n",
    "2. LLM-as-judge vs. human evaluation: While our LLM-based evaluation provides valuable insights, it serves as a proxy for human expert evaluation and would benefit from validation through targeted expert reviews.\n",
    "3. Additional context in press releases: Court press releases often contain contextual information not present in the original decision, which can confound factual consistency metrics.\n",
    "4. Divergence from Rolshoven et al. findings: Unlike Rolshoven et al. (2024), who found that fine-tuned smaller models could approach the performance of larger models, our results show a clear advantage for larger models. This difference may be attributed to our focus on press releases rather than technical summaries (\"Regesten\"), the different nature of our dataset, or the specific characteristics of German federal court decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bb56d",
   "metadata": {},
   "source": [
    "The CourtPressGER project demonstrates the potential of LLMs to assist in making legal information more accessible to the public while highlighting the ongoing challenges in maintaining factual accuracy when summarizing complex legal documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8047f9",
   "metadata": {},
   "source": [
    "{{<pagebreak>}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b87e52",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0f92a",
   "metadata": {},
   "source": [
    "::: {#refs}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f127490",
   "metadata": {},
   "source": [
    "## Ethics Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8d15d",
   "metadata": {},
   "source": [
    "All data originate from publicly available court websites. Personal names are already anonymised by the courts. Our fine‑tuning set will be released under the \\textit{DIPLO-DL} licence, excluding any confidential meta‑data. Automated press releases must be reviewed by qualified staff before publication to avoid misrepresentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807631c6",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0464c1",
   "metadata": {},
   "source": [
    "We used the following prompts for our experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93425ad",
   "metadata": {},
   "source": [
    "\n",
    "### Synthetic prompt generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da8546",
   "metadata": {},
   "source": [
    "We used the following prompt for synthetic prompt generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b1ec3",
   "metadata": {},
   "source": [
    ":::{.callout-note title=\"Synthetic prompt generation\"}\n",
    "Du bist ein Experte für juristische Texte und Kommunikation. Deine Aufgabe ist es, ein Gerichtsurteil und die \n",
    "dazugehörige Pressemitteilung zu analysieren und dann herauszufinden, welcher Prompt verwendet worden sein könnte, \n",
    "um diese Pressemitteilung aus dem Gerichtsurteil zu generieren, wenn man ihn einem LLM gegeben hätte.\n",
    "\n",
    "1. Analysiere, wie die Pressemitteilung Informationen aus dem Urteil vereinfacht, umstrukturiert und Schlüsselinformationen hervorhebt\n",
    "2. Berücksichtige den Ton, die Struktur und den Detaillierungsgrad der Pressemitteilung\n",
    "3. Identifiziere, welche Anweisungen nötig wären, um den juristischen Text in diese Pressemitteilung zu transformieren\n",
    "    \n",
    "Erkläre NICHT deine Überlegungen und füge KEINE Meta-Kommentare hinzu. Gib NUR den tatsächlichen Prompt aus, der die \n",
    "Pressemitteilung aus dem Gerichtsurteil generieren würde. Sei spezifisch und detailliert in deinem synthetisierten Prompt.\n",
    "\n",
    "Hier ist das originale Gerichtsurteil:\n",
    "{court_ruling}\n",
    "    \n",
    "Und hier ist die Pressemitteilung, die daraus erstellt wurde:\n",
    "    \n",
    "{press_release}\n",
    "    \n",
    "Erstelle einen detaillierten Prompt, der einem LLM gegeben werden könnte, um die obige Pressemitteilung aus dem Gerichtsurteil zu generieren. \n",
    "Schreibe NUR den Prompt selbst, ohne Erklärungen oder Meta-Kommentare.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3735c7",
   "metadata": {},
   "source": [
    "### Press release generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e11e2",
   "metadata": {},
   "source": [
    "We used the following prompt for press release generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858dcabd",
   "metadata": {},
   "source": [
    ":::{.callout-note title=\"Press release generation\"}\n",
    "{prompt}\n",
    "Gerichtsurteil: {ruling}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9834d",
   "metadata": {},
   "source": [
    "\n",
    "### LLM-as-a-judge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d45f96",
   "metadata": {},
   "source": [
    "We used the following prompt for LLM-as-a-judge evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0924a5",
   "metadata": {},
   "source": [
    "::::{.callout-note title=\"LLM-as-a-judge\"}\n",
    "You are an expert in legal texts and evaluate the quality of press releases for court decisions. Rate the generated press release according to the\n",
    "following criteria on a scale of 1-10:\n",
    "\n",
    "1. Factual Correctness: How accurately does the press release reflect the facts from the court decision? \n",
    "2. Completeness: Have all important information from the decision been included in the press release? \n",
    "3. Clarity: How understandable is the press release for a non-legal audience?\n",
    "4. Structure: How well is the press release structured and organized?\n",
    "5. Comparison with Reference: How good is the generated press release compared to the reference press release?\n",
    "\n",
    "For each criterion, provide a numerical value between 1 and 10 and a brief justification. Finally, calculate an overall score as the average\n",
    "of all individual values. Provide your answer in the following JSON format: \n",
    "{ \"faktische_korrektheit\": {\"wert\": X, \"begründung\": \"...\"}, \"vollständigkeit\": {\"wert\": X, \"begründung\": \"...\"}, \"klarheit\": {\"wert\":\n",
    "X, \"begründung\": \"...\"}, \"struktur\": {\"wert\": X, \"begründung\": \"...\"}, \"vergleich_mit_referenz\": {\"wert\": X, \"begründung\": \"...\"}, \"gesamtscore\":\n",
    "X.X }\n",
    "\n",
    "The user prompt contains:\n",
    "Court Decision [court_decision]\n",
    "Generated Press Release [generated_press_release]\n",
    "Reference Press Release [reference_press_release]\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56950d",
   "metadata": {},
   "source": [
    "```{=latex}\n",
    "\\clearpage\n",
    "\\onecolumn\n",
    "\\begin{landscape}\n",
    "\\scriptsize\n",
    "\\setlength{\\tabcolsep}{3.5pt}\n",
    "\\renewcommand{\\arraystretch}{1.1}\n",
    "\\centering\n",
    "\\begin{longtable}{@{\\extracolsep{\\fill}}l*{24}{r}}\n",
    "\\label{tab:all_combined}\\\\\n",
    "\\toprule\n",
    "Model &\n",
    "R1 & R2 & RL &\n",
    "B1 & B2 & B3 & B4 &\n",
    "MTR &\n",
    "BP & BR & BF1 &\n",
    "KW & ENT & Len &\n",
    "Fcc & FccC &\n",
    "QGS & Qn &\n",
    "LJ\\_Fact & LJ\\_Compl & LJ\\_Clar & LJ\\_Struc & LJ\\_Ref & LJ\\_Tot\n",
    "\\\\ \\midrule\n",
    "\\endfirsthead\n",
    "\\toprule\n",
    "Model & R1 & R2 & RL & B1 & B2 & B3 & B4 & MTR &\n",
    "BP & BR & BF1 & KW & ENT & Len & Fcc & FccC &\n",
    "QGS & Qn & LJ\\_Fact & LJ\\_Compl & LJ\\_Clar & LJ\\_Struc & LJ\\_Ref & LJ\\_Tot\n",
    "\\\\ \\midrule\n",
    "\\endhead\n",
    "openai\\_gpt\\_4o\\_full &\n",
    "0.3627 & 0.1452 & 0.1918 &\n",
    "0.2105 & 0.1266 & 0.0832 & 0.0559 &\n",
    "0.1845 &\n",
    "0.7746 & 0.7396 & 0.7563 &\n",
    "0.2082 & 0.2290 & 0.4572 &\n",
    "0.4991 & 0.5068 &\n",
    "0.2777 & 4.75 &\n",
    "\\textbf{8.3933} & \\textbf{7.1615} & \\textbf{8.8192} & \\textbf{8.5385} & \\textbf{7.0115} & \\textbf{7.9848}\n",
    "\\\\\n",
    "openai\\_gpt\\_4o\\_hier &\n",
    "0.3584 & 0.1242 & 0.1758 &\n",
    "0.2275 & 0.1280 & 0.0786 & 0.0495 &\n",
    "0.1836 &\n",
    "0.7835 & 0.7595 & 0.7711 &\n",
    "0.1883 & 0.2157 & 0.5114 &\n",
    "0.4915 & 0.4758 &\n",
    "0.2637 & 4.78 &\n",
    "8.1070 & 7.0885 & 8.7451 & 8.4076 & 6.8414 & 7.8379\n",
    "\\\\\n",
    "llama\\_3\\_3\\_70B\\_full &\n",
    "\\textbf{0.3823} & \\textbf{0.1601} & \\textbf{0.1997} &\n",
    "0.2248 & \\textbf{0.1385} & \\textbf{0.0946} & \\textbf{0.0668} &\n",
    "\\textbf{0.1986} &\n",
    "0.7889 & 0.7508 & 0.7691 &\n",
    "\\textbf{0.2198} & \\textbf{0.2311} & 0.4972 &\n",
    "0.5082 & 0.5144 &\n",
    "0.2898 & 4.87 &\n",
    "8.1721 & 6.8661 & 8.6333 & 8.1552 & 6.6603 & 7.6974\n",
    "\\\\\n",
    "llama\\_3\\_3\\_70B\\_hier &\n",
    "0.3746 & 0.1411 & 0.1864 &\n",
    "\\textbf{0.2327} & 0.1358 & 0.0879 & 0.0593 &\n",
    "0.1931 &\n",
    "\\textbf{0.7918} & 0.7557 & 0.7730 &\n",
    "0.2132 & 0.2158 & 0.5156 &\n",
    "0.4987 & 0.5005 &\n",
    "0.2863 & \\textbf{4.94} &\n",
    "7.3417 & 6.3637 & 8.1545 & 7.6200 & 5.9002 & 7.0760\n",
    "\\\\\n",
    "eurollm\\_9B\\_hier &\n",
    "0.2800 & 0.0611 & 0.1199 &\n",
    "0.1856 & 0.0832 & 0.0413 & 0.0212 &\n",
    "0.1451 &\n",
    "0.7570 & 0.7362 & 0.7459 &\n",
    "0.1275 & 0.1229 & 0.5249 &\n",
    "0.5065 & \\textbf{0.5290} &\n",
    "0.1875 & 4.84 &\n",
    "4.9739 & 4.4255 & 6.4043 & 6.6876 & 3.5435 & 5.2070\n",
    "\\\\\n",
    "llama\\_3\\_8B\\_hier &\n",
    "0.2927 & 0.0780 & 0.1344 &\n",
    "0.1829 & 0.0897 & 0.0499 & 0.0287 &\n",
    "0.1472 &\n",
    "0.7519 & 0.7239 & 0.7373 &\n",
    "0.1456 & 0.1444 & 0.4958 &\n",
    "0.5082 & 0.5081 &\n",
    "0.2289 & 4.90 &\n",
    "5.2780 & 4.5405 & 6.3069 & 6.4295 & 3.7751 & 5.2660\n",
    "\\\\\n",
    "mistral\\_v03\\_full &\n",
    "0.3612 & 0.1561 & 0.1844 &\n",
    "0.2126 & 0.1304 & 0.0907 & 0.0660 &\n",
    "0.1901 &\n",
    "0.7706 & 0.7255 & 0.7465 &\n",
    "0.2132 & 0.2074 & 0.4929 &\n",
    "0.5021 & 0.5044 &\n",
    "\\textbf{0.3252} & 4.72 &\n",
    "6.9612 & 5.7141 & 7.1395 & 6.8110 & 5.0271 & 6.3306\n",
    "\\\\\n",
    "mistral\\_v03\\_hier &\n",
    "0.3571 & 0.1218 & 0.1638 &\n",
    "0.2304 & 0.1264 & 0.0780 & 0.0509 &\n",
    "0.1871 &\n",
    "\\textbf{0.7918} & \\textbf{0.7645} & \\textbf{0.7777} &\n",
    "0.1884 & 0.1825 & \\textbf{0.5475} &\n",
    "\\textbf{0.5122} & 0.5189 &\n",
    "0.2386 & 4.69 &\n",
    "5.5376 & 4.9653 & 5.5578 & 5.2447 & 3.7370 & 5.0085\n",
    "\\\\\n",
    "teuken\\_hier &\n",
    "0.1630 & 0.0213 & 0.0703 &\n",
    "0.0794 & 0.0284 & 0.0105 & 0.0043 &\n",
    "0.0781 &\n",
    "0.6966 & 0.6303 & 0.6600 &\n",
    "0.0705 & 0.0673 & 0.3553 &\n",
    "0.5051 & 0.5068 &\n",
    "0.1607 & \\textbf{4.94} &\n",
    "3.0635 & 2.1606 & 4.2356 & 4.4077 & 1.8269 & 3.1388\n",
    "\\\\ \\bottomrule\n",
    "\\caption{Combined automatic and human evaluation scores\n",
    "         (hierarchical Summaries $\\to$ \\texttt{\\_hier\\_};\n",
    "          complete Judgements $\\to$ \\texttt{\\_full\\_})}\n",
    "\\end{longtable}\n",
    "\n",
    "\\noindent\\footnotesize\n",
    "\\begin{tabular}{@{}ll@{\\hspace{3em}}ll@{}}\n",
    "R1, R2, RL   & ROUGE-1/-2/-L F1            & KW          & Keyword-Overlap\\\\\n",
    "B1–B4        & BLEU-1 … BLEU-4             & ENT         & Entity-Overlap\\\\\n",
    "MTR          & METEOR                      & Len         & Length-Ratio\\\\\n",
    "BP, BR, BF1  & BERTScore Precision/Recall/F1 & Fcc, FccC   & FactCC Score / Consistency\\\\\n",
    "QGS, Qn      & QAGS Score / Ø Questions       & LJ\\_Fact    & \\texttt{llm\\_judge}\\,fact. Corr.\\\\\n",
    "             &                             & LJ\\_Compl   & – Completeness\\\\\n",
    "             &                             & LJ\\_Clar    & – Clarity\\\\\n",
    "             &                             & LJ\\_Struc   & – Structure\\\\\n",
    "             &                             & LJ\\_Ref     & – Comparison with Reference\\\\\n",
    "             &                             & LJ\\_Tot     & – Total Score\\\\\n",
    "\\end{tabular}\n",
    "\\end{landscape}\n",
    "\\twocolumn\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
