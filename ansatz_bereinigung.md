**Einleitung:** Ein Gerichtsurteils-Datensatz mit 6.500 Urteil–Pressemitteilung-Paaren soll bereinigt werden. Gesucht sind automatisierte Verfahren, um **irrelevante Pressemitteilungen** zu erkennen und zu entfernen – insbesondere solche **ohne direkten Urteilsbezug** (z.B. bloße **Ankündigungen** zukünftiger Verhandlungen oder allgemeine Mitteilungen). Im Folgenden werden mögliche Ansätze vorgestellt, geeignete Tools genannt und Best Practices sowie relevante Forschung aus dem Legal NLP skizziert.

## Mögliche Ansätze zur Erkennung irrelevanter Pressemitteilungen

### Regelbasierte Filter (Keywords und Muster) 
Ein naheliegender Ansatz ist das **herausfiltern anhand von Schlüsselwörtern** oder formalen Mustern. Pressemitteilungen, die z.B. Wörter wie *„Ankündigung“*, *„Verhandlung“* (im Kontext *„wird verhandelt“*), *„Einladung“* oder *„Termin“* enthalten, deuten auf kommende Ereignisse hin. Solche Keywords können als Indikatoren dienen. Ebenso können **Regex-Regeln** bestimmte Formulierungen erkennen (z.B. Datumsangaben in der Zukunft oder Phrasen wie *„findet statt“*). Regelbasierte **Phrase-Mapping**-Ansätze sind einfach und oft überraschend wirkungsvoll – tatsächlich gelten **Schlüsselwörter** in vielen NLP-Aufgaben als **starke Baseline】 ([Going Beyond Keywords with NLP. Many NLP tasks can be solved with… | by Jared Rand | TDS Archive | Medium](https://medium.com/data-science/going-beyond-keywords-with-nlp-42395f7e7c67#:~:text=Keywords%20Are%20Simple%20and%20Powerful)). Man könnte positive und negative Keyword-Listen definieren (z.B. Schlagwörter, die auf Urteilsankündigungen hinweisen, vs. solche, die ein echtes Urteil kennzeichnen) ([Going Beyond Keywords with NLP. Many NLP tasks can be solved with… | by Jared Rand | TDS Archive | Medium](https://medium.com/data-science/going-beyond-keywords-with-nlp-42395f7e7c67#:~:text=The%20absolute%20simplest%20way%20to,a%20set%20of%20negative%20keywords)). Dieser Ansatz ist transparent und benötigt kein Training. Allerdings erfordert er **manuelle Pflege** und wird nie alle Fälle abdecken; unterschiedliche Formulierungen aus verschiedenen Pressestellen können Regeln leicht umgehen. Außerdem besteht die Gefahr von **False Positives** (z.B. wenn das Wort „Verhandlung“ auch in einer inhaltlich relevanten Mitteilung vorkommt). Insgesamt bietet der regelbasierte Ansatz einen guten Startpunkt, sollte aber durch raffiniertere Methoden ergänzt werden.

### Semantische Ähnlichkeit mit Embeddings 
Statt konkreter Wörter zu suchen, kann man auf **semantischer Ebene** prüfen, ob eine Pressemitteilung inhaltlich zum zugehörigen Urteil passt. Moderne NLP-Modelle können Texte als **Vektoren (Embeddings)** darstellen, sodass inhaltlich ähnliche Texte im Vektorraum nah beieinander liegen ([GPT-3 Embeddings: Perform Text Similarity, Semantic Search, Classification, and Clustering(Part_4) | by Yashwanth Reddy | Medium](https://medium.com/@reddyyashu20/gpt-3-embeddings-perform-text-similarity-semantic-search-classification-and-clustering-part-4-abbcf447faf8#:~:text=The%20embedding%20is%20an%20information,representations%20should%20also%20be%20similar)). Die Idee: **Pressemitteilung und Urteil als Embeddings** berechnen und ihre **kosine**-**Ähnlichkeit** bestimmen. Ist die Ähnlichkeit gering, bezieht sich die Mitteilung vermutlich *nicht* auf den Urteilsinhalt (d.h. es fehlen gemeinsame Themen, Begriffe, Named Entities etc.). So ließen sich Mitteilungen mit **mangelnder Bezugnahme** auf das jeweilige Urteil automatisch erkennen. Diese Methode erfordert keinen festen Wortschatz, sondern erfasst die *Bedeutung* der Texte ([GPT-3 Embeddings: Perform Text Similarity, Semantic Search, Classification, and Clustering(Part_4) | by Yashwanth Reddy | Medium](https://medium.com/@reddyyashu20/gpt-3-embeddings-perform-text-similarity-semantic-search-classification-and-clustering-part-4-abbcf447faf8#:~:text=Each%20embedding%20is%20a%20vector,representations%20should%20also%20be%20similar)). Praktisch kann man z.B. **Sentence-BERT** oder andere **Sentence Embeddings** (ggf. multilingual oder domänenspezifisch für Deutsch) nutzen, um jeden Text in einen Semantik-Vektor zu konvertieren. Anschließend berechnet man den **Kosinuswert** zwischen Urteil- und Pressemitteilungs-Vektor; unterschreitet er einen Schwellenwert, wird die PM als irrelevant markiert. Vorteil: Auch subtile Unterschiede (z.B. stilistisch andere Wortwahl) können erkannt werden, solange die Semantik abweicht. Allerdings muss ein geeigneter Schwellenwert empirisch ermittelt werden, und domain-spezifische Sprache kann Herausforderungen bergen. Eine Studie zur Ähnlichkeitsmessung zwischen Pressetexten zeigte z.B., dass in einem Anwendungsfall ein **klassisches TF‑IDF mit Soft-Cosine** die semantische BERT-basierte Distanz sogar übertraf ([A comparison of different methods in their ability to compare semantic similarity between articles and press releases](https://www.diva-portal.org/smash/get/diva2:1711523/FULLTEXT01.pdf#:~:text=in%20their%20ability%20to%20compare,cosine%20similar%02ity)) – was darauf hindeutet, dass **fachspezifische Begriffe** und Wortüberschneidungen wichtig sind. Daher könnte man Embeddings auch mit einfacheren Bag-of-Words-Merkmalen kombinieren, um sicherzustellen, dass z.B. **Schlüsselbegriffe aus dem Urteil** in der Pressemitteilung auftauchen. Fehlen jegliche Namen, Aktenzeichen oder juristische Kernbegriffe des Urteils im Pressetext, wäre das ein starkes Indiz für Irrelevanz.

### Überwachtes Machine Learning (Klassifikation) 
Ein **klassifikator-basierter Ansatz** trainiert ein Modell darauf, relevante vs. irrelevante Pressemitteilungen zu unterscheiden. Dazu bräuchte man einen (kleinen) **Goldstandard**: z.B.  eine Teilmenge der 6.500 Paare von Hand labeln (Kategorie „gehört zum Urteil“ vs. „Allgemein/Ankündigung“). Mit diesen Beispielen lässt sich dann ein **Textklassifikator** trainieren. Klassische Verfahren wären etwa ein **Logistic Regression**-Modell auf TF‑IDF-Vektoren der Texte, oder moderne **Transformer-Modelle** (wie BERT) feinzutunen. Im juristischen Bereich haben sich vortrainierte Language Models bewährt – etwa zeigt ein **deutsches Legal-BERT-Modell** (Yeung, 2019) auf juristischen Texten **gute Leistungen bei Klassifikation und Ähnlichkeitsaufgaben ([](https://arxiv.org/pdf/2304.08649#:~:text=considering%20the%20context,including%20classification%2C%20regression%20and%20similarity))**. Für die Feinabstimmung könnte man z.B. **bert-base-german-cased** oder das juris-spezifisch vortrainierte Modell verwenden und auf die binäre Klassifikation fine-tunen. Solche BERT-basierten Klassifikatoren erzielen oft eine **hohe Genauigkeit** und können komplexe Sprachmuster lernen, die einfache Regeln nicht abdecken ([](https://arxiv.org/pdf/2304.08649#:~:text=with%20a%20German%20legal%20BERT,rel%02evance%20between%20two%20legal%20cases)). Ein Vorteil überwachter Modelle: Sie können *kombinierte Indizien* (Wortwahl, Kontext, Bezugnahme) gewichten und optimieren, anstatt nur starren Regeln zu folgen. In der Praxis ähnelt dies Aufgaben wie der automatischen **Relevanzbewertung** von Dokumenten im e-Discovery (elektronische Dokumentensuche), wo auch entschieden wird, welche Dokumente für einen Fall relevant sind ([](https://arxiv.org/pdf/2304.08649#:~:text=electronic%20discovery%20determining%20the%20relevance,generate%20routine%20legal%20documents%20and)) – dort setzt man ebenfalls ML-Klassifikatoren ein. Wichtig ist hier die Verfügbarkeit von **Labeldaten**: Falls nur wenige Beispiele vorhanden sind, kann man mit **Active Learning** iterativ weitere Beispiele labeln (das Modell flaggt unsichere Fälle zur Überprüfung). Technisch lässt sich ein solcher Klassifikator z.B. mit HuggingFace Transformers leicht umsetzen – man nimmt einen deutschen BERT-Basismodell-Checkpoint und hängt eine Klassifikationsschicht an ([python - HuggingFace Transformers model for German news classification - Stack Overflow](https://stackoverflow.com/questions/63672169/huggingface-transformers-model-for-german-news-classification#:~:text=In%20general%20I%20recommend%20you,in%20case%20of%20insufficient%20results)). Zu beachten ist, dass das Modell domain-spezifische Begriffe lernen muss; ggf. hilft es, den Wortschatz mit juristischen Termini zu erweitern oder ein spezialisiertes Modell (wie Legal-BERT) zu nutzen ([](https://arxiv.org/pdf/2304.08649#:~:text=by%20aggregating%20paragraph,fine%02tuning%20BERT%20on%20their%20Vietnamese)). Der Aufwand der Annotationsphase wird durch eine voraussichtlich hohen Präzision/Recall belohnt, sobald das Modell gut generalisiert.

### Unüberwachte Verfahren (Clustering/Topic Modeling) 
Ohne vorhandene Labels kann man versuchen, **ungeleitet Muster zu entdecken**. Durch **Clustering** der Pressemitteilungen nach Textähnlichkeit könnte sich zeigen, dass *thematisch ähnliche Mitteilungen gruppieren*. Idealerweise würden reine **Urteilszusammenfassungen** in anderen Clustern landen als **Verhandlungshinweise**. Ein einfacher Ansatz: berechne Embeddings aller Pressemitteilungstexte (z.B. mit Sentence-BERT) und wende einen **Clustering-Algorithmus** wie *k-Means* oder *HDBSCAN* an, um Gruppen zu bilden ([GPT-3 Embeddings: Perform Text Similarity, Semantic Search, Classification, and Clustering(Part_4) | by Yashwanth Reddy | Medium](https://medium.com/@reddyyashu20/gpt-3-embeddings-perform-text-similarity-semantic-search-classification-and-clustering-part-4-abbcf447faf8#:~:text=%23%20source%3A%20https%3A%2F%2Fstackoverflow.com%2Fquestions%2F55619176%2Fhow)). Wenn man z.B. 2–3 Hauptcluster erhält, könnte sich herausstellen, dass ein Cluster v.a. zukünftige Termine/Ankündigungen enthält (erkennbar an häufigen Wörtern wie „wird stattfinden“, „lädt ein“ etc.), während ein anderer Cluster typische Urteils-Pressemitteilungen enthält (mit Wörtern wie „hat entschieden“, „Urteil vom“ etc.). Zur Validierung müsste man einige Vertreter pro Cluster prüfen. Alternativ kann man **Topic Modeling** einsetzen, z.B. mit **LDA** (Latent Dirichlet Allocation) oder neueren Methoden. Ein modernes Verfahren ist **BERTopic**, das Transformer-Embeddings mit dimensionaler Reduktion und Clustering kombiniert, um aussagekräftige Topics zu finden ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=We%20propose%20the%20use%20of,the%20laws%20mentioned%20in%20the)). In einer Studie zu juristischen Dokumenten wurde BERTopic (mit LEGAL-BERT Vektoren und HDBSCAN-Clustering) erfolgreich genutzt, um **Themen in Urteilstexten** zu entdecken – die resultierenden Cluster hatten klare, inhaltlich kohärente Stichwörter ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=We%20propose%20the%20use%20of,the%20laws%20mentioned%20in%20the)) ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=document,and%20to%20compare%20the%20proposed)). So ein Ansatz könnte hier zeigen, welche Pressemitteilungen thematisch „aus der Reihe fallen“. **Vorteil**: Man benötigt keine Labels im Vorfeld und kann versteckte Muster finden. **Nachteil**: Die Zuordnung „irrelevant vs. relevant“ ergibt sich indirekt – man muss als Anwender die Cluster/Topics interpretieren. Es kann vorkommen, dass das Clustering nicht perfekt trennt (z.B. Mischcluster) oder kleine irrelevante Gruppen übersieht. Dennoch kann unüberwachtes Clustern ein guter Explorationsschritt sein, um beispielsweise eine **Liste prototypischer Ankündigungs-PMs** zu bekommen, die man dann mit Regeln oder als Trainingsdaten weiterverwendet.

### Zusammenfassung der Ansätze 
Zur besseren Übersicht zeigt die folgende Tabelle die genannten Ansätze mit ihren Hauptmerkmalen, Vor- und Nachteilen:

| **Ansatz**                         | **Prinzip**                                              | **Vorteile**                                           | **Nachteile**                                                |
|------------------------------------|----------------------------------------------------------|--------------------------------------------------------|--------------------------------------------------------------|
| **Regelbasiert (Keywords/Regex)**  | Feste Regeln suchen nach Indikator-Begriffen oder Mustern (z.B. „Ankündigung“, Datumsangaben in Zukunft). | + Einfach umzusetzen, keine Trainingsdaten nötig<br>+ Transparent und erklärbar (nachvollziehbare Kriterien) ([Going Beyond Keywords with NLP. Many NLP tasks can be solved with… | by Jared Rand | TDS Archive | Medium](https://medium.com/data-science/going-beyond-keywords-with-nlp-42395f7e7c67#:~:text=Keywords%20Are%20Simple%20and%20Powerful)) | – Hoher Pflegeaufwand, kaum generalisierend<br>– Evtl. niedrige Recall (übersieht unbekannte Formulierungen) ([Going Beyond Keywords with NLP. Many NLP tasks can be solved with… | by Jared Rand | TDS Archive | Medium](https://medium.com/data-science/going-beyond-keywords-with-nlp-42395f7e7c67#:~:text=multiclass%20or%20multilabel,a%20set%20of%20negative%20keywords))<br>– Anfällig für False Positives bei Mehrdeutigkeit |
| **Embeddings (semantische Ähnlichkeit)** | Texte als Vektoren, Ähnlichkeit (z.B. Kosinus) zwischen Urteil und PM messen. Niedrige Ähnlichkeit ⇒ PM vermutlich off-topic. | + Nutzt inhaltliche **Kontext-Information** statt einzelner Wörter ([GPT-3 Embeddings: Perform Text Similarity, Semantic Search, Classification, and Clustering(Part_4) | by Yashwanth Reddy | Medium](https://medium.com/@reddyyashu20/gpt-3-embeddings-perform-text-similarity-semantic-search-classification-and-clustering-part-4-abbcf447faf8#:~:text=The%20embedding%20is%20an%20information,representations%20should%20also%20be%20similar))<br>+ Kann domänenspezifische Synonyme/Paraphrasen erkennen<br>+ Keine expliziten Regeln erforderlich | – Benötigt geeignetes Sprachmodell für gute Ergebnisse (ggf. Legal Embeddings)<br>– Schwellenwert-Festlegung nötig (unsupervised Kriterium)<br>– Rechenaufwändiger als Keyword-Suche |
| **ML-Klassifikator (supervised)**  | Trainiertes Modell unterscheidet relevant vs. irrelevant auf Basis gelabelter Beispiele (Features: Wortvektoren, Embeddings, etc.). | + Lernt Kombinationen von Merkmalen, potentiell hohe Genauigkeit ([](https://arxiv.org/pdf/2304.08649#:~:text=with%20a%20German%20legal%20BERT,rel%02evance%20between%20two%20legal%20cases))<br>+ Anpassbar durch mehr Trainingsdaten, verbessert sich mit Feedback<br>+ Einmal trainiert: schnelle automatische Klassifikation aller Daten | – Benötigt *Goldstandard* (manuelle Label für Trainingsmenge)<br>– Möglichkeit von Bias/Überanpassung bei kleinem Datensatz<br>– Implementierung etwas aufwändiger (Training, Hyperparameter) |
| **Clustering/Topic Modeling (unsupervised)** | Gruppiert Pressemitteilungen nach Ähnlichkeit; irrelevante bilden hoffentlich eigene Cluster/Topics. | + Keine Vorab-Labels nötig<br>+ Kann unerwartete Gruppen aufdecken (explorativ)<br>+ Nützlich zur **Themen-Analyse** und Initial-Kategorisierung ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=document,and%20to%20compare%20the%20proposed)) | – Ergebnisse müssen manuell interpretiert werden<br>– Trennung relevanter/irrelevanter Themen nicht garantiert sauber<br>– Evtl. Feintuning der Cluster-Anzahl oder -Parameter nötig |

## Tools und Bibliotheken für die Umsetzung

Für die praktische Umsetzung der oben genannten Ansätze bieten sich diverse **Libraries** an:

- **Keyword-basierte Filter:** Hier reicht oft schon Python-Standardbibliothek wie `re` (Regex) oder einfache **String-Suchen**. Zur Verwaltung von Wortlisten und Scoring können Tools wie **NLTK** oder **spaCy** genutzt werden – z.B. spaCy’s **PhraseMatcher**, um bestimmte Ausdrücke robust im Text zu finden. Auch spezialisierte Keyword-Extraktionstools (etwa **YAKE**, **RAKE** oder **KeyBERT**) könnten helfen, charakteristische Wörter in Pressemitteilungen zu identifizieren, die für Ankündigungen typisch sind (KeyBERT nutzt BERT-Embeddings, um schlagwortartige Terme aus einem Dokument zu ziehen). Für das Scoring kann man relativ einfache Heuristiken einsetzen (z.B. Zählen der Treffer pro Liste, Vorkommen bestimmter Muster wie Zahlen-datum-Kombinationen etc.). 

- **Semantische Embeddings & Ähnlichkeit:** Die Python-Bibliothek **Sentence-Transformers** (`sentence_transformers`) bietet vortrainierte **Sentence-BERT-Modelle** an, darunter multilingual Modelle, die deutsche Texte gut unterstützen. Beispielsweise könnte man `paraphrase-multilingual-MiniLM-L12-v2` oder `distiluse-base-multilingual-cased` verwenden, um jeden Text als Vektor abzubilden. Für den juristischen Bereich gibt es auch **domänenspezifische Embeddings** (z.B. Modelle, die auf Gerichtsurteilen trainiert wurden – siehe **Legal-BERT** Familie ([](https://arxiv.org/pdf/2304.08649#:~:text=2020a%29%20releases%20Legal,of%20legal%20professional%20search%20and))). Hat man Vektoren, kann man mit gängigen Bibliotheken **Cosine Similarity** berechnen (z.B. via `sklearn.metrics.pairwise` oder einfach numpy). Falls das Volumen größer wäre, käme auch ein **Vektorindex** wie **FAISS** in Betracht, um schnell ähnliche Paare zu finden; für 6.500 Paare ist das aber nicht zwingend nötig – ein simpler Batch-Vergleich ist machbar. Alternativ kann man auch eine **semantische Suche** simulieren: z.B. den Urteilstext als Query und in einer Such-Engine (wie **Elasticsearch** mit **BM25** oder **Elasticsearch kNN** für Vektoren) die entsprechende PM suchen – sofern die PM nicht zum Urteil passt, würde sie schlecht „ranken“. Frameworks wie **Haystack** (deepset) erlauben die Kombination von klassischen und neuralen Suchen und könnten hier experimentell genutzt werden, um Pressemitteilungen ohne Urteilsbezug aufzuspüren (z.B. indem man prüft, ob wichtige Urteils-Keywords in der PM vorkommen, notfalls via embedding-augmented search).

- **Textklassifikation:** Für klassische ML-Ansätze bietet **scikit-learn** Werkzeuge wie `TfidfVectorizer` (zur Umwandlung von Text in Merkmalsvektoren) und Modelle wie `LogisticRegression` oder `LinearSVC`. Diese sind leichtgewichtig und liefern oft schon gute Ergebnisse bei genügend manuellen Labels. Für Deep-Learning-basierte Ansätze ist **Hugging Face Transformers** die erste Wahl: Mit wenigen Zeilen Code lässt sich ein Pretrained Model laden und auf eine eigene Trainingsmenge feinjustieren ([python - HuggingFace Transformers model for German news classification - Stack Overflow](https://stackoverflow.com/questions/63672169/huggingface-transformers-model-for-german-news-classification#:~:text=In%20general%20I%20recommend%20you,in%20case%20of%20insufficient%20results)). Für Deutsch stehen z.B. *bert-base-german-dbmdz* (case/uncased) oder *german-multilingual* Modelle bereit. Auch spezialisierte Modelle wie *legal-specific BERT* (etwa **BERTRAM** oder andere juristische Language Models) könnten getestet werden, sofern verfügbar. Als Alternative mit geringerem Rechenaufwand bietet sich **fastText** (Facebook) an – ein einfaches Modell, das auf n-Gramm-Ebene Texte klassifiziert, was als Baseline dienen kann. Ebenfalls erwähnenswert ist **Flair** (Zalando Research): eine NLP-Bibliothek mit vortrainierten Embeddings (inkl. Deutsch) und einfach zu nutzendem TextClassifier, die oft im NLP-Wettbewerben gute Ergebnisse zeigt. Wichtig bei allen Klassifikations-Tools ist die Möglichkeit, mit dem eigenen Labelschema zu trainieren – sowohl scikit-learn als auch Transformers/Flair unterstützen binäre oder Mehrklassen-Klassifikation problemlos.

- **Clustering & Topic Modeling:** Für unkompliziertes Clustering kann man wieder **scikit-learn** verwenden (z.B. `KMeans`, `DBSCAN`). In Kombination mit vorverarbeiteten Embeddings (wie oben erwähnt) lassen sich damit Gruppen bilden. Für hierarchisches Density-Clustering (automatische Cluster-Anzahl) gibt es **HDBSCAN** (als Python-Paket verfügbar), welches in komplexeren Text-Topic-Modellen oft genutzt wird ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=clustering%20is%20to%20find%20areas,has%20not%20been%20defined%2C%20while)). Zum **Topic Modeling** klassischer Art wäre **gensim** die Standardbibliothek – sie bietet LDA und LSI, wobei man die Texte vorher z.B. auf Wort-Stemmformen reduzieren könnte. Ein fortgeschritteneres Tool ist **BERTopic** (Python-Paket), das intern folgende Pipeline nutzt: Transformer Embedding -> **UMAP** zur Dimensionsreduktion -> **HDBSCAN** zum Clustern -> **c-TF-IDF** zur Schlagwort-Extraktion je Topic ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=Then%2C%20we%20identified%20a%20set,4%29%20below)). BERTopic ist attraktiv, weil es gleich interpretierbare Topic-Labels liefert (Top-Wörter pro Cluster), was helfen kann, „Ankündigungs“-Cluster an ihren Schlagworten zu erkennen. Für unsere Zwecke könnte man mit BERTopic testen, ob zwei dominante Topics entstehen (z.B. *Urteilsinhalte* vs. *Verfahrensankündigungen*). Generell kann man auch Visualisierungstools wie **TensorBoard** oder **UMAP-Plot** einsetzen, um die Text-Embeddings zweidimensional zu plotten und so ein Gefühl für Gruppen/Outlier zu bekommen.

## Best Practices und relevante Forschung

Bei der Bereinigung juristischer Textdaten gibt es einige **Bewährte Vorgehensweisen**:

- **Kombination von Methoden:** Oft führt eine Kombination zum Ziel. Z.B. könnte man **regelbasierte Filter** zuerst anwenden, um offensichtliche Ankündigungen (mit klaren Keywords) zu entfernen – diese haben meist hohe Präzision ([Going Beyond Keywords with NLP. Many NLP tasks can be solved with… | by Jared Rand | TDS Archive | Medium](https://medium.com/data-science/going-beyond-keywords-with-nlp-42395f7e7c67#:~:text=multiclass%20or%20multilabel,a%20set%20of%20negative%20keywords)). Die verbleibenden Grenzfälle kann man dann mittels **ML-Klassifikator** oder **Embedding-Ähnlichkeit** unterscheiden. Ein hybrides Vorgehen erhöht Robustheit: Regeln fangen einfache Fälle ab, das ML-Modell lernt den Rest. Auch ein manueller Review der vom Modell unsicher eingestuften Fälle (Active Learning) verbessert die Qualität iterativ.

- **Goldstandard und Evaluation:** Unabhängig vom Ansatz ist es ratsam, ein kleines **validiertes Testset** zu erstellen. Beispielsweise 100 zufällige Pressemitteilungen manuell labeln (relevant/irrelevant) und die automatischen Filter dagegen prüfen. So bekommt man Feedback über Precision/Recall der Methode und vermeidet, dass zu viele gültige PMs entfernt werden. Falls möglich, kann man auch **Cross-Validation** auf einem gelabelten Teil durchführen, insbesondere beim trainierten Klassifikator.

- **Domain-Anpassung:** Juristische Dokumente haben oft speziellen Sprachgebrauch. Forschung im Legal NLP zeigt, dass **domänenspezifische Modelle** die Leistung steigern: Chalkidis et al. (2020) etwa haben **Legal-BERT** veröffentlicht – eine BERT-Variante speziell auf juristischen Texten trainiert ([](https://arxiv.org/pdf/2304.08649#:~:text=2020a%29%20releases%20Legal,of%20legal%20professional%20search%20and)). Solche Modelle verstehen juristische Fachbegriffe und stilistische Eigenheiten besser. Ebenso wurde ein **deutsches Legal BERT** entwickelt ([](https://arxiv.org/pdf/2304.08649#:~:text=considering%20the%20context,including%20classification%2C%20regression%20and%20similarity)), das bei Klassifikation, Regression und Ähnlichkeitstasks mit juristischen Daten State-of-the-Art-Ergebnisse erzielt. Für unsere Aufgabe könnte der Einsatz eines solchen vortrainierten Modells (anstelle eines generischen BERT) die Erkennung subtiler Unterschiede (Urteil vs. Ankündigung) verbessern. Auch **Word Embeddings** können angepasst werden – z.B. durch Feintuning der Embeddings auf einem Korpus von Gerichtsurteilen, damit semantische Vergleiche treffsicherer werden.

- **Verwandte Arbeiten:** Das Problem der **Relevanzfilterung** kennt man im juristischen Bereich insbesondere vom **e-Discovery** und **Legal Information Retrieval**. Hier wird durch Klassifikatoren entschieden, welche Dokumente für einen Fall relevant sind ([](https://arxiv.org/pdf/2304.08649#:~:text=electronic%20discovery%20determining%20the%20relevance,generate%20routine%20legal%20documents%20and)). Ähnlich werden in unserem Szenario Urteils-Pressemitteilungen von irrelevanten Pressemitteilungen getrennt. Methoden wie in **(Ruch et al., 2011)** oder TREC’s Legal Track verwenden oft eine Mischung aus Stichwortsuche und statistischen Modellen, was auch unseren Ansätzen entspricht. Darüber hinaus gibt es Forschungsarbeiten zur **juristischen Dokumentklassifikation** (z.B. Elwany et al. 2019, der hunderttausende von Rechtsdokumenten per BERT kategorisiert hat ([](https://arxiv.org/pdf/2304.08649#:~:text=fine,other%20approaches%20to%20handle%20long))) und zu **vergleichbarer Fallsuche** (Case Matching), wo ebenfalls Embeddings genutzt werden, um semantisch passende Urteile zu finden ([](https://arxiv.org/pdf/2304.08649#:~:text=%28Zhong%20et%20al,to%20check%20that%20a%20contract)). Diese zeigen, dass **Transformer-Modelle traditionelle Ansätze oft übertreffen** ([](https://arxiv.org/pdf/2304.08649#:~:text=2020a%29%20releases%20Legal,2020)), sofern genügend Daten und Rechenleistung vorhanden sind. 

- **Handling von Metadaten:** Falls verfügbar, können auch **Metadaten** hinzugezogen werden. Beispielsweise, viele Pressemitteilungen haben vielleicht einen Titel oder Datum. Eine Mitteilung mit Titel "*Pressetermin am ...*" ist eindeutig irrelevant. Oder Pressemitteilungen, die keinem Aktenzeichen zuordenbar sind, könnten verdächtig sein. Solche strukturierten Hinweise sollte man nutzen, wo möglich – sie können als Features in einen Klassifikator einfließen oder als vorab-Filterregeln (z.B. wenn `Pressemitteilung.Titel` das Wort "Wochenvorschau" enthält, direkt entfernen – was z.B. beim Bundesverfassungsgericht Presse-Bereich regelmäßig vorkommt).

**Fazit:** Durch eine Kombination aus **heuristischen Filtern**, **semantischer Analyse** und ggf. **überwachtem Lernen** lässt sich der Datensatz vermutlich effektiv von nicht-urteilsbezogenen Pressemitteilungen säubern. Wichtig ist, das Vorgehen empirisch zu validieren (kleiner Goldstandard) und bei Bedarf zu iterieren. Die genannten Tools – von einfachen Regex über Transformer-Modelle bis zu Clustering-Frameworks – bieten dabei eine breite Palette an Möglichkeiten, um sowohl mit präzisen Regeln als auch mit lernenden Algorithmen ans Ziel zu gelangen. Die Erfahrungen aus Legal NLP Forschung (z.B. Einsatz von Legal-BERT ([](https://arxiv.org/pdf/2304.08649#:~:text=2020a%29%20releases%20Legal,of%20legal%20professional%20search%20and)), Topic Modelling auf Urteilen ([Microsoft Word - RELATED_2021 - Camera ready.docx](https://ceur-ws.org/Vol-2896/RELATED_2021_paper_6.pdf#:~:text=We%20propose%20the%20use%20of,the%20laws%20mentioned%20in%20the))) können dabei als Leitfaden dienen, die Methoden an die Besonderheiten juristischer Texte anzupassen und optimale Ergebnisse zu erzielen.

